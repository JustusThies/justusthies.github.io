<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<!-- <title>Publications - Justus Thies</title> -->
	<title>Publications</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Justus Thies" property="og:site_name">
  
    <meta content="Publications" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="Personal Website of Justus Thies covering his publications and teaching courses.
" property="og:description">
  
  
    <meta content="https://justusthies.github.io/publications/" property="og:url">
  
  
  
    <meta content="https://justusthies.github.io/assets/img/justus-thies.jpg" property="og:image">
  
  
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@JustusThies">
  
    <meta name="twitter:title" content="Publications">
  
  
    <meta name="twitter:url" content="https://justusthies.github.io/publications/">
  
  
    <meta name="twitter:description" content="Personal Website of Justus Thies covering his publications and teaching courses.
">
  
  
    <meta name="twitter:image:src" content="https://justusthies.github.io/assets/img/justus-thies.jpg">
  

	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/justus-thies.jpg" alt="Justus Thies"></a>
      </div>
      <div class="author-name">Justus Thies</div>
	  <div class="author-about">I am a postdoctoral researcher focusing on digital humans, video editing and forgery detection.</div>
    </div>
	

	<section class="subpages">	
		<a href="/publications"><div class="subpages-title">Publications</div></a>
    <a href="/teaching"><div class="subpages-title">Teaching</div></a>
    <a href="/tutorials"><div class="subpages-title">Tutorials  & Demos</div></a>
	</section>
	
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/JustusThies" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/100012738961011" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/justusthies" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
        
          <li class="email"><a href="mailto:justus.thies@tum.de"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
        <p>2020 &copy; Justus Thies</p>

        <p>Website is based on <a href="https://github.com/artemsheludko/flexible-jekyll" target="_blank">flexible-jekyll</a>.</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->

<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content"> 
  
	<div class="wrap-content">
		<header class="header-page">
			<h1 class="page-title">Publications</h1>
		</header>
		<div class="page-description">The main theme of my work is the creation of digital humans using commodity hardware. It includes the modeling of the human body, tracking, as well as the reconstruction and interaction with the environment. The digitization is needed for various applications in AR/VR as well as in movie (post-)production. Teleconferencing and working in VR is of high interest for many companies ranging from social media platforms to car manufacturer. It enables the remote interaction in VR, e.g., the inspection of 3D content like CAD models or scans from real objects. A realistic reproduction of appearances and motions is key for such applications. Thus, my work is closely related to photo-realistic video synthesis and editing. The development of algorithms for photo-realistic creation or editing of image content comes with a certain responsibility, since the generation of photo-realistic imagery can be misused. Thus, I'm also working on the detection of synthetic images or manipulations (Digital Multi-media Forensics).
</div>
	</div>


	
	
		<hr>
		<h3 class="post-subtitle">2020</h3>
		<hr>
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/neuralrenderingstar/thumb.jpg)" href="/posts/neuralrenderingstar/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.ohadf.com/" target="_blank" rel="noopener noreferrer">Ohad Fried</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://vsitzmann.github.io/" target="_blank" rel="noopener noreferrer">Vincent Sitzmann</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://stephenlombardi.github.io/" target="_blank" rel="noopener noreferrer">Stephen Lombardi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.kalyans.org/" target="_blank" rel="noopener noreferrer">Kalyan Sunkavalli</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.ricardomartinbrualla.com/" target="_blank" rel="noopener noreferrer">Ricardo Martin-Brualla</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.cs.cmu.edu/~tsimon/" target="_blank" rel="noopener noreferrer">Tomas Simon</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://jsaragih.org/Home_Page.html" target="_blank" rel="noopener noreferrer">Jason Saragih</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://research.google/people/106687/" target="_blank" rel="noopener noreferrer">Rohit K Pandey</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.seanfanello.it/" target="_blank" rel="noopener noreferrer">Sean Fanello</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://stanford.edu/~gordonwz/" target="_blank" rel="noopener noreferrer">Gordon Wetzstein</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.csail.mit.edu/junyanz/" target="_blank" rel="noopener noreferrer">Jun-Yan Zhu</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://graphics.stanford.edu/~maneesh/" target="_blank" rel="noopener noreferrer">Maneesh Agrawala</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://research.adobe.com/person/eli-shechtman/" target="_blank" rel="noopener noreferrer">Eli Shechtman</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.danbgoldman.com/" target="_blank" rel="noopener noreferrer">Dan B Goldman</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/neuralrenderingstar/">State of the Art on Neural Rendering</a></h2>
		

		<!--<p>Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted...</p>-->
		<p>Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. This state-of-the-art report summarizes the recent trends and applications of neural rendering.</p>
		<span class="post-date">2020, Apr 08&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2004.03805.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/neuralrenderingstar/tewari2020neuralrendering.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/advtex/thumb.jpg)" href="/posts/advtex/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://stanford.edu/~jingweih/" target="_blank" rel="noopener noreferrer">Jingwei Huang</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://abhijitkundu.info/" target="_blank" rel="noopener noreferrer">Abhijit Kundu</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.maxjiang.ml/" target="_blank" rel="noopener noreferrer">Chiyu 'Max' Jiang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://geometry.stanford.edu/member/guibas/" target="_blank" rel="noopener noreferrer">Leonidas Guibas</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/advtex/">Adversarial Texture Optimization from RGB-D Scans</a></h2>
		

		<!--<p>Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera...</p>-->
		<p>We present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views. Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors.</p>
		<span class="post-date">2020, Mar 19&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="http://stanford.edu/~jingweih/papers/advtex/supp/paper.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=52xlRn0ESek" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/advtex/huang2020advtex.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/ignor/thumb.jpg)" href="/posts/ignor/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/ignor/">Image-guided Neural Object Rendering</a></h2>
		

		<!--<p>We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to...</p>-->
		<p>We propose a new learning-based novel view synthesis approach for scanned objects that is trained based on a set of multi-view images, where we directly train a deep neural network to synthesize a view-dependent image of an object.</p>
		<span class="post-date">2020, Jan 15&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1811.10720.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=s79HG9yn7QM" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/ignor/thies2020ignor.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/neural-voice-puppetry/thumb.jpg)" href="/posts/neural-voice-puppetry/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://people.mpi-inf.mpg.de/~elgharib/" target="_blank" rel="noopener noreferrer">Mohamed Elgharib</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/neural-voice-puppetry/">Neural Voice Puppetry&#58; <br> Audio-driven Facial Reenactment</a></h2>
		

		<!--<p>We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis. Given an audio sequence of a source person or digital assistant, we...</p>-->
		<p>Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input.</p>
		<span class="post-date">2020, Jan 08&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1912.05566.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=s74_yQiJMXA" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/neural-voice-puppetry/thies2020nvp.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2019</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/spoc/thumb.jpg)" href="/posts/spoc/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/davide_cozzolino.html" target="_blank" rel="noopener noreferrer">Davide Cozzolino</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andreas_roessler/profile.html" target="_blank" rel="noopener noreferrer">Andreas Rössler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/verdoliva.html" target="_blank" rel="noopener noreferrer">Luisa Verdoilva</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/spoc/">SpoC&#58; Spoofing Camera Fingerprints</a></h2>
		

		<!--<p>Thanks to the fast progress in synthetic media generation, creating realistic false images has become very easy. Such images can be used to wrap “rich”...</p>-->
		<p>In this paper, we challenge forensic forgery detectors that are based on camera fingerprints (i.e., traces of the image capturing and processing pipeline) to gain insights into their vulnerabilities.</p>
		<span class="post-date">2019, Nov 26&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1911.12069.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/spoc/cozzolino2019spoc.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/faceforensics++/thumb.jpg)" href="/posts/faceforensics++/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andreas_roessler/profile.html" target="_blank" rel="noopener noreferrer">Andreas Rössler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/davide_cozzolino.html" target="_blank" rel="noopener noreferrer">Davide Cozzolino</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/verdoliva.html" target="_blank" rel="noopener noreferrer">Luisa Verdoilva</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs1.tf.fau.de/christian-riess/" target="_blank" rel="noopener noreferrer">Christian Riess</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/faceforensics++/">FaceForensics++&#58; <br> Learning to Detect Manipulated Facial Images</a></h2>
		

		<!--<p>The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society....</p>-->
		<p>In this paper, we examine the realism of state-of-the-art facial image manipulation methods, and how difficult it is to detect them - either automatically or by humans. In particular, we create a datasets that is focused on DeepFakes, Face2Face, FaceSwap, and Neural Textures as prominent representatives for facial manipulations.</p>
		<span class="post-date">2019, Aug 26&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1901.08971.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=x2g48Q2I2ZQ" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/faceforensics++/roessler2019faceforensicspp.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/deferred-neural-rendering/thumb.jpg)" href="/posts/deferred-neural-rendering/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/deferred-neural-rendering/">Deferred Neural Rendering&#58; <br> Image Synthesis using Neural Textures</a></h2>
		

		<!--<p>The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we...</p>-->
		<p>Deferred Neural Rendering is a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable Neural Textures. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect.</p>
		<span class="post-date">2019, Apr 28&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1904.12356.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=z-pVip6WeyY" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/deferred-neural-rendering/thies2019neural.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/deepvoxels/thumb.jpg)" href="/posts/deepvoxels/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://vsitzmann.github.io/" target="_blank" rel="noopener noreferrer">Vincent Sitzmann</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs.princeton.edu/~fheide/" target="_blank" rel="noopener noreferrer">Felix Heide</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://stanford.edu/~gordonwz/" target="_blank" rel="noopener noreferrer">Gordon Wetzstein</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/deepvoxels/">DeepVoxels&#58; Learning Persistent 3D Feature Embeddings</a></h2>
		

		<!--<p>In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To...</p>-->
		<p>In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D object without having to explicitly model its geometry.</p>
		<span class="post-date">2019, Apr 11&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1812.01024.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=HM_WsZhoGXw" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/deepvoxels/sitzmann2019deepvoxels.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/acm-research-highlight/thumb.png)" href="/posts/acm-research-highlight/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/acm-research-highlight/">Research Highlight&#58; Face2Face</a></h2>
		

		<!--<p>Face2Face is an approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video...</p>-->
		<p>Research highlight of the Face2Face approach featured on the cover of Communications of the ACM in January 2019. Face2Face is an approach for real-time facial reenactment of a monocular target video. The method had significant impact in the research community and far beyond; it won several wards, e.g., Siggraph ETech Best in Show Award, it was featured in countless media articles, e.g., NYT, WSJ, Spiegel, etc., and it had a massive reach on social media with millions of views.</p>
		<span class="post-date">2019, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://dl.acm.org/citation.cfm?id=3301004.3292039&coll=portal&dl=ACM" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=PJs3rlCBk1E" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/acm-research-highlight/thies2018face.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2018</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/forensictransfer/thumb.jpg)" href="/posts/forensictransfer/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/davide_cozzolino.html" target="_blank" rel="noopener noreferrer">Davide Cozzolino</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andreas_roessler/profile.html" target="_blank" rel="noopener noreferrer">Andreas Rössler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs1.tf.fau.de/christian-riess/" target="_blank" rel="noopener noreferrer">Christian Riess</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/verdoliva.html" target="_blank" rel="noopener noreferrer">Luisa Verdoilva</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/forensictransfer/">ForensicTransfer&#58; Weakly-supervised Domain Adaptation for Forgery Detection</a></h2>
		

		<!--<p>Distinguishing fakes from real images is becoming increasingly difficult as new sophisticated image manipulation approaches come out by the day. Convolutional neural networks (CNN) show...</p>-->
		<p>ForensicTransfer tackles two challenges in multimedia forensics. First, we devise a learning-based forensic detector which adapts well to new domains, i.e., novel manipulation methods. Second we handle scenarios where only a handful of fake examples are available during training.</p>
		<span class="post-date">2018, Dec 06&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1812.02510.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/forensictransfer/cozzolino2018forensictransfer.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/headon/thumb.jpg)" href="/posts/headon/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/headon/">HeadOn&#58; Real-time Reenactment of Human Portrait Videos</a></h2>
		

		<!--<p>We propose HeadOn, the first real-time source-to-target reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and...</p>-->
		<p>HeadOn is the first real-time reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and eye gaze. Given a short RGB-D video of the target actor, we automatically construct a personalized geometry proxy that embeds a parametric head, eye, and kinematic torso model. A novel reenactment algorithm employs this proxy to map the captured motion from the source to the target actor.</p>
		<span class="post-date">2018, May 29&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1805.11729.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=7Dg49wv2c_g" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/headon/thies2018headon.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/deepvideo/thumb.jpg)" href="/posts/deepvideo/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~hyeongwoo/" target="_blank" rel="noopener noreferrer">Hyeongwoo Kim</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~pgarrido/index.html" target="_blank" rel="noopener noreferrer">Pablo Garrido</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~wxu/" target="_blank" rel="noopener noreferrer">Weipeng Xu</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ptrckprz.github.io/" target="_blank" rel="noopener noreferrer">Patrick Perez</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://richardt.name/" target="_blank" rel="noopener noreferrer">Christian Richardt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/deepvideo/">Deep Video Portraits</a></h2>
		

		<!--<p>We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted...</p>-->
		<p>Our novel approach enables photo-realistic re-animation of portrait videos using only an input video. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor.</p>
		<span class="post-date">2018, May 29&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1805.11714.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=qc5P2bvfl44" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/deepvideo/kim2018deepvideo.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/inversefacenet/thumb.jpg)" href="/posts/inversefacenet/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~hyeongwoo/" target="_blank" rel="noopener noreferrer">Hyeongwoo Kim</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://richardt.name/" target="_blank" rel="noopener noreferrer">Christian Richardt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/inversefacenet/">InverseFaceNet&#58; Deep Monocular Inverse Face Rendering</a></h2>
		

		<!--<p>We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input...</p>-->
		<p>We introduce InverseFaceNet, a deep convolutional inverse rendering framework for faces that jointly estimates facial pose, shape, expression, reflectance and illumination from a single input image. This enables advanced real-time editing of facial imagery, such as appearance editing and relighting.</p>
		<span class="post-date">2018, May 16&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/abs/1703.10956" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/inversefacenet/kim2018inversefacenet.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/facestar/teaser.jpg)" href="/posts/facestar/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://la.disneyresearch.com/people/derek-bradley/" target="_blank" rel="noopener noreferrer">Derek Bradley</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~pgarrido/index.html" target="_blank" rel="noopener noreferrer">Pablo Garrido</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://la.disneyresearch.com/people/thabo-beeler/" target="_blank" rel="noopener noreferrer">Thabo Beeler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ptrckprz.github.io/" target="_blank" rel="noopener noreferrer">Patrick Perez</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/facestar/">State of the Art on Monocular 3D Face Reconstruction, Tracking, and Applications</a></h2>
		

		<!--<p>The computer graphics and vision communities have dedicated long standing efforts in building computerized tools for reconstructing, tracking, and analyzing human faces based on visual...</p>-->
		<p>This report summarizes recent trends in monocular facial performance capture and discusses its applications, which range from performance-based animation to real-time facial reenactment. We focus on methods where the central task is to recover and track a three dimensional model of the human face using optimization-based reconstruction algorithms.</p>
		<span class="post-date">2018, Apr 24&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="http://zollhoefer.com/papers/EG18_FaceSTAR/paper.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/facestar/zollhoefer2018facestar.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/faceforensics/teaser.jpg)" href="/posts/faceforensics/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andreas_roessler/profile.html" target="_blank" rel="noopener noreferrer">Andreas Rössler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/davide_cozzolino.html" target="_blank" rel="noopener noreferrer">Davide Cozzolino</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/verdoliva.html" target="_blank" rel="noopener noreferrer">Luisa Verdoilva</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs1.tf.fau.de/christian-riess/" target="_blank" rel="noopener noreferrer">Christian Riess</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/faceforensics/">FaceForensics&#58; A Large-scale Video Dataset for Forgery Detection in Human Faces</a></h2>
		

		<!--<p>FaceForensics is a video dataset consisting of more than 500,000 frames containing faces from 1004 videos that can be used to study image or video...</p>-->
		<p>In this paper, we introduce FaceForensics, a large scale video dataset consisting of 1004 videos with more than 500000 frames, altered with Face2Face, that can be used for forgery detection and to train generative refinement methods.</p>
		<span class="post-date">2018, Mar 24&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1803.09179.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=Tle7YaPkO_k" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/faceforensics/roessler2018faceforensics.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/facevr/thumb.jpg)" href="/posts/facevr/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/facevr/">FaceVR&#58; Real-Time Facial Reenactment and Eye Gaze Control in Virtual Reality</a></h2>
		

		<!--<p>We propose FaceVR, a novel image-based method that enables video teleconferencing in VR based on self-reenactment. State-of-the-art face tracking methods in the VR context are...</p>-->
		<p>We propose FaceVR, a novel image-based method that enables video teleconferencing in VR based on self-reenactment. The key component of FaceVR is a robust algorithm to perform real-time facial motion capture of an actor who is wearing a head-mounted display (HMD).</p>
		<span class="post-date">2018, Mar 21&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/abs/1610.03151" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=jIlujM5avU8" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/facevr/thies2018facevr.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2017</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/dissertation/thumb.jpg)" href="/posts/dissertation/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/dissertation/">Dissertation&#58; Face2Face - Facial Reenactment</a></h2>
		

		<!--<p>In this dissertation we show our advances in the field of 3D reconstruction of human faces using commodity hardware. Beside the reconstruction of the facial...</p>-->
		<p>This dissertation summarizes the work in the field of markerless motion tracking, face reconstruction and its applications. Especially, it shows real-time facial reenactment that enables the transfer of facial expressions from one video to another video.</p>
		<span class="post-date">2017, Oct 16&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://diglib.eg.org/bitstream/handle/10.2312/2631994/dissertation_justus_thies.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/dissertation/thies2017diss.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/faceforge/teaser.jpg)" href="/posts/faceforge/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/christian-siegl/" target="_blank" rel="noopener noreferrer">Christian Siegl</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/vanessa-lange/" target="_blank" rel="noopener noreferrer">Vanessa Lange</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/frank-bauer/" target="_blank" rel="noopener noreferrer">Frank Bauer</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/faceforge/">FaceForge&#58; Markerless Non-Rigid Face Multi-Projection Mapping</a></h2>
		

		<!--<p>Recent publications and art performances demonstrate amazing results using projection mapping. To our knowledge, there exists no multi-projection system that can project onto non-rigid target...</p>-->
		<p>In this paper, we introduce FaceForge, a multi-projection mapping system that is able to alter the appearance of a non-rigidly moving human face in real time.</p>
		<span class="post-date">2017, Oct 10&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1904.12356.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/faceforge/siegl2017faceforge.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2016</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/face2face/thumb.jpg)" href="/posts/face2face/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/face2face/">Face2Face&#58; Real-time Face Capture and Reenactment of RGB Videos</a></h2>
		

		<!--<p>We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular...</p>-->
		<p>We present a novel approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video stream, captured live with a commodity webcam. Our goal is to animate the facial expressions of the target video by a source actor and re-render the manipulated output video in a photo-realistic fashion.</p>
		<span class="post-date">2016, Mar 23&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://zollhoefer.com/papers/CACM19_F2F/paper.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=ohmajJTcpNk" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/face2face/thies2016face.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/knee-joint-alignment/thumb.jpg)" href="/posts/knee-joint-alignment/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://www5.cs.fau.de/en/our-team/berger-martin/projects/" target="_blank" rel="noopener noreferrer">Martin Berger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Kerstin Müller</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www5.cs.fau.de/en/our-team/aichert-andre" target="_blank" rel="noopener noreferrer">Andre Aichert</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www5.cs.fau.de/en/our-team/unberath-mathias/projects/" target="_blank" rel="noopener noreferrer">Mathias Unberath</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Jang-Hwan Choi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Rebecca Fahrig</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www5.cs.fau.de/en/our-team/maier-andreas" target="_blank" rel="noopener noreferrer">Andreas Maier</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/knee-joint-alignment/">Marker-free Motion Correction in Weight-Bearing Cone-Beam CT of the Knee Joint</a></h2>
		

		<!--<p>Weight-bearing imaging of the knee joint in a standing position poses additional requirements for the image reconstruction algorithm. In contrast to supine scans, patient motion...</p>-->
		<p>We present image-based motion estimation and compensation in weightbearing cone-beam computed tomography of the knee joint</p>
		<span class="post-date">2016, Feb 22&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://www5.informatik.uni-erlangen.de/Forschung/Publikationen/2016/Berger16-MMC.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/knee-joint-alignment/berger2016.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2015</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/projection-mapping/thumb.jpg)" href="/posts/projection-mapping/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/christian-siegl/" target="_blank" rel="noopener noreferrer">Christian Siegl</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/matteo-colaianni/" target="_blank" rel="noopener noreferrer">Matteo Colaianni</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/lucas-thies/" target="_blank" rel="noopener noreferrer">Lucas Thies</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://scholar.google.com/citations?user=hkCVqYkAAAAJ&hl=de" target="_blank" rel="noopener noreferrer">Shahram Izadi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/frank-bauer/" target="_blank" rel="noopener noreferrer">Frank Bauer</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/projection-mapping/">Real-Time Pixel Luminance Optimization for Dynamic Multi-Projection Mapping</a></h2>
		

		<!--<p>Using projection mapping enables us to bring virtual worlds into shared physical spaces. In this paper, we present a novel, adaptable and real-time projection mapping...</p>-->
		<p>Using projection mapping enables us to bring virtual worlds into shared physical spaces. In this paper, we present a novel, adaptable and real-time projection mapping system, which supports multiple projectors and high quality rendering of dynamic content on surfaces of complex geometrical shape. Our system allows for smooth blending across multiple projectors using a new optimization framework that simulates the diffuse direct light transport of the physical world to continuously adapt the color output of each projector pixel.</p>
		<span class="post-date">2015, Sep 14&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://niessnerlab.org/papers/2015/10projectionmapping/siegl2015projectionmapping.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/projection-mapping/siegl2015projectionmapping.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/face-rgbd/thumb.jpg)" href="/posts/face-rgbd/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Levi Valgaerts</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/face-rgbd/">Real-time Expression Transfer for Facial Reenactment</a></h2>
		

		<!--<p>We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video,...</p>-->
		<p>We present a method for the real-time transfer of facial expressions from an actor in a source video to an actor in a target video, thus enabling the ad-hoc control of the facial expressions of the target actor.</p>
		<span class="post-date">2015, Aug 27&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://web.stanford.edu/~zollhoef/papers/SGA2015_Face/paper.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=eXVspNUeiWw" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/face-rgbd/thies2015realtime.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2014</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/interactive-head-reconstruction/thumb.jpg)" href="/posts/interactive-head-reconstruction/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/matteo-colaianni/" target="_blank" rel="noopener noreferrer">Matteo Colaianni</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/guenther-greiner/" target="_blank" rel="noopener noreferrer">Günther Greiner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/interactive-head-reconstruction/">Interactive Model-based Reconstruction of the Human Head using an RGB-D Sensor</a></h2>
		

		<!--<p>We present a novel method for the interactive markerless reconstruction of human heads using a single commodity RGB-D sensor. Our entire reconstruction pipeline is implemented...</p>-->
		<p>We present a novel method for the interactive markerless reconstruction of human heads using a single commodity RGB‐D sensor. Our entire reconstruction pipeline is implemented on the graphics processing unit and allows to obtain high‐quality reconstructions of the human head using an interactive and intuitive reconstruction paradigm.</p>
		<span class="post-date">2014, Apr 28&nbsp;&nbsp;&nbsp;—&nbsp;</span>
		<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>

		
			<span class="post-paper-link"><a href="https://web.stanford.edu/~zollhoef/papers/CASA2014_Face/paper.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/interactive-head-reconstruction/zollhoefer2014Head.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	

  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>

</body>
</html>
