<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<!-- <title>Publications - Justus Thies</title> -->
	<title>Publications</title>

  <!-- Edit site and author settings in `_config.yml` to make the social details your own -->

    <meta content="Justus Thies" property="og:site_name">
  
    <meta content="Publications" property="og:title">
  
  
    <meta content="article" property="og:type">
  
  
    <meta content="Personal Website of Justus Thies covering his publications and teaching courses.
" property="og:description">
  
  
    <meta content="https://justusthies.github.io/publications/index.html" property="og:url">
  
  
  
    <meta content="https://justusthies.github.io/assets/img/justus-thies.jpg" property="og:image">
  
  
  

    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@">
    <meta name="twitter:creator" content="@JustusThies">
  
    <meta name="twitter:title" content="Publications">
  
  
    <meta name="twitter:url" content="https://justusthies.github.io/publications/index.html">
  
  
    <meta name="twitter:description" content="Personal Website of Justus Thies covering his publications and teaching courses.
">
  
  
    <meta name="twitter:image:src" content="https://justusthies.github.io/assets/img/justus-thies.jpg">
  

	<meta name="description" content="">
	<meta http-equiv="X-UA-Compatible" content="IE=edge">
	<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
	<meta property="og:image" content="">
	<link rel="shortcut icon" href="/assets/img/favicon/favicon.ico" type="image/x-icon">
	<link rel="apple-touch-icon" href="/assets/img/favicon/apple-touch-icon.png">
	<link rel="apple-touch-icon" sizes="72x72" href="/assets/img/favicon/apple-touch-icon-72x72.png">
	<link rel="apple-touch-icon" sizes="144x144" href="/assets/img/favicon/apple-touch-icon-144x144.png">
	<!-- Chrome, Firefox OS and Opera -->
	<meta name="theme-color" content="#263959">
	<!-- Windows Phone -->
	<meta name="msapplication-navbutton-color" content="#263959">
	<!-- iOS Safari -->
	<meta name="apple-mobile-web-app-status-bar-style" content="#263959">
	<!-- Google Fonts -->
	<link href="https://fonts.googleapis.com/css?family=PT+Serif:400,700" rel="stylesheet">
	<link href="https://fonts.googleapis.com/css?family=Lato:300,400,700" rel="stylesheet">
	<!-- Font Awesome -->
	<link rel="stylesheet" href="/assets/fonts/font-awesome/css/font-awesome.min.css">
	<!-- Styles -->
	<link rel="stylesheet" href="/assets/css/main.css">
</head>

<body>

  <div class="wrapper">
    <aside class="sidebar">
  <header>
    <div class="about">
      <div class="cover-author-image">
        <a href="/"><img src="/assets/img/justus-thies.jpg" alt="Justus Thies"></a>
      </div>
      <div class="author-name">Justus Thies</div>
	  <div class="author-about">I am a full professor at TU Darmstadt and research group leader at MPI-IS focusing on digital humans.</div>
    </div>
	

	<section class="subpages">
		<a href="/reconstruction"><div class="subpages-title">Capture & <br> Synthesis</div></a>
		<a href="/forensics"><div class="subpages-title">Multi-media <br> Forensics</div></a>
		<span class="dot"></span>
	</section>
	<section class="subpages">
		<a href="/publications"><div class="subpages-title2">Publications</div></a>
		<a href="/teaching"><div class="subpages-title2">Teaching</div></a>
		<a href="/tutorials"><div class="subpages-title2">Tutorials  & <br> Demos</div></a>
	</section>
	<br><br>
	<section class="subpages">
		<a href="/openings"><div class="subpages-title">Openings</div></a>
		<a href="https://ncs.is.tuebingen.mpg.de/" target="_blank"><div class="subpages-title">Group Website &#128279;</div></a>
		<a href="/services"><div class="subpages-title">Research Community Services</div></a>
	</section>
  </header> <!-- End Header -->
  <footer>
    <section class="contact">
      <h3 class="contact-title">Contact me</h3>
      <ul>
        
          <li><a href="https://twitter.com/JustusThies" target="_blank"><i class="fa fa-twitter" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://youtube.com/channel/UCwmSTvnV-sjtlIlNvWYC6Ow" target="_blank"><i class="fa fa-youtube" aria-hidden="true"></i></a></li>
        
        
          <li><a href="https://facebook.com/100012738961011" target="_blank"><i class="fa fa-facebook" aria-hidden="true"></i></a></li>
        
        
          <li class="github"><a href="http://github.com/justusthies" target="_blank"><i class="fa fa-github"></i></a></li>
        
        
        
          <li class="email"><a href="mailto:justus.thies@tuebingen.mpg.de"><i class="fa fa-envelope-o"></i></a></li>
        
      </ul>
    </section> <!-- End Section Contact -->
    <div class="copyright">
        <p>2024 &copy; Justus Thies</p>

        <p>Website is based on <a href="https://github.com/artemsheludko/flexible-jekyll" target="_blank">flexible-jekyll</a>.</p>
    </div>
  </footer> <!-- End Footer -->
</aside> <!-- End Sidebar -->

<div class="content-box clearfix">
  <article class="article-page">
  <div class="page-content"> 
  
	<div class="wrap-content">
		<header class="header-page">
			<h1 class="page-title">Publications</h1>
		</header>
		<div class="page-description">The main theme of my work is to capture and to (re-)synthesize the real world using commodity hardware. It includes the modeling of the human body, tracking, as well as the reconstruction and interaction with the environment. The digitization is needed for various applications in AR/VR as well as in movie (post-)production. Teleconferencing and working in VR is of high interest for many companies ranging from social media platforms to car manufacturer. It enables the remote interaction in VR, e.g., the inspection of 3D content like CAD models or scans from real objects. A realistic reproduction of appearances and motions is key for such applications. Thus, my work is closely related to photo-realistic video synthesis and editing. The development of algorithms for photo-realistic creation or editing of image content comes with a certain responsibility, since the generation of photo-realistic imagery can be misused. That's why I'm also working on the detection of synthetic or manipulated images and videos (Digital Multi-media Forensics).</div>
	</div>


	
	
		<hr>
		<h3 class="post-subtitle">2024</h3>
		<hr>
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/3DiFace/thumb.jpg)" href="/posts/3DiFace/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://balamuruganthambiraja.github.io/" target="_blank" rel="noopener noreferrer">Balamurugan Thambiraja</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://sadegh-aa.github.io/" target="_blank" rel="noopener noreferrer">Sadegh Aliakbarian</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.microsoft.com/en-us/research/people/coskerdarren/" target="_blank" rel="noopener noreferrer">Darren Cosker</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/3DiFace/">3DiFace&#58; Diffusion-based Speech-driven 3D Facial Animation and Editing</a></h2>
		

		<!--<p>We present 3DiFACE, a novel method for personalized speech-driven 3D facial animation and editing. While existing methods deterministically predict facial animations from speech, they overlook...</p>-->
		<p>To enable stochasticity as well as motion editing, we propose a lightweight audio-conditioned diffusion model for 3D facial motion calles 3DiFace. This diffusion model can be trained on a small 3D motion dataset, maintaining expressive lip motion output. In addition, it can be finetuned for specific subjects, requiring only a short video of the person.</p>
		<span class="post-date">2024, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2312.00870" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=-M5Pc2G2O0I" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/3DiFace/thambiraja20243diface.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/haar/thumb.jpg)" href="/posts/haar/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Vanessa Sklyarova</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Egor Zhakharov</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ait.ethz.ch/people/hilliges/" target="_blank" rel="noopener noreferrer">Otmar Hilliges</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/~black" target="_blank" rel="noopener noreferrer">Michael J. Black</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/haar/">HAAR&#58; Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles</a></h2>
		

		<!--<p>We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used...</p>-->
		<p>We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used as production-level assets in modern computer graphics engines.</p>
		<span class="post-date">2024, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2312.11666" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=mR96dqO2j88" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/haar/sklyarova2024haar.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/facetalk/thumb.jpg)" href="/posts/facetalk/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://niessnerlab.org/members/shivangi_aneja/profile.html" target="_blank" rel="noopener noreferrer">Shivangi Aneja</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/facetalk/">FaceTalk&#58; Audio-Driven Motion Diffusion for Neural Parametric Head Models</a></h2>
		

		<!--<p>We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the...</p>-->
		<p>We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences</p>
		<span class="post-date">2024, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2312.08459" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=7Jf0kawrA3Q" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/facetalk/aneja2024facetalk.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/dphm/thumb.jpg)" href="/posts/dphm/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://tangjiapeng.github.io/" target="_blank" rel="noopener noreferrer">Jiapeng Tang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer"></a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.linkedin.com/in/dr-lev-markhasin-5617a112b" target="_blank" rel="noopener noreferrer">Lev Markhasin</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/dphm/">DPHMs&#58; Diffusion Parametric Head Models for Depth-based Tracking</a></h2>
		

		<!--<p>We introduce Diffusion Parametric Head Models (DPHMs), a generative model that enables robust volumetric head reconstruction and tracking from monocular depth sequences. While recent volumetric...</p>-->
		<p>We introduce Diffusion Parametric Head Models (DPHMs), a generative model that enables robust volumetric head reconstruction and tracking from monocular depth sequences.</p>
		<span class="post-date">2024, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2312.01068" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=w_EJ5LDJ7T4" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/dphm/tang2024dphm.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/dega/thumb.jpg)" href="/posts/dega/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://zielon.github.io/" target="_blank" rel="noopener noreferrer">Wojciech Zielonka</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Timur Bagautdinov</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Shunsuke Saito</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Javier Romero</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/dega/">D3GA&#58; Drivable 3D Gaussian Avatars</a></h2>
		

		<!--<p>We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats. Current photorealistic drivable avatars require either...</p>-->
		<p>We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats.</p>
		<span class="post-date">2024, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2311.08581" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=C4IT1gnkaF0" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/dega/zielonka2024dega.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/3VP/thumb.jpg)" href="/posts/3VP/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Jalees Nehvi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Berna Kabadayi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://github.com/julienvalentin" target="_blank" rel="noopener noreferrer">Julien Valentin</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/3VP/">360° Volumetric Portrait Avatar</a></h2>
		

		<!--<p>We propose 360° Volumetric Portrait (3VP) Avatar, a novel method for reconstructing 360° photo-realistic portrait avatars of human subjects solely based on monocular video inputs....</p>-->
		<p>We propose 360° Volumetric Portrait (3VP) Avatar, a novel method for reconstructing 360° photo-realistic portrait avatars of human subjects solely based on monocular video inputs.</p>
		<span class="post-date">2024, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2312.05311" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=hSPnf6Kuyvo" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/3VP/nehvi20243VP.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/esp/thumb.jpg)" href="/posts/esp/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Mirela Ostrek</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Soubhik Sanyal</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Carol O'Sullivan</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/~black" target="_blank" rel="noopener noreferrer">Michael J. Black</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/esp/">Environment-Specific People</a></h2>
		

		<!--<p>Despite significant progress in generative image synthesis and full-body generation in particular, state-of-the-art methods are either context-independent, overly reliant to text prompts, or bound to...</p>-->
		<p>We present ESP, a novel method for context-aware full-body generation, that enables photo-realistic inpainting of people into existing "in-the-wild" photographs.</p>
		<span class="post-date">2024, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2312.14579" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/esp/ostrek2024esp.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/ganavatar/thumb.jpg)" href="/posts/ganavatar/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Berna Kabadayi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zielon.github.io/" target="_blank" rel="noopener noreferrer">Wojciech Zielonka</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer"></a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Gerard Pons-Moll</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/ganavatar/">GAN-Avatar&#58; Controllable Personalized GAN-based Human Head Avatars</a></h2>
		

		<!--<p>Digital humans and, especially, 3D facial avatars have raised a lot of attention in the past years, as they are the backbone of several applications...</p>-->
		<p>We propose to learn person-specific animatable avatars from images without assuming to have access to precise facial expression tracking. At the core of our method, we leverage a 3D-aware generative model that is trained to reproduce the distribution of facial expressions from the training data.</p>
		<span class="post-date">2023, Dec 31&nbsp;&nbsp;&nbsp;—&nbsp; 3DV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2311.13655" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=uAi5IVrzzZY" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/ganavatar/kabadayi2024ganavatar.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/diffuscene/thumb.jpg)" href="/posts/diffuscene/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://tangjiapeng.github.io/" target="_blank" rel="noopener noreferrer">Jiapeng Tang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer"></a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.linkedin.com/in/dr-lev-markhasin-5617a112b" target="_blank" rel="noopener noreferrer">Lev Markhasin</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/diffuscene/">DiffuScene&#58; Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis</a></h2>
		

		<!--<p>We present DiffuScene for indoor 3D scene synthesis based on a novel scene graph denoising diffusion probabilistic model, which generates 3D instance properties stored in...</p>-->
		<p>We present DiffuScene, a diffusion-based method for indoor 3D scene synthesis which operates on a 3D scene graph representation.</p>
		<span class="post-date">2023, Dec 31&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2303.14207" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=VkBey2ZHA6E" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/diffuscene/tang2024diffuscene.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2023</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/sculpt/thumb.jpg)" href="/posts/sculpt/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Soubhik Sanyal</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Partha Ghosh</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Jinlong Yangy</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/~black" target="_blank" rel="noopener noreferrer">Michael J. Black</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://sites.google.com/site/bolkartt/" target="_blank" rel="noopener noreferrer">Timo Bolkart</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/sculpt/">SCULPT&#58; Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes</a></h2>
		

		<!--<p>We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns...</p>-->
		<p>We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns to represent the geometry and appearance distribution of clothed human bodies.</p>
		<span class="post-date">2023, Dec 31&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2308.10638" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/sculpt/sanyal2023sculpt.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/imitator/thumb.jpg)" href="/posts/imitator/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://balamuruganthambiraja.github.io/" target="_blank" rel="noopener noreferrer">Balamurugan Thambiraja</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~ihabibie/" target="_blank" rel="noopener noreferrer">Ikhsanul Habibie</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://sadegh-aa.github.io/" target="_blank" rel="noopener noreferrer">Sadegh Aliakbarian</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.microsoft.com/en-us/research/people/coskerdarren/" target="_blank" rel="noopener noreferrer">Darren Cosker</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/imitator/">Imitator&#58; Personalized Speech-driven 3D Facial Animation</a></h2>
		

		<!--<p>Speech-driven 3D facial animation has been widely explored, with applications in gaming, character animation, virtual reality, and telepresence systems. State-of-the-art methods deform the face topology...</p>-->
		<p>We present Imitator, a speech-driven facial expression synthesis method, which learns identity-specific details from a short input video and produces novel facial expressions matching the identity-specific speaking style and facial idiosyncrasies of the target actor. Specifically, we train a style-agnostic transformer on a large facial expression dataset which we use as a prior for audio-driven facial expressions. Based on this prior, we optimize for identity-specific speaking style based on a short reference video.</p>
		<span class="post-date">2023, Aug 14&nbsp;&nbsp;&nbsp;—&nbsp; ICCV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2301.00023" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=Ij_z4JvatnA" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/imitator/thambiraja2023imitator.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/caphy/thumb.jpg)" href="/posts/caphy/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://suzhaoqi.github.io/" target="_blank" rel="noopener noreferrer">Zhaoqi Su</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://huliangxiao.github.io/" target="_blank" rel="noopener noreferrer">Liangxiao Hu</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://jsnln.github.io/" target="_blank" rel="noopener noreferrer">Siyou Lin</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://hongwenzhang.github.io/" target="_blank" rel="noopener noreferrer">Hongwen Zhang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Shengping Zhang</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.liuyebin.com/" target="_blank" rel="noopener noreferrer">Yebin Liu</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/caphy/">CaPhy&#58; Capturing Physical Properties for Animatable Human Avatars</a></h2>
		

		<!--<p>We present CaPhy, a novel method for reconstructing animatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geometric and...</p>-->
		<p>We present CaPhy, a novel method for reconstructing animatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geometric and physical properties of the clothing from real observations. This allows us to apply novel poses to the human avatar with physically correct deformations and wrinkles of the clothing.</p>
		<span class="post-date">2023, Aug 14&nbsp;&nbsp;&nbsp;—&nbsp; ICCV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2308.05925" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=kTus5NP7H6E" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/caphy/su2023caphy.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/tada/thumb.jpg)" href="/posts/tada/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://github.com/tingtingliao" target="_blank" rel="noopener noreferrer">Tingting Liao</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://xyyhw.top/" target="_blank" rel="noopener noreferrer">Hongwei Yi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://xiuyuliang.cn/" target="_blank" rel="noopener noreferrer">Yuliang Xiu</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://me.kiui.moe/" target="_blank" rel="noopener noreferrer">Jiangxiang Tang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Yangyi Huang</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/~black" target="_blank" rel="noopener noreferrer">Michael J. Black</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/tada/">TADA&#58; Text to Animatable Digital Avatars</a></h2>
		

		<!--<p>We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated...</p>-->
		<p>We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines.</p>
		<span class="post-date">2023, Aug 13&nbsp;&nbsp;&nbsp;—&nbsp; 3DV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2308.10899" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=w5rdcPQWktE" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/tada/liao2023tada.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/tech/thumb.jpg)" href="/posts/tech/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Yangyi Huang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://xyyhw.top/" target="_blank" rel="noopener noreferrer">Hongwei Yi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://xiuyuliang.cn/" target="_blank" rel="noopener noreferrer">Yuliang Xiu</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://github.com/tingtingliao" target="_blank" rel="noopener noreferrer">Tingting Liao</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://me.kiui.moe/" target="_blank" rel="noopener noreferrer">Jiangxiang Tang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.cad.zju.edu.cn/home/dengcai/" target="_blank" rel="noopener noreferrer">Deng Cai</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/tech/">TeCH&#58; Text-guided Reconstruction of Lifelike Clothed Humans</a></h2>
		

		<!--<p>Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the “unseen regions” with high-level details remains an unsolved challenge that...</p>-->
		<p>TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the "indescribable" appearance.</p>
		<span class="post-date">2023, Aug 13&nbsp;&nbsp;&nbsp;—&nbsp; 3DV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2308.08545" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=SjzQ6158Pho" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/tech/huang2023tech.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/polface/thumb.jpg)" href="/posts/polface/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/dejan_azinovic/profile.html" target="_blank" rel="noopener noreferrer">Dejan Azinovic</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Olivier Maury</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Christophe Hery</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/polface/">High-Res Facial Appearance Capture from Polarized Smartphone Images</a></h2>
		

		<!--<p>We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we...</p>-->
		<p>We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil.</p>
		<span class="post-date">2023, Mar 30&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2212.01160" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=xkLZ824jYjg" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/polface/azinovic2023polface.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/clipface/thumb.jpg)" href="/posts/clipface/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://niessnerlab.org/members/shivangi_aneja/profile.html" target="_blank" rel="noopener noreferrer">Shivangi Aneja</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/clipface/">ClipFace&#58; Text-guided Editing of Textured 3D Morphable Models</a></h2>
		

		<!--<p>We propose ClipFace, a novel self-supervised approach for text-guided editing of textured 3D morphable model of faces. Specifically, we employ user-friendly language prompts to enable...</p>-->
		<p>ClipFace is a novel self-supervised approach for text-guided editing of textured 3D morphable model of faces. Controllable editing and manipulation are given by language prompts to adapt texture and expression of the 3D morphable model.</p>
		<span class="post-date">2023, Mar 30&nbsp;&nbsp;&nbsp;—&nbsp; SIGGRAPH —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2212.01406" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=toGOQqFuNmA" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/clipface/aneja2022clipface.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/mime/thumb.jpg)" href="/posts/mime/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://xyyhw.top/" target="_blank" rel="noopener noreferrer">Hongwei Yi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://is.mpg.de/person/chuang2" target="_blank" rel="noopener noreferrer">Chun-Hao P. Huang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Shashank Tripathi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Lea Hering</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/~black" target="_blank" rel="noopener noreferrer">Michael J. Black</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/mime/">MIME&#58; Human-Aware 3D Scene Generation</a></h2>
		

		<!--<p>Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and...</p>-->
		<p>Humans constantly interact with their environment. They walk through a room, touch objects, rest on a chair, or sleep in a bed. All these interactions contain information about the scene layout and object placement which we leverage to generate scenes from human motion.</p>
		<span class="post-date">2023, Mar 29&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2212.04360" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
					<span class="post-video-link"><a href="https://download.is.tue.mpg.de/mime/perceivingsyste%20-%20MIME-1202_v2.mp4" target="_blank">[Video]</a>&nbsp;</span>
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/mime/yi2023mime.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/insta/thumb.jpg)" href="/posts/insta/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://zielon.github.io/" target="_blank" rel="noopener noreferrer">Wojciech Zielonka</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://sites.google.com/site/bolkartt/" target="_blank" rel="noopener noreferrer">Timo Bolkart</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/insta/">INSTA&#58; Instant Volumetric Head Avatars</a></h2>
		

		<!--<p>For immersive telepresence in AR or VR, we aim for digital humans (avatars) that mimic the motions and facial expressions of the actual subjects participating...</p>-->
		<p>Instead of prerecorded, old avatars, we aim to instantaneously reconstruct the subject's look to capture the actual appearance during a meeting. To this end, we propose INSTA, which enables the reconstruction of an avatar within a few minutes (~10 min) and can be driven at interactive frame rates.</p>
		<span class="post-date">2023, Mar 28&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2211.16630" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=HOgaeWTih7Q" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/insta/zielonka2023insta.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/diner/thumb.jpg)" href="/posts/diner/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://ncs.is.mpg.de/person/mprinzler" target="_blank" rel="noopener noreferrer">Malte Prinzler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ait.ethz.ch/people/hilliges/" target="_blank" rel="noopener noreferrer">Otmar Hilliges</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/diner/">DINER&#58; Depth-aware Image-based NEural Radiance Fields</a></h2>
		

		<!--<p>We present Depth-aware Image-based NEural Radiance fields (DINER). Given a sparse set of RGB input views, we predict depth and feature maps to guide the...</p>-->
		<p>Given a sparse set of RGB input views, we predict depth and feature maps to guide the reconstruction of a volumetric scene representation that allows us to render 3D objects under novel views.</p>
		<span class="post-date">2023, Mar 28&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2211.16630" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=iI_fpjY5k8Y" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/diner/prinzler2023diner.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2022</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/defprior/thumb.jpg)" href="/posts/defprior/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://tangjiapeng.github.io/" target="_blank" rel="noopener noreferrer">Jiapeng Tang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.linkedin.com/in/dr-lev-markhasin-5617a112b" target="_blank" rel="noopener noreferrer">Lev Markhasin</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Bi Wang</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/defprior/">Neural Deformation Priors</a></h2>
		

		<!--<p>We present Neural Shape Deformation Priors, a novel method for shape manipulation that predicts mesh deformations of non-rigid objects from user-provided handle movements. State-of-the-art methods...</p>-->
		<p>We present Neural Shape Deformation Priors, a novel method for shape manipulation that predicts mesh deformations of non-rigid objects from user-provided handle movements.</p>
		<span class="post-date">2022, Dec 01&nbsp;&nbsp;&nbsp;—&nbsp; NeurIPS —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2210.05616" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=neKuf85H0nE" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/defprior/tang2022defprior.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/mica/thumb.jpg)" href="/posts/mica/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://zielon.github.io/" target="_blank" rel="noopener noreferrer">Wojciech Zielonka</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://sites.google.com/site/bolkartt/" target="_blank" rel="noopener noreferrer">Timo Bolkart</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/mica/">MICA&#58; Towards Metrical Reconstruction of Human Faces</a></h2>
		

		<!--<p>Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely...</p>-->
		<p>Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context. Thus, we present MICA, a novel metrical face reconstruction method that combines face recognition with supervised face shape learning.</p>
		<span class="post-date">2022, Jul 04&nbsp;&nbsp;&nbsp;—&nbsp; ECCV —&nbsp;</span>
		<!--<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2204.06607" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=vzzEbvv08VA" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/mica/zielonka2022mica.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/texturify/thumb.jpg)" href="/posts/texturify/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/yawar_siddiqui/profile.html" target="_blank" rel="noopener noreferrer">Yawar Siddiqui</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://fangchangma.github.io/" target="_blank" rel="noopener noreferrer">Fangchang Ma</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://shanqi.github.io/" target="_blank" rel="noopener noreferrer">Qi Shan</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/texturify/">Texturify&#58; Generating Textures on 3D Shape Surfaces</a></h2>
		

		<!--<p>Texture cues on 3D objects are key to compelling visual representations, with the possibility to create high visual fidelity with inherent spatial consistency across different...</p>-->
		<p>Texturify learns to generate geometry-aware textures for untextured collections of 3D objects. Our method trains from only a collection of images and a collection of untextured shapes, which are both often available, without requiring any explicit 3D color supervision or shape-image correspondence. Textures are created directly on the surface of a given 3D shape, enabling generation of high-quality, compelling textured 3D shapes.</p>
		<span class="post-date">2022, Jul 04&nbsp;&nbsp;&nbsp;—&nbsp; ECCV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2204.02411" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=M5OU_fiD3Jk" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/texturify/siddiqui2022texturify.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/neuralhead/thumb.jpg)" href="/posts/neuralhead/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://hci.iwr.uni-heidelberg.de/vislearn/people" target="_blank" rel="noopener noreferrer">Philip-William Grassal</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ncs.is.mpg.de/person/mprinzler" target="_blank" rel="noopener noreferrer">Malte Prinzler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://titus-leistner.de/pages/about-me.html" target="_blank" rel="noopener noreferrer">Titus Leistner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/" target="_blank" rel="noopener noreferrer">Carsten Rother</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/neuralhead/">Neural Head Avatars from Monocular RGB Videos</a></h2>
		

		<!--<p>We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be...</p>-->
		<p>We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar using a deep neural network. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture.</p>
		<span class="post-date">2022, Mar 22&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2112.01554.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=S7LY1DtJCsI" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/neuralhead/grassal2022neuralhead.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/rgbdnerf/thumb.jpg)" href="/posts/rgbdnerf/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/dejan_azinovic/profile.html" target="_blank" rel="noopener noreferrer">Dejan Azinovic</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.ricardomartinbrualla.com/" target="_blank" rel="noopener noreferrer">Ricardo Martin-Brualla</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.danbgoldman.com/" target="_blank" rel="noopener noreferrer">Dan B Goldman</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/rgbdnerf/">Neural RGB-D Surface Reconstruction</a></h2>
		

		<!--<p>In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance...</p>-->
		<p>We demonstrate how depth measurements can be incorporated into the neural radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone.</p>
		<span class="post-date">2022, Mar 22&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://dazinovic.github.io/neural-rgbd-surface-reconstruction/" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=iWuSowPsC3g" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/rgbdnerf/azinovic2021rgbdnerf.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/mover/thumb.jpg)" href="/posts/mover/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://xyyhw.top/" target="_blank" rel="noopener noreferrer">Hongwei Yi</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://is.mpg.de/person/chuang2" target="_blank" rel="noopener noreferrer">Chun-Hao P. Huang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/~dtzionas" target="_blank" rel="noopener noreferrer">Dimitrios Tzionas</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/person/mkocabas" target="_blank" rel="noopener noreferrer">Muhammed Kocabas</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/person/mhassan" target="_blank" rel="noopener noreferrer">Mohamed Hassan</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://inf.ethz.ch/people/person-detail.MjYyNzgw.TGlzdC8zMDQsLTg3NDc3NjI0MQ==.html" target="_blank" rel="noopener noreferrer">Siyu Tang</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ps.is.mpg.de/~black" target="_blank" rel="noopener noreferrer">Michael J. Black</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/mover/">Mover&#58; Human-Aware Object Placement for Visual Environment Reconstruction</a></h2>
		

		<!--<p>Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information...</p>-->
		<p>We demonstrate that human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout.</p>
		<span class="post-date">2022, Mar 22&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2203.03609.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=n_ejtZarRaM" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/mover/yi2022mover.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/advancedneuralrenderingstar/thumb.jpg)" href="/posts/advancedneuralrenderingstar/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://bmild.github.io/" target="_blank" rel="noopener noreferrer">Ben Mildenhall</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://pratulsrinivasan.github.io/" target="_blank" rel="noopener noreferrer">Pratul_Srinivasan</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~tretschk/" target="_blank" rel="noopener noreferrer">Edgar Tretschk</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://yifita.github.io/" target="_blank" rel="noopener noreferrer">Yifan Wanf</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://christophlassner.de/" target="_blank" rel="noopener noreferrer">Christoph Lassner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://vsitzmann.github.io/" target="_blank" rel="noopener noreferrer">Vincent Sitzmann</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.ricardomartinbrualla.com/" target="_blank" rel="noopener noreferrer">Ricardo Martin-Brualla</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer"></a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.cs.cmu.edu/~tsimon/" target="_blank" rel="noopener noreferrer">Tomas Simon</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://jonbarron.info/" target="_blank" rel="noopener noreferrer">Jonathan T. Barron</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://stanford.edu/~gordonwz/" target="_blank" rel="noopener noreferrer">Gordon Wetzstein</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~golyanik/" target="_blank" rel="noopener noreferrer">Vladislav Golyanik</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/advancedneuralrenderingstar/">Advances in Neural Rendering</a></h2>
		

		<!--<p>Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of...</p>-->
		<p>This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene.</p>
		<span class="post-date">2022, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; Eurographics —&nbsp;</span>
		<!--<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2111.05849.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/advancedneuralrenderingstar/tewari2021advances.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2021</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/transfusion/thumb.jpg)" href="/posts/transfusion/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/aljaz_bozic/profile.html" target="_blank" rel="noopener noreferrer">Aljaz Bozic</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://pablorpalafox.github.io" target="_blank" rel="noopener noreferrer">Pablo Palafox</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/transfusion/">TransformerFusion&#58; Monocular RGB Scene Reconstruction using Transformers</a></h2>
		

		<!--<p>We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that...</p>-->
		<p>We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation.</p>
		<span class="post-date">2021, Jul 12&nbsp;&nbsp;&nbsp;—&nbsp; NeurIPS 2021 —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2107.02191.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=LIpTKYfKSqw" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/transfusion/bozic2021transfusion.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/dsfn/thumb.jpg)" href="/posts/dsfn/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andrei_burov/profile.html" target="_blank" rel="noopener noreferrer">Andrei Burov</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/dsfn/">Dynamic Surface Function Networks for Clothed Human Bodies</a></h2>
		

		<!--<p>We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model...</p>-->
		<p>We present a novel method for temporal coherent reconstruction and tracking of clothed humans using dynamic surface function networks which can be trained with a monocular RGB-D sequence.</p>
		<span class="post-date">2021, Apr 12&nbsp;&nbsp;&nbsp;—&nbsp; ICCV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2104.03978.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=4wbSi9Sqdm4" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/dsfn/burov2021dsfn.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/npm/thumb.jpg)" href="/posts/npm/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://pablorpalafox.github.io" target="_blank" rel="noopener noreferrer">Pablo Palafox</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/aljaz_bozic/profile.html" target="_blank" rel="noopener noreferrer">Aljaz Bozic</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/npm/">Neural Parametric Models for 3D Deformable Shapes</a></h2>
		

		<!--<p>Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the...</p>-->
		<p>We propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints.</p>
		<span class="post-date">2021, Apr 12&nbsp;&nbsp;&nbsp;—&nbsp; ICCV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2104.00702.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=muZXXgkkMPY" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/npm/palafox2021npm.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/retrievalfuse/thumb.jpg)" href="/posts/retrievalfuse/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/yawar_siddiqui/profile.html" target="_blank" rel="noopener noreferrer">Yawar Siddiqui</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://fangchangma.github.io/" target="_blank" rel="noopener noreferrer">Fangchang Ma</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://shanqi.github.io/" target="_blank" rel="noopener noreferrer">Qi Shan</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/retrievalfuse/">RetrievalFuse&#58; Neural 3D Scene Reconstruction with a Database</a></h2>
		

		<!--<p>3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In...</p>-->
		<p>In this paper, we introduce a new method that directly leverages scene geometry from the training database. It is able to reconstruct a high quality scene from pointcloud or low-res inputs using geometry patches from a database and an attention-based refinement.</p>
		<span class="post-date">2021, Apr 12&nbsp;&nbsp;&nbsp;—&nbsp; ICCV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="http://arxiv.org/pdf/2104.00024" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=HbsUU0YODqE" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/retrievalfuse/siddiqui2021retrievalfuse.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/nerface/thumb.jpg)" href="/posts/nerface/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/guy_gafni/profile.html" target="_blank" rel="noopener noreferrer">Guy Gafni</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/nerface/">NerFACE&#58; Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction</a></h2>
		

		<!--<p>We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a...</p>-->
		<p>We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup.</p>
		<span class="post-date">2021, Mar 03&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2012.03065.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=sUjULZ0gDpU" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/nerface/gafni2020nerface.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/neuraldeformationgraphs/thumb.jpg)" href="/posts/neuraldeformationgraphs/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/aljaz_bozic/profile.html" target="_blank" rel="noopener noreferrer">Aljaz Bozic</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://pablorpalafox.github.io" target="_blank" rel="noopener noreferrer">Pablo Palafox</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/neuraldeformationgraphs/">Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction</a></h2>
		

		<!--<p>We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep...</p>-->
		<p>We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking.</p>
		<span class="post-date">2021, Mar 03&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2012.01451.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=30HQk2Av6ds" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/neuraldeformationgraphs/bozic2020neuraldeformationgraphs.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/spsg/thumb.jpg)" href="/posts/spsg/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/yawar_siddiqui/profile.html" target="_blank" rel="noopener noreferrer">Yawar Siddiqui</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://github.com/julienvalentin" target="_blank" rel="noopener noreferrer">Julien Valentin</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/spsg/">SPSG&#58; Self-Supervised Photometric Scene Generation from RGB-D Scans</a></h2>
		

		<!--<p>We present Self-Supervised Photometric Scene Generation (SPSG), a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to...</p>-->
		<p>We present a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion.</p>
		<span class="post-date">2021, Mar 02&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2006.14660.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=1cj962m9zqo" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/spsg/dai2020spsg.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/spoc/thumb.jpg)" href="/posts/spoc/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/davide_cozzolino.html" target="_blank" rel="noopener noreferrer">Davide Cozzolino</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andreas_roessler/profile.html" target="_blank" rel="noopener noreferrer">Andreas Rössler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/verdoliva.html" target="_blank" rel="noopener noreferrer">Luisa Verdoilva</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/spoc/">SpoC&#58; Spoofing Camera Fingerprints</a></h2>
		

		<!--<p>Thanks to the fast progress in synthetic media generation, creating realistic false images has become very easy. Such images can be used to wrap “rich”...</p>-->
		<p>In this paper, we challenge forensic forgery detectors that are based on camera fingerprints (i.e., traces of the image capturing and processing pipeline) to gain insights into their vulnerabilities.</p>
		<span class="post-date">2021, Jan 26&nbsp;&nbsp;&nbsp;—&nbsp; WMF2021 —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1911.12069.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/spoc/cozzolino2021spoc.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/idreveal/teaser.jpg)" href="/posts/idreveal/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/davide_cozzolino.html" target="_blank" rel="noopener noreferrer">Davide Cozzolino</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andreas_roessler/profile.html" target="_blank" rel="noopener noreferrer">Andreas Rössler</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/verdoliva.html" target="_blank" rel="noopener noreferrer">Luisa Verdoilva</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/idreveal/">ID-Reveal&#58; Identity-aware DeepFake Video Detection</a></h2>
		

		<!--<p>State-of-the-art DeepFake forgery detectors are trained in a supervised fashion to answer the question ‘is this video real or fake?’. Given that their training is...</p>-->
		<p>We introduce the DeepFake detection approach ID-Reveal, which is based on learned temporal facial features, specific of how each person moves while talking, by means of metric learning coupled with an adversarial training strategy.</p>
		<span class="post-date">2021, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; ICCV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2012.02512.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=RsFxsOLvRdY" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/idreveal/cozzolino2020idreveal.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2020</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/neuraltracking/thumb.jpg)" href="/posts/neuraltracking/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/aljaz_bozic/profile.html" target="_blank" rel="noopener noreferrer">Aljaz Bozic</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://pablorpalafox.github.io" target="_blank" rel="noopener noreferrer">Pablo Palafox</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/neuraltracking/">Neural Non-Rigid Tracking</a></h2>
		

		<!--<p>We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction. Given two input RGB-D frames of a non-rigidly moving object, we...</p>-->
		<p>We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction. By enabling gradient back-propagation through a non-rigid as-rigid-as-possible optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking.</p>
		<span class="post-date">2020, Sep 29&nbsp;&nbsp;&nbsp;—&nbsp; NeurIPS —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/abs/2006.13240" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=Kj_P-lHtWkU" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/neuraltracking/bozic2020neuraltracking.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/egochat/thumb.jpg)" href="/posts/egochat/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://people.mpi-inf.mpg.de/~elgharib/" target="_blank" rel="noopener noreferrer">Mohamed Elgharib</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://gvv.mpi-inf.mpg.de/GVV_Team.html" target="_blank" rel="noopener noreferrer">Mohit Mendiratta</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~hpseidel/english.html" target="_blank" rel="noopener noreferrer">Hans-Peter Seidel</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~golyanik/" target="_blank" rel="noopener noreferrer">Vladislav Golyanik</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/egochat/">Egocentric Videoconferencing</a></h2>
		

		<!--<p>We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices. Videoconferencing portrays...</p>-->
		<p>We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices.</p>
		<span class="post-date">2020, Sep 28&nbsp;&nbsp;&nbsp;—&nbsp; SIGGRAPH ASIA —&nbsp;</span>
		<!--<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="http://gvv.mpi-inf.mpg.de/projects/EgoChat/data/Main.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
					<span class="post-video-link"><a href="http://gvv.mpi-inf.mpg.de/projects/EgoChat/data/Main.mp4" target="_blank">[Video]</a>&nbsp;</span>
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/egochat/elgharib2020egochat.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/learningadaptivesampling/thumb.jpg)" href="/posts/learningadaptivesampling/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://www.in.tum.de/cg/people/weiss/" target="_blank" rel="noopener noreferrer">Sebastian Weiss</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer">Mustafa Isik</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.in.tum.de/cg/cover-page/" target="_blank" rel="noopener noreferrer">Rüdiger Westermann</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/learningadaptivesampling/">Learning Adaptive Sampling and Reconstruction for Volume Visualization</a></h2>
		

		<!--<p>A central challenge in data visualization is to understand which data samples are required to generate an image of a data set in which the...</p>-->
		<p>We introduce a novel neural rendering pipeline, which is trained end-to-end to generate a sparse adaptive sampling structure from a given low-resolution input image, and reconstructs a high-resolution image from the sparse set of samples.</p>
		<span class="post-date">2020, Jul 22&nbsp;&nbsp;&nbsp;—&nbsp; TVCG —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2007.10093.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/learningadaptivesampling/weiss2020learningadaptivesampling.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/intrinsicautoencoder/thumb.jpg)" href="/posts/intrinsicautoencoder/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://hassanhaija.github.io/" target="_blank" rel="noopener noreferrer">Hassan Abu Alhaija</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://sivakm.github.io/" target="_blank" rel="noopener noreferrer">Siva Karthik Mustikovela</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.cvlibs.net/" target="_blank" rel="noopener noreferrer">Andreas Geiger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://hci.iwr.uni-heidelberg.de/vislearn/people/carsten-rother/" target="_blank" rel="noopener noreferrer">Carsten Rother</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/intrinsicautoencoder/">Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition</a></h2>
		

		<!--<p>Neural rendering techniques promise efficient photo-realistic image synthesis while at the same time providing rich control over scene parameters by learning the physical image formation...</p>-->
		<p>We propose an autoencoder for joint generation of realistic images from synthetic 3D models while simultaneously decomposing real images into their intrinsic shape and appearance properties.</p>
		<span class="post-date">2020, Jun 23&nbsp;&nbsp;&nbsp;—&nbsp; 3DV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2006.16011.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/intrinsicautoencoder/alhaija2020intrinsicae.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/neuralrenderingstar/thumb.jpg)" href="/posts/neuralrenderingstar/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.ohadf.com/" target="_blank" rel="noopener noreferrer">Ohad Fried</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://vsitzmann.github.io/" target="_blank" rel="noopener noreferrer">Vincent Sitzmann</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="" target="_blank" rel="noopener noreferrer"></a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.kalyans.org/" target="_blank" rel="noopener noreferrer">Kalyan Sunkavalli</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.ricardomartinbrualla.com/" target="_blank" rel="noopener noreferrer">Ricardo Martin-Brualla</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.cs.cmu.edu/~tsimon/" target="_blank" rel="noopener noreferrer">Tomas Simon</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://jsaragih.org/Home_Page.html" target="_blank" rel="noopener noreferrer">Jason Saragih</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://research.google/people/106687/" target="_blank" rel="noopener noreferrer">Rohit K Pandey</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.seanfanello.it/" target="_blank" rel="noopener noreferrer">Sean Fanello</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://stanford.edu/~gordonwz/" target="_blank" rel="noopener noreferrer">Gordon Wetzstein</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.csail.mit.edu/junyanz/" target="_blank" rel="noopener noreferrer">Jun-Yan Zhu</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://graphics.stanford.edu/~maneesh/" target="_blank" rel="noopener noreferrer">Maneesh Agrawala</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://research.adobe.com/person/eli-shechtman/" target="_blank" rel="noopener noreferrer">Eli Shechtman</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.danbgoldman.com/" target="_blank" rel="noopener noreferrer">Dan B Goldman</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/neuralrenderingstar/">State of the Art on Neural Rendering</a></h2>
		

		<!--<p>Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted...</p>-->
		<p>Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. This state-of-the-art report summarizes the recent trends and applications of neural rendering.</p>
		<span class="post-date">2020, Apr 08&nbsp;&nbsp;&nbsp;—&nbsp; Eurographics —&nbsp;</span>
		<!--<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/2004.03805.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/neuralrenderingstar/tewari2020neuralrendering.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/advtex/thumb.jpg)" href="/posts/advtex/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://stanford.edu/~jingweih/" target="_blank" rel="noopener noreferrer">Jingwei Huang</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://angeladai.github.io" target="_blank" rel="noopener noreferrer">Angela Dai</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://abhijitkundu.info/" target="_blank" rel="noopener noreferrer">Abhijit Kundu</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.maxjiang.ml/" target="_blank" rel="noopener noreferrer">Chiyu 'Max' Jiang</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://geometry.stanford.edu/member/guibas/" target="_blank" rel="noopener noreferrer">Leonidas Guibas</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs.princeton.edu/~funk/" target="_blank" rel="noopener noreferrer">Thomas Funkhouser</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/advtex/">Adversarial Texture Optimization from RGB-D Scans</a></h2>
		

		<!--<p>Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera...</p>-->
		<p>We present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views. Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors.</p>
		<span class="post-date">2020, Mar 19&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="http://stanford.edu/~jingweih/papers/advtex/supp/paper.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=52xlRn0ESek" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/advtex/huang2020advtex.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/ignor/thumb.jpg)" href="/posts/ignor/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/ignor/">Image-guided Neural Object Rendering</a></h2>
		

		<!--<p>We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to...</p>-->
		<p>We propose a new learning-based novel view synthesis approach for scanned objects that is trained based on a set of multi-view images, where we directly train a deep neural network to synthesize a view-dependent image of an object.</p>
		<span class="post-date">2020, Jan 15&nbsp;&nbsp;&nbsp;—&nbsp; ICLR —&nbsp;</span>
		<!--<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1811.10720.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=S9yhekwyAiA" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/ignor/thies2020ignor.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/neural-voice-puppetry/thumb.jpg)" href="/posts/neural-voice-puppetry/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://people.mpi-inf.mpg.de/~elgharib/" target="_blank" rel="noopener noreferrer">Mohamed Elgharib</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/neural-voice-puppetry/">Neural Voice Puppetry&#58; <br> Audio-driven Facial Reenactment</a></h2>
		

		<!--<p>We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis. Given an audio sequence of a source person or digital assistant, we...</p>-->
		<p>Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input.</p>
		<span class="post-date">2020, Jan 08&nbsp;&nbsp;&nbsp;—&nbsp; ECCV —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1912.05566.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=iFJuskGpWJw" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/neural-voice-puppetry/thies2020nvp.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2019</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/faceforensics++/thumb.jpg)" href="/posts/faceforensics++/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andreas_roessler/profile.html" target="_blank" rel="noopener noreferrer">Andreas Rössler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/davide_cozzolino.html" target="_blank" rel="noopener noreferrer">Davide Cozzolino</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/verdoliva.html" target="_blank" rel="noopener noreferrer">Luisa Verdoilva</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs1.tf.fau.de/christian-riess/" target="_blank" rel="noopener noreferrer">Christian Riess</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/faceforensics++/">FaceForensics++&#58; <br> Learning to Detect Manipulated Facial Images</a></h2>
		

		<!--<p>The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society....</p>-->
		<p>In this paper, we examine the realism of state-of-the-art facial image manipulation methods, and how difficult it is to detect them - either automatically or by humans. In particular, we create a datasets that is focused on DeepFakes, Face2Face, FaceSwap, and Neural Textures as prominent representatives for facial manipulations.</p>
		<span class="post-date">2019, Aug 26&nbsp;&nbsp;&nbsp;—&nbsp; ICCV —&nbsp;</span>
		<!--<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1901.08971.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=iwJocDEnL3E" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/faceforensics++/roessler2019faceforensicspp.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/deferred-neural-rendering/thumb.jpg)" href="/posts/deferred-neural-rendering/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
		<h2 class="post-title"><a href="/posts/deferred-neural-rendering/">Deferred Neural Rendering&#58; <br> Image Synthesis using Neural Textures</a></h2>
		

		<!--<p>The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we...</p>-->
		<p>Deferred Neural Rendering is a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable Neural Textures. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect.</p>
		<span class="post-date">2019, Apr 28&nbsp;&nbsp;&nbsp;—&nbsp; SIGGRAPH —&nbsp;</span>
		<!--<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1904.12356.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=ofVgAEb1FiE" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/deferred-neural-rendering/thies2019neural.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/deepvoxels/thumb.jpg)" href="/posts/deepvoxels/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://vsitzmann.github.io/" target="_blank" rel="noopener noreferrer">Vincent Sitzmann</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs.princeton.edu/~fheide/" target="_blank" rel="noopener noreferrer">Felix Heide</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://stanford.edu/~gordonwz/" target="_blank" rel="noopener noreferrer">Gordon Wetzstein</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/deepvoxels/">DeepVoxels&#58; Learning Persistent 3D Feature Embeddings</a></h2>
		

		<!--<p>In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To...</p>-->
		<p>In this work, we address the lack of 3D understanding of generative neural networks by introducing a persistent 3D feature embedding for view synthesis. To this end, we propose DeepVoxels, a learned representation that encodes the view-dependent appearance of a 3D object without having to explicitly model its geometry.</p>
		<span class="post-date">2019, Apr 11&nbsp;&nbsp;&nbsp;—&nbsp; CVPR —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1812.01024.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=HM_WsZhoGXw" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/deepvoxels/sitzmann2019deepvoxels.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/acm-research-highlight/thumb.png)" href="/posts/acm-research-highlight/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/acm-research-highlight/">Research Highlight&#58; Face2Face</a></h2>
		

		<!--<p>Face2Face is an approach for real-time facial reenactment of a monocular target video sequence (e.g., Youtube video). The source sequence is also a monocular video...</p>-->
		<p>Research highlight of the Face2Face approach featured on the cover of Communications of the ACM in January 2019. Face2Face is an approach for real-time facial reenactment of a monocular target video. The method had significant impact in the research community and far beyond; it won several wards, e.g., Siggraph ETech Best in Show Award, it was featured in countless media articles, e.g., NYT, WSJ, Spiegel, etc., and it had a massive reach on social media with millions of views.</p>
		<span class="post-date">2019, Jan 01&nbsp;&nbsp;&nbsp;—&nbsp; Research Highlight —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://dl.acm.org/citation.cfm?id=3301004.3292039&coll=portal&dl=ACM" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=KUjn6SrNbSo" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/acm-research-highlight/thies2018face.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
			<br>
			<hr>
			<h3 class="post-subtitle">2018</h3>
			<hr>
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/forensictransfer/thumb.jpg)" href="/posts/forensictransfer/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/davide_cozzolino.html" target="_blank" rel="noopener noreferrer">Davide Cozzolino</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://niessnerlab.org/members/andreas_roessler/profile.html" target="_blank" rel="noopener noreferrer">Andreas Rössler</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.cs1.tf.fau.de/christian-riess/" target="_blank" rel="noopener noreferrer">Christian Riess</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="http://www.grip.unina.it/people/userprofile/verdoliva.html" target="_blank" rel="noopener noreferrer">Luisa Verdoilva</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/forensictransfer/">ForensicTransfer&#58; Weakly-supervised Domain Adaptation for Forgery Detection</a></h2>
		

		<!--<p>Distinguishing fakes from real images is becoming increasingly difficult as new sophisticated image manipulation approaches come out by the day. Convolutional neural networks (CNN) show...</p>-->
		<p>ForensicTransfer tackles two challenges in multimedia forensics. First, we devise a learning-based forensic detector which adapts well to new domains, i.e., novel manipulation methods. Second we handle scenarios where only a handful of fake examples are available during training.</p>
		<span class="post-date">2018, Dec 06&nbsp;&nbsp;&nbsp;—&nbsp; ArXiv —&nbsp;</span>
		<!--<span class="post-words">2 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1812.02510.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			
				
				
			
		
		
			<span class="post-paper-bibtex"><a href="/posts/forensictransfer/cozzolino2018forensictransfer.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/deepvideo/thumb.jpg)" href="/posts/deepvideo/"></a>
	  
	  <div class="post-content">
		
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~hyeongwoo/" target="_blank" rel="noopener noreferrer">Hyeongwoo Kim</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~pgarrido/index.html" target="_blank" rel="noopener noreferrer">Pablo Garrido</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~atewari/" target="_blank" rel="noopener noreferrer">Ayush Tewari</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~wxu/" target="_blank" rel="noopener noreferrer">Weipeng Xu</a>&emsp;</span>
						
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://ptrckprz.github.io/" target="_blank" rel="noopener noreferrer">Patrick Perez</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://richardt.name/" target="_blank" rel="noopener noreferrer">Christian Richardt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/deepvideo/">Deep Video Portraits</a></h2>
		

		<!--<p>We present a novel approach that enables photo-realistic re-animation of portrait videos using only an input video. In contrast to existing approaches that are restricted...</p>-->
		<p>Our novel approach enables photo-realistic re-animation of portrait videos using only an input video. The core of our approach is a generative neural network with a novel space-time architecture. The network takes as input synthetic renderings of a parametric face model, based on which it predicts photo-realistic video frames for a given target actor.</p>
		<span class="post-date">2018, May 29&nbsp;&nbsp;&nbsp;—&nbsp; SIGGRAPH —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1805.11714.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=qc5P2bvfl44" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/deepvideo/kim2018deepvideo.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	
	
		
	
	

	<article class="post">
	  
		<a class="post-thumbnail" style="background-image: url(/posts/headon/thumb.jpg)" href="/posts/headon/"></a>
	  
	  <div class="post-content">
		
		
			
			<span class="post-author-bold"><a href="/">Justus Thies</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://zollhoefer.com" target="_blank" rel="noopener noreferrer">Michael Zollhöfer</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://www.lgdv.tf.fau.de/person/marc-stamminger/" target="_blank" rel="noopener noreferrer">Marc Stamminger</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://people.mpi-inf.mpg.de/~theobalt/" target="_blank" rel="noopener noreferrer">Christian Theobalt</a>&emsp;</span>
						
		
						
			<span class="post-author"><a href="https://niessnerlab.org" target="_blank" rel="noopener noreferrer">Matthias Nießner</a>&emsp;</span>
						
		
		
		
			<h2 class="post-title"><a href="/posts/headon/">HeadOn&#58; Real-time Reenactment of Human Portrait Videos</a></h2>
		

		<!--<p>We propose HeadOn, the first real-time source-to-target reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and...</p>-->
		<p>HeadOn is the first real-time reenactment approach for complete human portrait videos that enables transfer of torso and head motion, face expression, and eye gaze. Given a short RGB-D video of the target actor, we automatically construct a personalized geometry proxy that embeds a parametric head, eye, and kinematic torso model. A novel reenactment algorithm employs this proxy to map the captured motion from the source to the target actor.</p>
		<span class="post-date">2018, May 29&nbsp;&nbsp;&nbsp;—&nbsp; SIGGRAPH —&nbsp;</span>
		<!--<span class="post-words">1 minute read&nbsp;&nbsp;&nbsp;&nbsp;</span>-->

		
			<span class="post-paper-link"><a href="https://arxiv.org/pdf/1805.11729.pdf" target="_blank">[Paper]</a>&nbsp;</span>
			
		
			<span class="post-video-link"><a href="https://www.youtube.com/watch?v=UploR8HlEeo" target="_blank">[Video]</a>&nbsp;</span>
		
		
			<span class="post-paper-bibtex"><a href="/posts/headon/thies2018headon.bib" target="_blank">[Bibtex]</a>&nbsp;</span>
			
		</div>
	</article>
	

  </div> <!-- End Page Content -->
</article> <!-- End Article Page -->

</div>

  </div>

</body>
</html>
