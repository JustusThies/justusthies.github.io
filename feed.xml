<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2024-01-01T15:52:18+01:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">3DiFace - Diffusion-based Speech-driven 3D Facial Animation and Editing</title><link href="https://justusthies.github.io/posts/3DiFace/" rel="alternate" type="text/html" title="3DiFace - Diffusion-based Speech-driven 3D Facial Animation and Editing" /><published>2024-01-01T10:00:00+01:00</published><updated>2024-01-01T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/3DiFace</id><content type="html" xml:base="https://justusthies.github.io/posts/3DiFace/"><![CDATA[<p>We present 3DiFACE, a novel method for personalized speech-driven 3D facial animation and editing. While existing methods deterministically predict facial animations from speech, they overlook the inherent one-to-many relationship between speech and facial expressions, i.e., there are multiple reasonable facial expression animations matching an audio input. It is especially important in content creation to be able to modify generated motion or to specify keyframes.
To enable stochasticity as well as motion editing, we propose a lightweight audio-conditioned diffusion model for 3D facial motion. This diffusion model can be trained on a small 3D motion dataset, maintaining expressive lip motion output. In addition, it can be finetuned for specific subjects, requiring only a short video of the person. Through quantitative and qualitative evaluations, we show that our method outperforms existing state-of-the-art techniques and yields speech-driven animations with greater fidelity and diversity.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Balamurugan Thambiraja&quot;, &quot;website_link&quot;=&gt;&quot;https://balamuruganthambiraja.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[To enable stochasticity as well as motion editing, we propose a lightweight audio-conditioned diffusion model for 3D facial motion calles 3DiFace. This diffusion model can be trained on a small 3D motion dataset, maintaining expressive lip motion output. In addition, it can be finetuned for specific subjects, requiring only a short video of the person.]]></summary></entry><entry><title type="html">HAAR - Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles</title><link href="https://justusthies.github.io/posts/haar/" rel="alternate" type="text/html" title="HAAR - Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles" /><published>2024-01-01T10:00:00+01:00</published><updated>2024-01-01T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/haar</id><content type="html" xml:base="https://justusthies.github.io/posts/haar/"><![CDATA[<p>We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used as production-level assets in modern computer graphics engines. Current AI-based generative models take advantage of powerful 2D priors to reconstruct 3D content in the form of point clouds, meshes, or volumetric functions. However, by using the 2D priors, they are intrinsically limited to only recovering the visual parts. Highly occluded hair structures can not be reconstructed with those methods, and they only model the ‘‘outer shell’’, which is not ready to be used in physics-based rendering or simulation pipelines. In contrast, we propose a first text-guided generative method that uses 3D hair strands as an underlying representation. Leveraging 2D visual question-answering (VQA) systems, we automatically annotate synthetic hair models that are generated from a small set of artist-created hairstyles. This allows us to train a latent diffusion model that operates in a common hairstyle UV space. In qualitative and quantitative studies, we demonstrate the capabilities of the proposed model and compare it to existing hairstyle generation approaches.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Vanessa Sklyarova&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used as production-level assets in modern computer graphics engines.]]></summary></entry><entry><title type="html">FaceTalk - Audio-Driven Motion Diffusion for Neural Parametric Head Models</title><link href="https://justusthies.github.io/posts/facetalk/" rel="alternate" type="text/html" title="FaceTalk - Audio-Driven Motion Diffusion for Neural Parametric Head Models" /><published>2024-01-01T10:00:00+01:00</published><updated>2024-01-01T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/facetalk</id><content type="html" xml:base="https://justusthies.github.io/posts/facetalk/"><![CDATA[<p>We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the NPHM shape space. Our experimental results substantiate the effectiveness of FaceTalk, consistently achieving superior and visually natural motion, encompassing diverse facial expressions and styles, outperforming existing methods by 75% in perceptual user study evaluation.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Shivangi Aneja&quot;, &quot;website_link&quot;=&gt;&quot;https://niessnerlab.org/members/shivangi_aneja/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences]]></summary></entry><entry><title type="html">DPHMs - Diffusion Parametric Head Models for Depth-based Tracking</title><link href="https://justusthies.github.io/posts/dphm/" rel="alternate" type="text/html" title="DPHMs - Diffusion Parametric Head Models for Depth-based Tracking" /><published>2024-01-01T10:00:00+01:00</published><updated>2024-01-01T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/dphm</id><content type="html" xml:base="https://justusthies.github.io/posts/dphm/"><![CDATA[<p>We introduce Diffusion Parametric Head Models (DPHMs), a generative model that enables robust volumetric head reconstruction and tracking from monocular depth sequences. While recent volumetric head models, such as NPHMs, can now excel in representing high-fidelity head geometries, tracking and reconstruction heads from real-world single-view depth sequences remains very challenging, as the fitting to partial and noisy observations is underconstrained. To tackle these challenges, we propose a latent diffusion-based prior to regularize volumetric head reconstruction and tracking. This prior-based regularizer effectively constrains the identity and expression codes to lie on the underlying latent manifold which represents plausible head shapes. To evaluate the effectiveness of the diffusion-based prior, we collect a dataset of monocular Kinect sequences consisting of various complex facial expression motions and rapid transitions. We compare our method to state-of-the-art tracking methods, and demonstrate improved head identity reconstruction as well as robust expression tracking.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Jiapeng Tang&quot;, &quot;website_link&quot;=&gt;&quot;https://tangjiapeng.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[We introduce Diffusion Parametric Head Models (DPHMs), a generative model that enables robust volumetric head reconstruction and tracking from monocular depth sequences.]]></summary></entry><entry><title type="html">D3GA - Drivable 3D Gaussian Avatars</title><link href="https://justusthies.github.io/posts/dega/" rel="alternate" type="text/html" title="D3GA - Drivable 3D Gaussian Avatars" /><published>2024-01-01T09:00:00+01:00</published><updated>2024-01-01T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/dega</id><content type="html" xml:base="https://justusthies.github.io/posts/dega/"><![CDATA[<p>We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats. Current photorealistic drivable avatars require either accurate 3D registrations during training, dense input images during testing, or both. The ones based on neural radiance fields also tend to be prohibitively slow for telepresence applications. This work uses the recently presented 3D Gaussian Splatting (3DGS) technique to render realistic humans at real-time framerates, using dense calibrated multi-view videos as input. To deform those primitives, we depart from the commonly used point deformation method of linear blend skinning (LBS) and use a classic volumetric deformation method: cage deformations. Given their smaller size, we drive these deformations with joint angles and keypoints, which are more suitable for communication applications. Our experiments on nine subjects with varied body shapes, clothes, and motions obtain higher-quality results than state-of-the-art methods when using the same training and test data.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Wojciech Zielonka&quot;, &quot;website_link&quot;=&gt;&quot;https://zielon.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats.]]></summary></entry><entry><title type="html">360° Volumetric Portrait Avatar</title><link href="https://justusthies.github.io/posts/3VP/" rel="alternate" type="text/html" title="360° Volumetric Portrait Avatar" /><published>2024-01-01T07:00:00+01:00</published><updated>2024-01-01T07:00:00+01:00</updated><id>https://justusthies.github.io/posts/3VP</id><content type="html" xml:base="https://justusthies.github.io/posts/3VP/"><![CDATA[<p>We propose 360° Volumetric Portrait (3VP) Avatar, a novel method for reconstructing 360° photo-realistic portrait avatars of human subjects solely based on monocular video inputs. State-of-the-art monocular avatar reconstruction methods rely on stable facial performance capturing. However, the common usage of 3DMM-based facial tracking has its limits; side-views can hardly be captured and it fails, especially, for back-views, as required inputs like facial landmarks or human parsing masks are missing. This results in incomplete avatar reconstructions that only cover the frontal hemisphere. In contrast to this, we propose a template-based tracking of the torso, head and facial expressions which allows us to cover the appearance of a human subject from all sides. Thus, given a sequence of a subject that is rotating in front of a single camera, we train a neural volumetric representation based on neural radiance fields. A key challenge to construct this representation is the modeling of appearance changes, especially, in the mouth region (i.e., lips and teeth). We, therefore, propose a deformation-field-based blend basis which allows us to interpolate between different appearance states. We evaluate our approach on captured real-world data and compare against state-of-the-art monocular reconstruction methods. In contrast to those, our method is the first monocular technique that reconstructs an entire 360° avatar.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Jalees Nehvi&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[We propose 360° Volumetric Portrait (3VP) Avatar, a novel method for reconstructing 360° photo-realistic portrait avatars of human subjects solely based on monocular video inputs.]]></summary></entry><entry><title type="html">Environment-Specific People</title><link href="https://justusthies.github.io/posts/esp/" rel="alternate" type="text/html" title="Environment-Specific People" /><published>2024-01-01T00:00:00+01:00</published><updated>2024-01-01T00:00:00+01:00</updated><id>https://justusthies.github.io/posts/esp</id><content type="html" xml:base="https://justusthies.github.io/posts/esp/"><![CDATA[<p>Despite significant progress in generative image synthesis and full-body generation in particular, state-of-the-art methods are either context-independent, overly reliant to text prompts, or bound to the curated training datasets, such as fashion images with monotonous backgrounds. Here, our goal is to generate people in clothing that is semantically appropriate for a given scene. To this end, we present ESP, a novel method for context-aware full-body generation, that enables photo-realistic inpainting of people into existing “in-the-wild” photographs. ESP is conditioned on a 2D pose and contextual cues that are extracted from the environment photograph and integrated into the generation process. Our models are trained on a dataset containing a set of in-the-wild photographs of people covering a wide range of different environments. The method is analyzed quantitatively and qualitatively, and we show that ESP outperforms state-of-the-art on the task of contextual full-body generation.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Mirela Ostrek&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[We present ESP, a novel method for context-aware full-body generation, that enables photo-realistic inpainting of people into existing "in-the-wild" photographs.]]></summary></entry><entry><title type="html">GAN-Avatar - Controllable Personalized GAN-based Human Head Avatars</title><link href="https://justusthies.github.io/posts/ganavatar/" rel="alternate" type="text/html" title="GAN-Avatar - Controllable Personalized GAN-based Human Head Avatars" /><published>2023-12-31T23:00:00+01:00</published><updated>2023-12-31T23:00:00+01:00</updated><id>https://justusthies.github.io/posts/ganavatar</id><content type="html" xml:base="https://justusthies.github.io/posts/ganavatar/"><![CDATA[<p>Digital humans and, especially, 3D facial avatars have raised a lot of attention in the past years, as they are the backbone of several applications like immersive telepresence in AR or VR. Despite the progress, facial avatars reconstructed from commodity hardware are incomplete and miss out on parts of the side and back of the head, severely limiting the usability of the avatar. This limitation in prior work stems from their requirement of face tracking, which fails for profile and back views. To address this issue, we propose to learn person-specific animatable avatars from images without assuming to have access to precise facial expression tracking. At the core of our method, we leverage a 3D-aware generative model that is trained to reproduce the distribution of facial expressions from the training data. To train this appearance model, we only assume to have a collection of 2D images with the corresponding camera parameters. For controlling the model, we learn a mapping from 3DMM facial expression parameters to the latent space of the generative model. This mapping can be learned by sampling the latent space of the appearance model and reconstructing the facial parameters from a normalized frontal view, where facial expression estimation performs well. With this scheme, we decouple 3D appearance reconstruction and animation control to achieve high fidelity in image synthesis. In a series of experiments, we compare our proposed technique to state-of-the-art monocular methods and show superior quality while not requiring expression tracking of the training data.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Berna Kabadayi&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[We propose to learn person-specific animatable avatars from images without assuming to have access to precise facial expression tracking. At the core of our method, we leverage a 3D-aware generative model that is trained to reproduce the distribution of facial expressions from the training data.]]></summary></entry><entry><title type="html">DiffuScene - Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis</title><link href="https://justusthies.github.io/posts/diffuscene/" rel="alternate" type="text/html" title="DiffuScene - Scene Graph Denoising Diffusion Probabilistic Model for Generative Indoor Scene Synthesis" /><published>2023-12-31T22:00:00+01:00</published><updated>2023-12-31T22:00:00+01:00</updated><id>https://justusthies.github.io/posts/diffuscene</id><content type="html" xml:base="https://justusthies.github.io/posts/diffuscene/"><![CDATA[<p>We present DiffuScene for indoor 3D scene synthesis based on a novel scene graph denoising diffusion probabilistic model, which generates 3D instance properties stored in a fully-connected scene graph and then retrieves the most similar object geometry for each graph node i.e. object instance which is characterized as a concatenation of different attributes, including location, size, orientation, semantic, and geometry features. Based on this scene graph, we designed a diffusion model to determine the placements and types of 3D instances. Our method can facilitate many downstream applications, including scene completion, scene arrangement, and text-conditioned scene synthesis. Experiments on the 3D-FRONT dataset show that our method can synthesize more physically plausible and diverse indoor scenes than state-of-the-art methods. Extensive ablation studies verify the effectiveness of our design choice in scene diffusion models.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Jiapeng Tang&quot;, &quot;website_link&quot;=&gt;&quot;https://tangjiapeng.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[We present DiffuScene, a diffusion-based method for indoor 3D scene synthesis which operates on a 3D scene graph representation.]]></summary></entry><entry><title type="html">SCULPT - Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes</title><link href="https://justusthies.github.io/posts/sculpt/" rel="alternate" type="text/html" title="SCULPT - Shape-Conditioned Unpaired Learning of Pose-dependent Clothed and Textured Human Meshes" /><published>2023-12-31T10:00:00+01:00</published><updated>2023-12-31T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/sculpt</id><content type="html" xml:base="https://justusthies.github.io/posts/sculpt/"><![CDATA[<p>We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns to represent the geometry and appearance distribution of clothed human bodies. Training such a model is challenging, as datasets of textured 3D meshes for humans are limited in size and accessibility. Our key observation is that there exist medium-sized 3D scan datasets like CAPE, as well as large-scale 2D image datasets of clothed humans and multiple appearances can be mapped to a single geometry. To effectively learn from the two data modalities, we propose an unpaired learning procedure for pose-dependent clothed and textured human meshes. Specifically, we learn a pose-dependent geometry space from 3D scan data. We represent this as per vertex displacements w.r.t. the SMPL model. Next, we train a geometry conditioned texture generator in an unsupervised way using the 2D image data. We use intermediate activations of the learned geometry model to condition our texture generator. To alleviate entanglement between pose and clothing type, and pose and clothing appearance, we condition both the texture and geometry generators with attribute labels such as clothing types for the geometry, and clothing colors for the texture generator. We automatically generated these conditioning labels for the 2D images based on the visual question answering model BLIP and CLIP. We validate our method on the SCULPT dataset, and compare to state-of-the-art 3D generative models for clothed human bodies. We will release the codebase for research purposes.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Soubhik Sanyal&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ArXiv" /><summary type="html"><![CDATA[We present SCULPT, a novel 3D generative model for clothed and textured 3D meshes of humans. Specifically, we devise a deep neural network that learns to represent the geometry and appearance distribution of clothed human bodies.]]></summary></entry></feed>