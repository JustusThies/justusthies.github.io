<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2024-10-14T13:27:24+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">German Pattern Recognition Award 2024</title><link href="https://justusthies.github.io/posts/german_pattern_recognition_award/" rel="alternate" type="text/html" title="German Pattern Recognition Award 2024" /><published>2024-10-01T11:00:00+02:00</published><updated>2024-10-01T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/german_pattern_recognition_award</id><content type="html" xml:base="https://justusthies.github.io/posts/german_pattern_recognition_award/"><![CDATA[<p>Since 2011, the DAGM has annually granted the German Pattern Recognition Award for outstanding, internationally visible research in the fields of pattern recognition, computer vision, and machine learning. The German Pattern Recognition Award continues the tradition of the Olympus Award for Pattern Recognition (1992-2010).</p>

<p>Its aim is to promote outstanding young scientists in the fields of pattern recognition, computer vision, and machine learning. Internationally visible researchers in these fields who have not yet passed the age of 35 in the year of the awards presentation can be proposed. An association of the candidates with the DAGM is desirable. The German Pattern Recognition Award is currently donated by Daimler AG and consists of a financial award of 5000 euros.</p>

<p>The German Pattern Recognition Award was granted to Prof. Dr. Justus Thies for his outstanding scientific contributions in the area of Real-Time Face Tracking, Facial Reenactment, AI-driven Rendering as well as Digital Multi-Media Forensics.</p>

<p><a href="https://www.dagm.de/award-winners/german-pattern-recognition-award">DAGM Pattern Recogniton Award Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="DAGM" /><summary type="html"><![CDATA[I have been awarded with the DAGM German Pattern Recognition Award 2024.]]></summary></entry><entry><title type="html">ERC Starting Grant 2024</title><link href="https://justusthies.github.io/posts/erc_starting_grant/" rel="alternate" type="text/html" title="ERC Starting Grant 2024" /><published>2024-09-30T11:00:00+02:00</published><updated>2024-09-30T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/erc_starting_grant</id><content type="html" xml:base="https://justusthies.github.io/posts/erc_starting_grant/"><![CDATA[<p>The European Union uses the Starting Grant to promote outstanding research and, at the same time, early career researchers. The Starting Grant is aimed at researchers who can already demonstrate excellent work and now want to expand their own research or set up their own research group at the start of their careers.</p>

<p>Justus Thies is being funded for his project “Learning Digital Humans in Motion” . His aim is to develop AI-based image processing and graphic tools to create lifelike digital representations of people for the immersive digital world. Both projects will receive a total of around 1.5 million euros each over a period of five years.</p>

<p><a href="https://www.tu-darmstadt.de/universitaet/aktuelles_meldungen/einzelansicht_471040.en.jsp">TUDa News</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="ERC" /><summary type="html"><![CDATA[I have been awarded with an ERC Starting Grant 2024. We will work on learning digital humans in motion.]]></summary></entry><entry><title type="html">EG Young Researcher Award 2024</title><link href="https://justusthies.github.io/posts/eg_young_researcher_award/" rel="alternate" type="text/html" title="EG Young Researcher Award 2024" /><published>2024-05-01T11:00:00+02:00</published><updated>2024-05-01T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/eg_young_researcher_award</id><content type="html" xml:base="https://justusthies.github.io/posts/eg_young_researcher_award/"><![CDATA[<p>Justus Thies receives the EUROGRAPHICS Young Researcher Award 2024. Justus obtained his PhD from the University of Erlangen Nuremberg. He is now a full Professor at the Technical University of Darmstadt where he is leading the 3D Graphics &amp; Vision group. In addition, he is an independent research group leader at the Max Planck Institute for Intelligent Systems.<br />
Justus studies the capture and synthesis of digital humans. His work blends elements of computer graphics, computer vision, and machine learning, with the end goal of capturing and re-synthesizing reality. Justus has done profound work on important aspects of “digital humans”, namely marker-less capture, neural synthesis, and multi-media forensics, among others. 
Justus’ Face2Face algorithm was a pioneering and very successful approach to real-time facial performance capture from video data which also enabled facial re-enactment. Later, with the advent of neural networks, Justus again considerably advanced the state of the art with his work on deferred neural rendering. Importantly, new methods for high-quality image and video, colloquially known as “deep fakes,” can also be abused, e.g. for misinformation. Justus and colleagues have made great strides on this equally important aspect in their work FaceForensics++, which allows to detect manipulated facial images. As the saying goes: “It takes (someone that makes) one to know one”, thus it is of paramount importance that researchers that develop new machine learning-based image synthesis approaches with all their great potential for computer graphics, additionally invest in identifying synthesized imagery. Justus’ groundbreaking work is a demonstration of this principle. 
The work of Justus Thies is published in the top tier conferences and journals of computer graphics and computer vision, and has been very widely cited, His work has been honored with a Research Highlight report of the Communication of the ACM 2019, and an emerging technology award at SIGGRAPH 2016.  With his considerable and impactful contributions to the field of digital humans, Justus helps shaping the future of how the real and the virtual interact.</p>

<p>EUROGRAPHICS is pleased to recognize Justus Thies with the 2024 Young Researcher Award in recognition of his outstanding contributions to Computer Graphics in the area of marker-less motion capture and synthesis.</p>

<p><a href="https://www.eg.org/wp/eurographics-awards-programme/the-young-researcher-award/young-researcher-award-2024-justus-thies/">Eurographics Website</a></p>

<p><a href="https://www.informatik.tu-darmstadt.de/fb20/ueber_uns_details_299776.en.jsp">TUDa News</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="EG" /><summary type="html"><![CDATA[I have been awarded with the Eurographics Young Researcher Award 2024.]]></summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS24/" rel="alternate" type="text/html" title="Lecture: 3D Scanning and Motion Capture" /><published>2024-04-24T23:00:00+02:00</published><updated>2024-04-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS24</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS24/"><![CDATA[<h3 id="content">Content</h3>
<p>The lecture and exercises will cover 3D reconstruction from various input
modalities (Webcams, RGB-D cameras (Kinect, Realsense, …). It will start with
basic concepts of what is 3D, the different representations, how to capture 3D
and how the devices and sensors function. Based on this introduction, rigid and
non-rigid tracking and reconstruction will be discussed. Specialized face and
body tracking methods will be covered and the applications of the 3D
reconstruction and tracking will be shown. In addition to the 3D surface
reconstruction, techniques for appearance modelling and material estimation
will be shown.</p>

<p>++ Basic concepts of geometry (Meshes, Point Clouds, Pixels &amp; Voxels)
++ RGB and Depth Cameras (Calibration, active/passive stereo, Time of
Flight (ToF), Structured Light, Laser Scanner, Lidar)
++ Surface Representations (Polygonal meshes, parametric surfaces,
implicit surfaces (Radial basis functions, signed distance functions,
indicator function), Marching cubes)
++ Overview of reconstruction methods (Structure from Motion (SfM),
Multi-view Stereo (MVS), SLAM, Bundle Adjustment)
++ Rigid Surface Tracking &amp; Reconstruction (Pose alignment, ICP, online
surface reconstruction pipeline (KinectFusion), scalable surface
representations (VoxelHashing, OctTrees), loop closures and global
optimization)
++ Non-rigid Surface Tracking &amp; Reconstruction (Surface deformation for
modeling, Regularizers: ARAP, ED, etc., Non-rigid surface fitting: e.g.,
non-rigid ICP. Non-rigid reconstruction:
DynamicFusion/VolumeDeform/KillingFusion)
++ Face Tracking &amp; Reconstruction (Keypoint detection &amp; tracking,
Parametric / Statistical Models -&gt; BlendShapes)
++ Body Tracking &amp; Reconstruction (Skeleton Tracking and Inverse
Kinematics, Marker-based motion capture)
++ Material capture (Lightstage, BRDF estimation)
++ Outlook DeepLearning-based tracking</p>

<h3 id="aim">Aim</h3>
<p>Basic understanding of 3D capturing devices and underlying principles (active vs.
passive stereo, ToF etc.), modelling of geometry and conversion between
different representations, principles of static reconstruction (fusion, ICP) and
non-rigid reconstruction using deformation priors. Basic understanding of
specialized class-specific tracking (face, body, hands) and their applications.</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[This hands-on lecture focuses on cutting-edge 3D reconstruction approaches, including volumetric fusion approaches on implicit functions, face tracking, hand tracking, and much more! We also cover essential optimization techniques such as Gauss-Newton and Levenberg-Marquardt.]]></summary></entry><entry><title type="html">Lecture: AI-based 3D Graphics and Vision</title><link href="https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-SS24/" rel="alternate" type="text/html" title="Lecture: AI-based 3D Graphics and Vision" /><published>2024-04-24T23:00:00+02:00</published><updated>2024-04-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-SS24</id><content type="html" xml:base="https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-SS24/"><![CDATA[<h3 id="content">Content</h3>
<p>The lecture will cover AI-based 3D reconstruction from various input modalities (Webcams, RGB-D cameras (Kinect, Realsense, …). The lecture builds upon the classical 3D reconstruction methods discussed in the ‘3D Scanning &amp; Motion Capture’ lecture and shows how components and data structures of those methods can be replaced or extended by methods from AI. It will start with basic concepts of 2D neural rendering, including methods like Pix2Pix, Deferred Neural Rendering and alike. Then, more advanced topics like 3D/4D neural scene representations are discussed. To train those representations, differentiable rendering needs to be understood. The lecture will introduce methods for static and dynamic reconstruction with different levels of controllability and editability.
++ Concept of 2D Neural Rendering
++ Deferred Neural Rendering, AI-based Image-based Rendering.
++ 3D Neural Rendering and Neural Scene Representations
++ Neural Radiance Fields (NeRFs)
++ Neural Point-based Graphics, Gaussian Splatting
++ Differentiable Rendering (Rasterization, Volume Rendering, Shading)
++ Relighting and Material Reconstruction
++ DeepFakes
++ Outlook: detection of synthetic media</p>

<h3 id="aim">Aim</h3>
<p>Basic understanding of how methods of AI can be used for 3D capturing of objects, scenes, and humans. It includes the principles of 2D and 3D neural rendering, as well as the differentiable rendering (Inverse Graphics).</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[The lecture will cover AI-based 3D reconstruction, synthesis, and rendering from various input modalities (image, text, audio). It covers topics like 2D/3D neural rendering, deepfakes, natural image synthesis with different controls, as well as motion and video generation.]]></summary></entry><entry><title type="html">HAAR - Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles</title><link href="https://justusthies.github.io/posts/haar/" rel="alternate" type="text/html" title="HAAR - Text-Conditioned Generative Model of 3D Strand-based Human Hairstyles" /><published>2024-02-01T10:00:00+01:00</published><updated>2024-02-01T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/haar</id><content type="html" xml:base="https://justusthies.github.io/posts/haar/"><![CDATA[<p>We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used as production-level assets in modern computer graphics engines. Current AI-based generative models take advantage of powerful 2D priors to reconstruct 3D content in the form of point clouds, meshes, or volumetric functions. However, by using the 2D priors, they are intrinsically limited to only recovering the visual parts. Highly occluded hair structures can not be reconstructed with those methods, and they only model the ‘‘outer shell’’, which is not ready to be used in physics-based rendering or simulation pipelines. In contrast, we propose a first text-guided generative method that uses 3D hair strands as an underlying representation. Leveraging 2D visual question-answering (VQA) systems, we automatically annotate synthetic hair models that are generated from a small set of artist-created hairstyles. This allows us to train a latent diffusion model that operates in a common hairstyle UV space. In qualitative and quantitative studies, we demonstrate the capabilities of the proposed model and compare it to existing hairstyle generation approaches.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Vanessa Sklyarova&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We present HAAR, a new strand-based generative model for 3D human hairstyles. Specifically, based on textual inputs, HAAR produces 3D hairstyles that could be used as production-level assets in modern computer graphics engines.]]></summary></entry><entry><title type="html">TECA - Text-Guided Generation and Editing of Compositional 3D Avatars</title><link href="https://justusthies.github.io/posts/teca/" rel="alternate" type="text/html" title="TECA - Text-Guided Generation and Editing of Compositional 3D Avatars" /><published>2024-01-13T10:00:00+01:00</published><updated>2024-01-13T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/teca</id><content type="html" xml:base="https://justusthies.github.io/posts/teca/"><![CDATA[<p>Our goal is to create a realistic 3D facial avatar with hair and accessories using only a text description. While this challenge has attracted significant recent interest, existing methods either lack realism, produce unrealistic shapes, or do not support editing, such as modifications to the hairstyle. We argue that existing methods are limited because they employ a monolithic modeling approach, using a single representation for the head, face, hair, and accessories. Our observation is that the hair and face, for example, have very different structural qualities that benefit from different representations. Building on this insight, we generate avatars with a compositional model, in which the head, face, and upper body are represented with traditional 3D meshes, and the hair, clothing, and accessories with neural radiance fields (NeRF). The model-based mesh representation provides a strong geometric prior for the face region, improving realism while enabling editing of the person’s appearance. By using NeRFs to represent the remaining components, our method is able to model and synthesize parts with complex geometry and appearance, such as curly hair and fluffy scarves. Our novel system synthesizes these high-quality compositional avatars from text descriptions. Specifically, we generate a face image using text, fit a parametric shape model to it, and inpaint texture using diffusion models. Conditioned on the generated face, we sequentially generate style components such as hair or clothing using Score Distillation Sampling (SDS) with guidance from CLIPSeg segmentations. However, this alone is not sufficient to produce avatars with a high degree of realism. Consequently, we introduce a hierarchical approach to refine the non-face regions using a BLIP-based loss combined with SDS. The experimental results demonstrate that our method, Text-guided generation and Editing of Compositional Avatars (TECA), produces avatars that are more realistic than those of recent methods while being editable because of their compositional nature. For example, our TECA enables the seamless transfer of compositional features like hairstyles, scarves, and other accessories between avatars. This capability supports applications such as virtual try-on.</p>

<p><a href="https://yfeng95.github.io/teca/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Hao Zhang&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[Given a text description, our method produces a compositional 3D avatar consisting of a mesh-based face and body and NeRF-based hair, clothing and other accessories.]]></summary></entry><entry><title type="html">TeCH - Text-guided Reconstruction of Lifelike Clothed Humans</title><link href="https://justusthies.github.io/posts/tech/" rel="alternate" type="text/html" title="TeCH - Text-guided Reconstruction of Lifelike Clothed Humans" /><published>2024-01-01T10:00:00+01:00</published><updated>2024-01-01T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/tech</id><content type="html" xml:base="https://justusthies.github.io/posts/tech/"><![CDATA[<p>Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the “unseen regions” with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the “indescribable” appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an implicit distance field. Guided by the descriptive prompts + personalized T2I diffusion model, the geometry and texture of the 3D humans are optimized through multi-view Score Distillation Sampling (SDS) and reconstruction losses based on the original observation. TeCH produces high-fidelity 3D clothed humans with consistent &amp; delicate texture, and detailed full-body geometry. Quantitative and qualitative experiments demonstrate that TeCH outperforms the state-of-the-art methods in terms of reconstruction accuracy and rendering quality.</p>

<p><a href="https://huangyangyi.github.io/TeCH/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Yangyi Huang&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the "indescribable" appearance.]]></summary></entry><entry><title type="html">FaceTalk - Audio-Driven Motion Diffusion for Neural Parametric Head Models</title><link href="https://justusthies.github.io/posts/facetalk/" rel="alternate" type="text/html" title="FaceTalk - Audio-Driven Motion Diffusion for Neural Parametric Head Models" /><published>2024-01-01T10:00:00+01:00</published><updated>2024-01-01T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/facetalk</id><content type="html" xml:base="https://justusthies.github.io/posts/facetalk/"><![CDATA[<p>We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences. We propose a new latent diffusion model for this task, operating in the expression space of neural parametric head models, to synthesize audio-driven realistic head sequences. In the absence of a dataset with corresponding NPHM expressions to audio, we optimize for these correspondences to produce a dataset of temporally-optimized NPHM expressions fit to audio-video recordings of people talking. To the best of our knowledge, this is the first work to propose a generative approach for realistic and high-quality motion synthesis of volumetric human heads, representing a significant advancement in the field of audio-driven 3D animation. Notably, our approach stands out in its ability to generate plausible motion sequences that can produce high-fidelity head animation coupled with the NPHM shape space. Our experimental results substantiate the effectiveness of FaceTalk, consistently achieving superior and visually natural motion, encompassing diverse facial expressions and styles, outperforming existing methods by 75% in perceptual user study evaluation.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Shivangi Aneja&quot;, &quot;website_link&quot;=&gt;&quot;https://niessnerlab.org/members/shivangi_aneja/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We introduce FaceTalk, a novel generative approach designed for synthesizing high-fidelity 3D motion sequences of talking human heads from input audio signal. To capture the expressive, detailed nature of human heads, including hair, ears, and finer-scale eye movements, we propose to couple speech signal with the latent space of neural parametric head models to create high-fidelity, temporally coherent motion sequences]]></summary></entry><entry><title type="html">DPHMs - Diffusion Parametric Head Models for Depth-based Tracking</title><link href="https://justusthies.github.io/posts/dphm/" rel="alternate" type="text/html" title="DPHMs - Diffusion Parametric Head Models for Depth-based Tracking" /><published>2024-01-01T10:00:00+01:00</published><updated>2024-01-01T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/dphm</id><content type="html" xml:base="https://justusthies.github.io/posts/dphm/"><![CDATA[<p>We introduce Diffusion Parametric Head Models (DPHMs), a generative model that enables robust volumetric head reconstruction and tracking from monocular depth sequences. While recent volumetric head models, such as NPHMs, can now excel in representing high-fidelity head geometries, tracking and reconstruction heads from real-world single-view depth sequences remains very challenging, as the fitting to partial and noisy observations is underconstrained. To tackle these challenges, we propose a latent diffusion-based prior to regularize volumetric head reconstruction and tracking. This prior-based regularizer effectively constrains the identity and expression codes to lie on the underlying latent manifold which represents plausible head shapes. To evaluate the effectiveness of the diffusion-based prior, we collect a dataset of monocular Kinect sequences consisting of various complex facial expression motions and rapid transitions. We compare our method to state-of-the-art tracking methods, and demonstrate improved head identity reconstruction as well as robust expression tracking.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Jiapeng Tang&quot;, &quot;website_link&quot;=&gt;&quot;https://tangjiapeng.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We introduce Diffusion Parametric Head Models (DPHMs), a generative model that enables robust volumetric head reconstruction and tracking from monocular depth sequences.]]></summary></entry></feed>