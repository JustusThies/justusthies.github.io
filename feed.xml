<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2024-11-25T12:24:23+01:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Joker - Conditional 3D Head Synthesis with Extreme Facial Expressions</title><link href="https://justusthies.github.io/posts/joker/" rel="alternate" type="text/html" title="Joker - Conditional 3D Head Synthesis with Extreme Facial Expressions" /><published>2024-11-25T09:00:00+01:00</published><updated>2024-11-25T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/joker</id><content type="html" xml:base="https://justusthies.github.io/posts/joker/"><![CDATA[<p>We introduce Joker, a new method for the conditional synthesis of 3D human heads with extreme expressions. Given a single reference image of a person, we synthesize a volumetric human head with the reference’s identity and a new expression. We offer control over the expression via a 3D morphable model (3DMM) and textual inputs. This multi-modal conditioning signal is essential since 3DMMs alone fail to define subtle emotional changes and extreme expressions, including those involving the mouth cavity and tongue articulation. Our method is built upon a 2D diffusion-based prior that generalizes well to out-of-domain samples, such as sculptures, heavy makeup, and paintings while achieving high levels of expressiveness. To improve view consistency, we propose a new 3D distillation technique that converts predictions of our 2D prior into a neural radiance field (NeRF). Both the 2D prior and our distillation technique produce state-of-the-art results, which are confirmed by our extensive evaluations. Also, to the best of our knowledge, our method is the first to achieve view-consistent extreme tongue articulation.</p>

<p><a href="https://malteprinzler.github.io/projects/joker/index.html">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Malte Prinzler&quot;, &quot;website_link&quot;=&gt;&quot;https://ncs.is.mpg.de/person/mprinzler&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[Joker uses one reference image to generate a 3D reconstruction with a novel extreme expression. The target expression is defined through 3DMM parameters and text prompts. The text prompts effectively resolve ambiguities in the 3DMM input and can control emotion-related expression subtleties and tongue articulation.]]></summary></entry><entry><title type="html">3DiFACE - Diffusion-based Speech-driven 3D Facial Animation and Editing</title><link href="https://justusthies.github.io/posts/3diface/" rel="alternate" type="text/html" title="3DiFACE - Diffusion-based Speech-driven 3D Facial Animation and Editing" /><published>2024-11-02T09:00:00+01:00</published><updated>2024-11-02T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/3diface</id><content type="html" xml:base="https://justusthies.github.io/posts/3diface/"><![CDATA[<p>Creating an animation of a specific person with audio-synced lip motions, realistic head motion and editing via artist-defined keyframes are a set of tasks that challenge existing speech-driven 3D facial animation methods. Especially, editing 3D facial animation is a complex and time-consuming task carried out by highly skilled animators. Also, most existing works overlook the inherent one-to-many relationship between speech and facial motion, where multiple plausible lip and head animations could sync with the audio input. To this end, we present 3DiFACE, a novel method for holistic speech-driven 3D facial animation, which produces diverse plausible lip and head motions for a single audio input, while also allowing editing via keyframing and interpolation. 3DiFACE is a lightweight audio-conditioned diffusion model, which can be fine-tuned to generate personalized 3D facial animation requiring only a short video of the subject. Specifically, we leverage the viseme-level diversity in our training corpus to train a fully-convolutional diffusion model that produces diverse sequences for single audio input. Additionally, we employ a modified guided motion diffusion to enable head-motion synthesis and editing using masking. Through quantitative and qualitative evaluations, we demonstrate that our method is capable of generating and editing diverse holistic 3D facial animations given a single audio input, with control between high fidelity and diversity.</p>

<p><a href="https://balamuruganthambiraja.github.io/3DiFACE/">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Balamurugan Thambiraja&quot;, &quot;website_link&quot;=&gt;&quot;https://balamuruganthambiraja.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[We present 3DiFACE, a novel audio-conditioned diffusion model for holistic speech-driven 3D facial animation, which produces diverse plausible lip and head motions for a single audio input, while also allowing editing via keyframing and interpolation.]]></summary></entry><entry><title type="html">D3GA - Drivable 3D Gaussian Avatars</title><link href="https://justusthies.github.io/posts/dega/" rel="alternate" type="text/html" title="D3GA - Drivable 3D Gaussian Avatars" /><published>2024-11-01T09:00:00+01:00</published><updated>2024-11-01T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/dega</id><content type="html" xml:base="https://justusthies.github.io/posts/dega/"><![CDATA[<p>We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats. Current photorealistic drivable avatars require either accurate 3D registrations during training, dense input images during testing, or both. The ones based on neural radiance fields also tend to be prohibitively slow for telepresence applications. This work uses the recently presented 3D Gaussian Splatting (3DGS) technique to render realistic humans at real-time framerates, using dense calibrated multi-view videos as input. To deform those primitives, we depart from the commonly used point deformation method of linear blend skinning (LBS) and use a classic volumetric deformation method: cage deformations. Given their smaller size, we drive these deformations with joint angles and keypoints, which are more suitable for communication applications. Our experiments on nine subjects with varied body shapes, clothes, and motions obtain higher-quality results than state-of-the-art methods when using the same training and test data.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Wojciech Zielonka&quot;, &quot;website_link&quot;=&gt;&quot;https://zielon.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats.]]></summary></entry><entry><title type="html">Gaussian-Haircut - Human Hair Reconstruction with Strand-Aligned 3D Gaussians</title><link href="https://justusthies.github.io/posts/gaussianhair/" rel="alternate" type="text/html" title="Gaussian-Haircut - Human Hair Reconstruction with Strand-Aligned 3D Gaussians" /><published>2024-10-01T11:00:00+02:00</published><updated>2024-10-01T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/gaussianhair</id><content type="html" xml:base="https://justusthies.github.io/posts/gaussianhair/"><![CDATA[<p>We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction.</p>

<p><a href="https://eth-ait.github.io/GaussianHaircut/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Egor Zhakharov&quot;, &quot;website_link&quot;=&gt;&quot;https://egorzakharov.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data.]]></summary></entry><entry><title type="html">German Pattern Recognition Award 2024</title><link href="https://justusthies.github.io/posts/german_pattern_recognition_award/" rel="alternate" type="text/html" title="German Pattern Recognition Award 2024" /><published>2024-10-01T11:00:00+02:00</published><updated>2024-10-01T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/german_pattern_recognition_award</id><content type="html" xml:base="https://justusthies.github.io/posts/german_pattern_recognition_award/"><![CDATA[<p>Since 2011, the DAGM has annually granted the German Pattern Recognition Award for outstanding, internationally visible research in the fields of pattern recognition, computer vision, and machine learning. The German Pattern Recognition Award continues the tradition of the Olympus Award for Pattern Recognition (1992-2010).</p>

<p>Its aim is to promote outstanding young scientists in the fields of pattern recognition, computer vision, and machine learning. Internationally visible researchers in these fields who have not yet passed the age of 35 in the year of the awards presentation can be proposed. An association of the candidates with the DAGM is desirable. The German Pattern Recognition Award is currently donated by Daimler AG and consists of a financial award of 5000 euros.</p>

<p>The German Pattern Recognition Award was granted to Prof. Dr. Justus Thies for his outstanding scientific contributions in the area of Real-Time Face Tracking, Facial Reenactment, AI-driven Rendering as well as Digital Multi-Media Forensics.</p>

<p><a href="https://www.dagm.de/award-winners/german-pattern-recognition-award">DAGM Pattern Recogniton Award Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="DAGM" /><summary type="html"><![CDATA[I have been awarded with the DAGM German Pattern Recognition Award 2024.]]></summary></entry><entry><title type="html">Stable Video Portraits</title><link href="https://justusthies.github.io/posts/svp/" rel="alternate" type="text/html" title="Stable Video Portraits" /><published>2024-10-01T01:00:00+02:00</published><updated>2024-10-01T01:00:00+02:00</updated><id>https://justusthies.github.io/posts/svp</id><content type="html" xml:base="https://justusthies.github.io/posts/svp/"><![CDATA[<p>Rapid advances in the field of generative AI and text-to-image methods in particular have transformed the way we interact with and perceive computer-generated imagery today. In parallel, much progress has been made in 3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we present Stable Video Portraits, a novel hybrid 2D/3D generation method that outputs photorealistic videos of talking faces leveraging a large pre-trained text-to-image prior (2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific fine-tuning of a general 2D stable diffusion model which we lift to a video model by providing temporal 3DMM sequences as conditioning and by introducing a temporal denoising procedure. As an output, this model generates temporally smooth imagery of a person with 3DMM-based controls, i.e., a person-specific avatar. The facial appearance of this person-specific avatar can be edited and morphed to text-defined celebrities, without any test-time fine-tuning. The method is analyzed quantitatively and qualitatively, and we show that our method outperforms state-of-the-art monocular head avatar methods.</p>

<p><a href="https://svp.is.tue.mpg.de/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Mirela Ostrek&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[Stable Video Portraits is a novel hybrid 2D/3D generation method that outputs photorealistic videos of talking faces leveraging a large pre-trained text-to-image prior (2D), controlled via a 3DMM (3D). It is based on a personalized image diffusion prior which allows us to generate new videos of the subject, and also to edit the appearance by blending the personalized image prior with a general text-conditioned model.]]></summary></entry><entry><title type="html">ERC Starting Grant 2024</title><link href="https://justusthies.github.io/posts/erc_starting_grant/" rel="alternate" type="text/html" title="ERC Starting Grant 2024" /><published>2024-09-30T11:00:00+02:00</published><updated>2024-09-30T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/erc_starting_grant</id><content type="html" xml:base="https://justusthies.github.io/posts/erc_starting_grant/"><![CDATA[<p>The European Union uses the Starting Grant to promote outstanding research and, at the same time, early career researchers. The Starting Grant is aimed at researchers who can already demonstrate excellent work and now want to expand their own research or set up their own research group at the start of their careers.</p>

<p>Justus Thies is being funded for his project “Learning Digital Humans in Motion” . His aim is to develop AI-based image processing and graphic tools to create lifelike digital representations of people for the immersive digital world. Both projects will receive a total of around 1.5 million euros each over a period of five years.</p>

<p><a href="https://www.tu-darmstadt.de/universitaet/aktuelles_meldungen/einzelansicht_471040.en.jsp">TUDa News</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="ERC" /><summary type="html"><![CDATA[I have been awarded with an ERC Starting Grant 2024. We will work on learning digital humans in motion.]]></summary></entry><entry><title type="html">TeSMo - Generating Human Interaction Motions in Scenes with Text Control</title><link href="https://justusthies.github.io/posts/tesmo/" rel="alternate" type="text/html" title="TeSMo - Generating Human Interaction Motions in Scenes with Text Control" /><published>2024-09-29T11:00:00+02:00</published><updated>2024-09-29T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/tesmo</id><content type="html" xml:base="https://justusthies.github.io/posts/tesmo/"><![CDATA[<p>We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions.</p>

<p><a href="https://research.nvidia.com/labs/toronto-ai/tesmo/">Project page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Hongwei Yi&quot;, &quot;website_link&quot;=&gt;&quot;https://xyyhw.top/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[TeSMo is a method for text-controlled scene-aware motion generation based on denoising diffusion models. Specifically, we pre-train a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. Then, we enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes.]]></summary></entry><entry><title type="html">Environment-Specific People</title><link href="https://justusthies.github.io/posts/esp/" rel="alternate" type="text/html" title="Environment-Specific People" /><published>2024-09-01T01:00:00+02:00</published><updated>2024-09-01T01:00:00+02:00</updated><id>https://justusthies.github.io/posts/esp</id><content type="html" xml:base="https://justusthies.github.io/posts/esp/"><![CDATA[<p>Despite significant progress in generative image synthesis and full-body generation in particular, state-of-the-art methods are either context-independent, overly reliant to text prompts, or bound to the curated training datasets, such as fashion images with monotonous backgrounds. Here, our goal is to generate people in clothing that is semantically appropriate for a given scene. To this end, we present ESP, a novel method for context-aware full-body generation, that enables photo-realistic inpainting of people into existing “in-the-wild” photographs. ESP is conditioned on a 2D pose and contextual cues that are extracted from the environment photograph and integrated into the generation process. Our models are trained on a dataset containing a set of in-the-wild photographs of people covering a wide range of different environments. The method is analyzed quantitatively and qualitatively, and we show that ESP outperforms state-of-the-art on the task of contextual full-body generation.</p>

<p><a href="https://esp.is.tue.mpg.de/">Project page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Mirela Ostrek&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[We present ESP, a novel method for context-aware full-body generation, that enables photo-realistic inpainting of people into existing "in-the-wild" photographs.]]></summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS24-25/" rel="alternate" type="text/html" title="Lecture: 3D Scanning and Motion Capture" /><published>2024-08-24T23:00:00+02:00</published><updated>2024-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS24-25</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS24-25/"><![CDATA[<h3 id="content">Content</h3>
<p>The lecture and exercises will cover 3D reconstruction from various input
modalities (Webcams, RGB-D cameras (Kinect, Realsense, …). It will start with
basic concepts of what is 3D, the different representations, how to capture 3D
and how the devices and sensors function. Based on this introduction, rigid and
non-rigid tracking and reconstruction will be discussed. Specialized face and
body tracking methods will be covered and the applications of the 3D
reconstruction and tracking will be shown. In addition to the 3D surface
reconstruction, techniques for appearance modelling and material estimation
will be shown.</p>

<p>++ Basic concepts of geometry (Meshes, Point Clouds, Pixels &amp; Voxels)
++ RGB and Depth Cameras (Calibration, active/passive stereo, Time of
Flight (ToF), Structured Light, Laser Scanner, Lidar)
++ Surface Representations (Polygonal meshes, parametric surfaces,
implicit surfaces (Radial basis functions, signed distance functions,
indicator function), Marching cubes)
++ Overview of reconstruction methods (Structure from Motion (SfM),
Multi-view Stereo (MVS), SLAM, Bundle Adjustment)
++ Rigid Surface Tracking &amp; Reconstruction (Pose alignment, ICP, online
surface reconstruction pipeline (KinectFusion), scalable surface
representations (VoxelHashing, OctTrees), loop closures and global
optimization)
++ Non-rigid Surface Tracking &amp; Reconstruction (Surface deformation for
modeling, Regularizers: ARAP, ED, etc., Non-rigid surface fitting: e.g.,
non-rigid ICP. Non-rigid reconstruction:
DynamicFusion/VolumeDeform/KillingFusion)
++ Face Tracking &amp; Reconstruction (Keypoint detection &amp; tracking,
Parametric / Statistical Models -&gt; BlendShapes)
++ Body Tracking &amp; Reconstruction (Skeleton Tracking and Inverse
Kinematics, Marker-based motion capture)
++ Material capture (Lightstage, BRDF estimation)
++ Outlook DeepLearning-based tracking</p>

<h3 id="aim">Aim</h3>
<p>Basic understanding of 3D capturing devices and underlying principles (active vs.
passive stereo, ToF etc.), modelling of geometry and conversion between
different representations, principles of static reconstruction (fusion, ICP) and
non-rigid reconstruction using deformation priors. Basic understanding of
specialized class-specific tracking (face, body, hands) and their applications.</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[This hands-on lecture focuses on cutting-edge 3D reconstruction approaches, including volumetric fusion approaches on implicit functions, face tracking, hand tracking, and much more! We also cover essential optimization techniques such as Gauss-Newton and Levenberg-Marquardt.]]></summary></entry></feed>