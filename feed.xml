<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2021-05-10T11:22:59+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Dynamic Surface Function Networks for Clothed Human Bodies</title><link href="https://justusthies.github.io/posts/dsfn/" rel="alternate" type="text/html" title="Dynamic Surface Function Networks for Clothed Human Bodies" /><published>2021-04-12T10:00:00+02:00</published><updated>2021-04-12T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/dsfn</id><content type="html" xml:base="https://justusthies.github.io/posts/dsfn/">&lt;p&gt;We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Andrei Burov&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/andrei_burov/profile.html&quot;}</name></author><category term="ARXIV" /><summary type="html">We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.</summary></entry><entry><title type="html">Neural Parametric Models for 3D Deformable Shapes</title><link href="https://justusthies.github.io/posts/npm/" rel="alternate" type="text/html" title="Neural Parametric Models for 3D Deformable Shapes" /><published>2021-04-12T09:30:00+02:00</published><updated>2021-04-12T09:30:00+02:00</updated><id>https://justusthies.github.io/posts/npm</id><content type="html" xml:base="https://justusthies.github.io/posts/npm/">&lt;p&gt;Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Pablo Palafox&quot;, &quot;website_link&quot;=&gt;&quot;https://pablorpalafox.github.io&quot;}</name></author><category term="ARXIV" /><summary type="html">Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs.</summary></entry><entry><title type="html">Neural 3D Scene Reconstruction with a Database</title><link href="https://justusthies.github.io/posts/retrievalfuse/" rel="alternate" type="text/html" title="Neural 3D Scene Reconstruction with a Database" /><published>2021-04-12T09:00:00+02:00</published><updated>2021-04-12T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/retrievalfuse</id><content type="html" xml:base="https://justusthies.github.io/posts/retrievalfuse/">&lt;p&gt;3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In contrast to traditional generative learned models which encode the full generative process into a neural network and can struggle with maintaining local details at the scene level, we introduce a new method that directly leverages scene geometry from the training database. First, we learn to synthesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can effectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neural scene reconstruction with a database for the tasks of 3D super-resolution and surface reconstruction from sparse point clouds, showing that our approach enables generation of more coherent, accurate 3D scenes, improving on average by over 8% in IoU over state-of-the-art scene reconstruction.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Yawar Siddiqui&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/yawar_siddiqui/profile.html&quot;}</name></author><category term="ARXIV" /><summary type="html">3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In contrast to traditional generative learned models which encode the full generative process into a neural network and can struggle with maintaining local details at the scene level, we introduce a new method that directly leverages scene geometry from the training database. First, we learn to synthesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can effectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neural scene reconstruction with a database for the tasks of 3D super-resolution and surface reconstruction from sparse point clouds, showing that our approach enables generation of more coherent, accurate 3D scenes, improving on average by over 8% in IoU over state-of-the-art scene reconstruction.</summary></entry><entry><title type="html">Neural RGB-D Surface Reconstruction</title><link href="https://justusthies.github.io/posts/rgbdnerf/" rel="alternate" type="text/html" title="Neural RGB-D Surface Reconstruction" /><published>2021-04-12T08:30:00+02:00</published><updated>2021-04-12T08:30:00+02:00</updated><id>https://justusthies.github.io/posts/rgbdnerf</id><content type="html" xml:base="https://justusthies.github.io/posts/rgbdnerf/">&lt;p&gt;In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dejan Azinovic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/dejan_azinovic/profile.html&quot;}</name></author><category term="ARXIV" /><summary type="html">In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.</summary></entry><entry><title type="html">NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction</title><link href="https://justusthies.github.io/posts/nerface/" rel="alternate" type="text/html" title="NerFACE&amp;#58; Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction" /><published>2021-03-03T08:00:00+01:00</published><updated>2021-03-03T08:00:00+01:00</updated><id>https://justusthies.github.io/posts/nerface</id><content type="html" xml:base="https://justusthies.github.io/posts/nerface/">&lt;p&gt;We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face.
Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications.
Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoint or head-poses is required.
In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks.
To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions.
We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup.
In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gafniguy.github.io/4D-Facial-Avatars&quot;&gt;Project Page&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Guy Gafni&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/guy_gafni/profile.html&quot;}</name></author><category term="CVPR" /><summary type="html">We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoint or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.</summary></entry><entry><title type="html">Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction</title><link href="https://justusthies.github.io/posts/neuraldeformationgraphs/" rel="alternate" type="text/html" title="Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction" /><published>2021-03-03T07:00:00+01:00</published><updated>2021-03-03T07:00:00+01:00</updated><id>https://justusthies.github.io/posts/neuraldeformationgraphs</id><content type="html" xml:base="https://justusthies.github.io/posts/neuraldeformationgraphs/">&lt;p&gt;We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. 
Specifically, we implicitly model a deformation graph via a deep neural network.
This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking.
Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object.
Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints,  the underlying network is trained in a self-supervised fashion.
We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation.
Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings.
Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="CVPR" /><summary type="html">We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.</summary></entry><entry><title type="html">Self-Supervised Photometric Scene Generation from RGB-D Scans</title><link href="https://justusthies.github.io/posts/spsg/" rel="alternate" type="text/html" title="Self-Supervised Photometric Scene Generation from RGB-D Scans" /><published>2021-03-02T09:00:00+01:00</published><updated>2021-03-02T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/spsg</id><content type="html" xml:base="https://justusthies.github.io/posts/spsg/">&lt;p&gt;We present Self-Supervised Photometric Scene Generation (SPSG), a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion.
Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan.
Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes.
This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies.
Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Angela Dai&quot;, &quot;website_link&quot;=&gt;&quot;https://angeladai.github.io&quot;}</name></author><category term="CVPR" /><summary type="html">We present Self-Supervised Photometric Scene Generation (SPSG), a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion. Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan. Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes. This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies. Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.</summary></entry><entry><title type="html">Neural Capture and Synthesis at the Max Planck Institute for Intelligent Systems</title><link href="https://justusthies.github.io/posts/mpi-neural-capture-and-synthesis/" rel="alternate" type="text/html" title="Neural Capture and Synthesis at the Max Planck Institute for Intelligent Systems" /><published>2021-01-31T10:00:00+01:00</published><updated>2021-01-31T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/mpi-neural-capture-and-synthesis</id><content type="html" xml:base="https://justusthies.github.io/posts/mpi-neural-capture-and-synthesis/">&lt;p&gt; &lt;/p&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
I am happy to announce that I am joining the Max Planck Institute for Intelligent Systems as a research group leader in April 2021.
My Max Planck Research Group 'Neural Capture and Synthesis' will work at the intersection of computer graphics, computer vision and machine learning.
&lt;/div&gt;

&lt;div style=&quot;text-align: justify&quot;&gt; 
The main theme of my work is to capture and to (re-)synthesize the real world using commodity hardware.
It includes the modeling of the human body, tracking, as well as the reconstruction and interaction with the environment.
The digitization is needed for various applications in AR/VR as well as in movie (post-)production.
Teleconferencing and working in VR is of high interest for many companies ranging from social media platforms to car manufacturer.
It enables the remote interaction in VR, e.g., the inspection of 3D content like CAD models or scans from real objects.
A realistic reproduction of appearances and motions is key for such applications. 
Capturing natural motions and expressions as well as the photorealistic reproduction of images under novel views are challenging.
With the rise of deep learning methods and, especially, neural rendering, we see immense progress to succeed in these challenges.
The goal of my work is to develop methods for AI-based image synthesis of humans, the underlying representation of appearance, geometry and motion to allow for explicit and implicit control over the synthesis process.
My work on 3D reconstruction, tracking and rendering does not focus exclusively on humans but also on the environment and objects we interact with, thus, enabling applications like 3d telepresence or collaborative working in VR.
In both areas, reconstruction and rendering, hybrid approaches that combine novel findings in machine learning with classical computer graphics and computer vision approaches show promising results.
Nevertheless, these methods still suffer from limitations like generalizability, controllability and editability which I will tackle in my ongoing and future work.
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;face_overview.jpg&quot; alt=&quot;Face Projects&quot; /&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;scene_reco.jpg&quot; alt=&quot;Scene Reconstruction Projects&quot; /&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;non-rigid-tracking.jpg&quot; alt=&quot;Neural Non-rigid Tracking Projects&quot; /&gt;  &lt;/p&gt;

&lt;p&gt;On my &lt;a href=&quot;https://justusthies.github.io/publications/&quot;&gt;website&lt;/a&gt; and on my &lt;a href=&quot;https://www.youtube.com/channel/UCwmSTvnV-sjtlIlNvWYC6Ow&quot;&gt;YouTube page&lt;/a&gt; you will find a couple of interesting projects I have been working on in the past:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=p7UIF1Mw5GI&amp;amp;list=PLVOhpVSqYrEYMhjQ0MQ62JZgtRGTW5hPW&amp;amp;ab_channel=JustusThies&quot;&gt;Facial Reenactment / Reconstruction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ofVgAEb1FiE&amp;amp;list=PLVOhpVSqYrEbIFkFsvdtO8FUkqbmn7cUQ&amp;amp;ab_channel=JustusThies&quot;&gt;Neural Rendering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Kj_P-lHtWkU&amp;amp;list=PLVOhpVSqYrEYqHZIEfY4R6Lgp90LJFUd2&amp;amp;ab_channel=JustusThies&quot;&gt;Neural Non-rigid Tracking&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=52xlRn0ESek&amp;amp;list=PLVOhpVSqYrEZ6D3FFabhQolw5ZRCNVfiI&amp;amp;ab_channel=MatthiasNiessner&quot;&gt;3D Reconstruction / Texturing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Tle7YaPkO_k&amp;amp;list=PLVOhpVSqYrEaZUh3ZktMfXwWuiQTMvuV8&amp;amp;ab_channel=MatthiasNiessner&quot;&gt;Multi-media Forensics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;openings&quot;&gt;Openings&lt;/h1&gt;
&lt;div style=&quot;text-align: justify&quot;&gt; 
My group as several open positions for Postdocs and PhD students.
If you are interested and highly motivated to work in the above mentioned research fields, please write me an email (&lt;a href=&quot;mailto:justus.thies@tuebingen.mpg.de&quot;&gt;justus.thies@tuebingen.mpg.de&lt;/a&gt;)
including the relevant application documents (CV, recommendation letters, transcript of records, etc.) and a short description of your research interests / plans (research statement).
&lt;/div&gt;

&lt;h3 id=&quot;qualification&quot;&gt;Qualification&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Applicants for a PhD must hold a Master’s degree in computer science or equivalent&lt;/li&gt;
  &lt;li&gt;Fluent written and spoken English skills are essential&lt;/li&gt;
  &lt;li&gt;Proficient coding skills (Python and/or C++)&lt;/li&gt;
  &lt;li&gt;Experience with deep learning frameworks (TensorFlow / PyTorch)&lt;/li&gt;
  &lt;li&gt;Excitement, self-motivation, and commitment to revolutionize the field&lt;/li&gt;
  &lt;li&gt;Post Doc Applicants Only: academic track record with publications at top-tier venues in computer vision,
graphics, or machine learning (CVPR, ECCV/ICCV, Siggraph, Siggraph Asia, NeurIPS, ICML, ICLR)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-apply&quot;&gt;How to Apply&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Documents required: CV, research statement, BA and MA transcripts, MA thesis [optional]&lt;/li&gt;
  &lt;li&gt;Ask two recommenders who know your work to directly email recommendation letters&lt;/li&gt;
  &lt;li&gt;Send all documents (incl. rec. letters) directly to Justus Thies (&lt;a href=&quot;mailto:justus.thies@tuebingen.mpg.de&quot;&gt;justus.thies@tuebingen.mpg.de&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Please understand that we cannot review incomplete applications&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;additional-doctoral-programs&quot;&gt;Additional doctoral programs&lt;/h3&gt;

&lt;p&gt;The MPI for Intelligent System also offers the following doctoral programs you can apply for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;div style=&quot;text-align: justify&quot;&gt; The &lt;a href=&quot;https://imprs.is.mpg.de/&quot;&gt;International Max Planck Research School for Intelligent Systems&lt;/a&gt; (IMPRS-IS) brings together the MPI for Intelligent Systems with the University of Stuttgart and the University of Tübingen to form a highly visible and unique graduate school of internationally recognized faculty, working at the leading edge of the field.&lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;div style=&quot;text-align: justify&quot;&gt; The &lt;a href=&quot;https://learning-systems.org/&quot;&gt;Max Planck ETH Center for Learning Systems&lt;/a&gt; (CLS) is a doctoral training program. Ph.D. students are jointly supervised by ETH Professors and MPI Directors and Group Leaders, and obtain their doctoral degree from ETH Zurich.&lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="MPIIS" /><summary type="html">  I am happy to announce that I am joining the Max Planck Institute for Intelligent Systems as a research group leader in April 2021. My Max Planck Research Group 'Neural Capture and Synthesis' will work at the intersection of computer graphics, computer vision and machine learning.</summary></entry><entry><title type="html">ID-Reveal: Identity-aware DeepFake Video Detection</title><link href="https://justusthies.github.io/posts/idreveal/" rel="alternate" type="text/html" title="ID-Reveal&amp;#58; Identity-aware DeepFake Video Detection" /><published>2020-12-08T06:00:00+01:00</published><updated>2020-12-08T06:00:00+01:00</updated><id>https://justusthies.github.io/posts/idreveal</id><content type="html" xml:base="https://justusthies.github.io/posts/idreveal/">&lt;p&gt;State-of-the-art DeepFake forgery detectors are trained in a supervised fashion to answer the question ‘is this video real or fake?’.
Given that their training is typically method-specific, these approaches show poor generalization across different types of facial manipulations, e.g., face swapping or facial reenactment.
In this work, we look at the problem from a different perspective by focusing on the facial characteristics of a specific identity; i.e., we want to answer the question ‘Is this the person who is claimed to be?’.
To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how each person moves while talking, by means of metric learning coupled with an adversarial training strategy. 
ur method is independent of the specific type of manipulation since it is trained only on real videos. Moreover, relying on high-level semantic features, it is robust to widespread and disruptive forms of post-processing.
We performed a thorough experimental analysis on several publicly available benchmarks, such as FaceForensics++, Google’s DFD, and Celeb-DF.
Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks.
In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/grip-unina/id-reveal&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Davide Cozzolino&quot;, &quot;website_link&quot;=&gt;&quot;http://www.grip.unina.it/people/userprofile/davide_cozzolino.html&quot;}</name></author><category term="ArXiv" /><summary type="html">State-of-the-art DeepFake forgery detectors are trained in a supervised fashion to answer the question ‘is this video real or fake?’. Given that their training is typically method-specific, these approaches show poor generalization across different types of facial manipulations, e.g., face swapping or facial reenactment. In this work, we look at the problem from a different perspective by focusing on the facial characteristics of a specific identity; i.e., we want to answer the question ‘Is this the person who is claimed to be?’. To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how each person moves while talking, by means of metric learning coupled with an adversarial training strategy. ur method is independent of the specific type of manipulation since it is trained only on real videos. Moreover, relying on high-level semantic features, it is robust to widespread and disruptive forms of post-processing. We performed a thorough experimental analysis on several publicly available benchmarks, such as FaceForensics++, Google’s DFD, and Celeb-DF. Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks. In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.</summary></entry><entry><title type="html">BIDT: Echt oder?</title><link href="https://justusthies.github.io/posts/bidt/" rel="alternate" type="text/html" title="BIDT&amp;#58; Echt oder?" /><published>2020-10-20T18:00:00+02:00</published><updated>2020-10-20T18:00:00+02:00</updated><id>https://justusthies.github.io/posts/bidt</id><content type="html" xml:base="https://justusthies.github.io/posts/bidt/">&lt;h2 id=&quot;was-ist-ein-deepfake&quot;&gt;Was ist ein DeepFake?&lt;/h2&gt;
&lt;p&gt;Die Bezeichnung DeepFake wird umgangssprachlich für jegliche Art der digitalen Bild- und Videomanipulation genutzt. Der Begriff DeepFake wurde vor allem von der digitalen Manipulation von Portraitbildern geprägt. Da neuere Methoden auf maschinellem Lernen, insbesondere dem sog. Deep Learning, basieren, um photorealistische Inhalte zu erzeugen, kam es zu der Wortneuschöpfung Deep-Fake. DeepFakes sind bereits weit verbreitet und entsprechende Programme werden von einer Vielzahl von Nutzern verwendet. Einige „Bildfilter“ in Apps wie Instagram, die das Aussehen einer Person verändern, können als DeepFakes aufgefasst werden. Insbesondere Face Swapping, also das Vertauschen von zwei Gesichtern, wird als DeepFake bezeichnet. Solche Manipulationen erzeugen virtuelle Personen, die so nicht existieren (das Gesicht passt nicht zum Rest des Körpers). Jedoch gibt es auch Techniken, die die Identität einer Person erhalten und nur die Gesichtszüge verändern - man spricht hierbei vom Facial Reenactment. D.h. der Gesichtsausdruck kann von einer Person auf eine andere übertragen werden.&lt;/p&gt;

&lt;h2 id=&quot;inwieweit-können-deepfakes-zu-einer-gefahr-für-die-gesellschaft-werden&quot;&gt;Inwieweit können DeepFakes zu einer Gefahr für die Gesellschaft werden?&lt;/h2&gt;
&lt;p&gt;DeepFakes sind zurzeit bereits eine Gefahr für einzelne Personen. Cyber-Mobbing (also digitales Mobbing) durch manipulierte Bilder und sogenannte FakePorns (Gesicht einer Person wird in ein Pornovideo eingefügt) werden bereits zur Diskreditierung von Personen erstellt.
Für die Gesellschaft im Allgemeinen ergeben sich auch gewisse Gefahren. Allein die Frage ob ein Inhalt gefälscht wurde oder nicht, hat einen Vertrauensverlust als Folge. Politiker sähen bereits Zweifel an diversen Berichterstattungen mit dem Hinweis auf fake news (siehe USA). Diese Entwicklungen sind besorgniserregend, da es für die Menschen zunehmend schwieriger wird, zu erkennen was der Realität entspricht und was nicht.&lt;/p&gt;

&lt;h2 id=&quot;welche-anwendungsfälle-gibt-es-für-solche-digital-erzeugten-medien&quot;&gt;Welche Anwendungsfälle gibt es für solche digital erzeugten Medien?&lt;/h2&gt;
&lt;p&gt;Die grundlegenden Techniken mit denen DeepFakes erstellt werden, haben diverse Anwendungsfälle. Digitale Inhalte sind omnipräsent, augmented reality (angereicherte Realität) und virtual reality (virtuelle Realität) kurz AR bzw. VR finden immer weitere Anwendungsfelder. Firmen arbeiten mit Hochdruck daran Telekonferenzsysteme weiterzuentwickeln - anstatt einfache 2D Videos ist es das Ziel 3D Inhalte zu erzeugen. 3D Inhalte haben den Vorteil, dass der Nutzer entscheiden kann, wo er hinschauen möchte - Telekonferenzen in 3D werden möglich. Die grundlegenden Algorithmen, um dies zu erreichen, basieren auch auf den Techniken und Methoden, die zu DeepFakes benutzt werden. Es geht darum photorealistische Abbilder von uns Menschen zu erzeugen (einen digitalen Double). Virtuelle Spiegel, um Makeup zu testen, eine virtuelle Anprobe von Kleidung, das Simulieren von chirurgischen Operationen, all dies wird durch einen digitalen Double möglich.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://padlet.com/bidt_Dialog/werkstattdigital_Deepfakes&quot;&gt;Padlet website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="BIDT" /><summary type="html">Was ist ein DeepFake? Die Bezeichnung DeepFake wird umgangssprachlich für jegliche Art der digitalen Bild- und Videomanipulation genutzt. Der Begriff DeepFake wurde vor allem von der digitalen Manipulation von Portraitbildern geprägt. Da neuere Methoden auf maschinellem Lernen, insbesondere dem sog. Deep Learning, basieren, um photorealistische Inhalte zu erzeugen, kam es zu der Wortneuschöpfung Deep-Fake. DeepFakes sind bereits weit verbreitet und entsprechende Programme werden von einer Vielzahl von Nutzern verwendet. Einige „Bildfilter“ in Apps wie Instagram, die das Aussehen einer Person verändern, können als DeepFakes aufgefasst werden. Insbesondere Face Swapping, also das Vertauschen von zwei Gesichtern, wird als DeepFake bezeichnet. Solche Manipulationen erzeugen virtuelle Personen, die so nicht existieren (das Gesicht passt nicht zum Rest des Körpers). Jedoch gibt es auch Techniken, die die Identität einer Person erhalten und nur die Gesichtszüge verändern - man spricht hierbei vom Facial Reenactment. D.h. der Gesichtsausdruck kann von einer Person auf eine andere übertragen werden.</summary></entry></feed>