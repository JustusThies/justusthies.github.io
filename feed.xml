<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2020-09-30T13:35:13+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Neural Non-Rigid Tracking</title><link href="https://justusthies.github.io/posts/neuraltracking/" rel="alternate" type="text/html" title="Neural Non-Rigid Tracking" /><published>2020-09-29T11:00:00+02:00</published><updated>2020-09-29T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuraltracking</id><content type="html" xml:base="https://justusthies.github.io/posts/neuraltracking/">&lt;p&gt;We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction.
Given two input RGB-D frames of a non-rigidly moving object, we employ a convolutional neural network to predict dense correspondences.
These correspondences are used as constraints in an as-rigid-as-possible (ARAP) optimization problem.
By enabling gradient back-propagation through the non-rigid optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking.
Furthermore, this formulation allows for learning correspondence weights in a self-supervised manner.
Thus, outliers and wrong correspondences are down-weighted to enable robust tracking.
Compared to state-of-the-art approaches, our algorithm shows improved reconstruction performance, while simultaneously achieving 85x faster correspondence prediction than comparable deep-learning based methods.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="NeurIPS" /><summary type="html">We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction. Given two input RGB-D frames of a non-rigidly moving object, we employ a convolutional neural network to predict dense correspondences. These correspondences are used as constraints in an as-rigid-as-possible (ARAP) optimization problem. By enabling gradient back-propagation through the non-rigid optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking. Furthermore, this formulation allows for learning correspondence weights in a self-supervised manner. Thus, outliers and wrong correspondences are down-weighted to enable robust tracking. Compared to state-of-the-art approaches, our algorithm shows improved reconstruction performance, while simultaneously achieving 85x faster correspondence prediction than comparable deep-learning based methods.</summary></entry><entry><title type="html">Egocentric Videoconferencing</title><link href="https://justusthies.github.io/posts/egochat/" rel="alternate" type="text/html" title="Egocentric Videoconferencing" /><published>2020-09-28T11:00:00+02:00</published><updated>2020-09-28T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/egochat</id><content type="html" xml:base="https://justusthies.github.io/posts/egochat/">&lt;p&gt;We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices.
Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera.
Using a frontal camera in a hands-free setting when a person is on the move is impractical.
Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. 
To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses.
Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video.
To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. 
Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods.
We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. 
To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face.
This allows us to synthesis results at different head poses. 
Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator.
We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Mohamed Elgharib&quot;, &quot;website_link&quot;=&gt;&quot;http://people.mpi-inf.mpg.de/~elgharib/&quot;}</name></author><category term="Siggraph Asia" /><summary type="html">We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices. Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera. Using a frontal camera in a hands-free setting when a person is on the move is impractical. Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses. Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video. To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods. We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face. This allows us to synthesis results at different head poses. Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator. We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.</summary></entry><entry><title type="html">WIPO: Artifical Intelligence and Intellectual Property</title><link href="https://justusthies.github.io/posts/wipo/" rel="alternate" type="text/html" title="WIPO&amp;#58; Artifical Intelligence and Intellectual Property" /><published>2020-09-18T09:00:00+02:00</published><updated>2020-09-18T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/wipo</id><content type="html" xml:base="https://justusthies.github.io/posts/wipo/">&lt;p&gt;This virtual exhibition presents current approaches of AI-driven media creation.
It raises interesting questions regarding intellectual property.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.wipo.int/about-ip/en/artificial_intelligence/&quot;&gt;Weblink to the official website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="WIPO" /><summary type="html">This virtual exhibition presents current approaches of AI-driven media creation. It raises interesting questions regarding intellectual property.</summary></entry><entry><title type="html">Seminar: 3D Vision</title><link href="https://justusthies.github.io/posts/3D-Vision-WS20-21/" rel="alternate" type="text/html" title="Seminar&amp;#58; 3D Vision" /><published>2020-08-24T23:00:00+02:00</published><updated>2020-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Vision-WS20-21</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Vision-WS20-21/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).&lt;/p&gt;

&lt;p&gt;The talks have to cover the following topics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;face tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;facial reenactment (audio driven, video driven animation)&lt;/li&gt;
  &lt;li&gt;body tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;hand tracking&lt;/li&gt;
  &lt;li&gt;static / dynamic scene reconstruction&lt;/li&gt;
  &lt;li&gt;scene understanding&lt;/li&gt;
  &lt;li&gt;inverse rendering, material estimation, light estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;organization--requirements&quot;&gt;Organization / Requirements&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Lecture: 3D Scanning &amp;amp; Motion Capturing&lt;/li&gt;
  &lt;li&gt;The participants have to present their topics in a talk (in English), which should last 30 minutes.&lt;/li&gt;
  &lt;li&gt;The semi-final slides (PDF) should be sent one week before the talk; otherwise, the talk will be canceled.&lt;/li&gt;
  &lt;li&gt;A short report (approximately 2-3 pages in the ACM SIGGRAPH TOG format (acmtog)) should be prepared and sent within two weeks after the talk. When you send the report, please send the final slides (PDF) together.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).</summary></entry><entry><title type="html">Practical Course: 3D Scanning and Spatial Learning</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS20-21/" rel="alternate" type="text/html" title="Practical Course&amp;#58; 3D Scanning and Spatial Learning" /><published>2020-08-24T23:00:00+02:00</published><updated>2020-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS20-21</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS20-21/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp;amp; Motion Capture as well as by the Introduction to Deep Learning lecture.
In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field.
Specifically, we will have projects on:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;human motion capturing (e.g., Fusion4D, BodyFusion)&lt;/li&gt;
  &lt;li&gt;real-time facial motion capturing (spare and dense approaches)&lt;/li&gt;
  &lt;li&gt;3D scene reconstruction (e.g., BundleFusion)&lt;/li&gt;
  &lt;li&gt;scan refinement (e.g., ShapeFromShading)&lt;/li&gt;
  &lt;li&gt;neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering)&lt;/li&gt;
  &lt;li&gt;scene completion (e.g., ScanComplete)&lt;/li&gt;
  &lt;li&gt;3D object retrieval and alignment (e.g., Scan2CAD)&lt;/li&gt;
  &lt;li&gt;scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS)&lt;/li&gt;
  &lt;li&gt;neural scene representations (DeepSDF, OccupancyNets, NeRF,…)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;objective&quot;&gt;Objective&lt;/h3&gt;
&lt;p&gt;Upon completion of this module, students will have acquired extensive theoretical concepts behind state-of-the art 3D reconstruction methods, in particular in the context of human motion capturing, static object scanning, scene understanding and synthesis of captured scenes. Besides the theoretical foundations, a significant aspect lies on the practical realization and implementation of such algorithms.&lt;/p&gt;

&lt;h3 id=&quot;organization&quot;&gt;Organization&lt;/h3&gt;
&lt;p&gt;In the practical course students shall get familiar with state-of-the-art 3D scanning. They will be assisted by current PhD students working in this field (regular office hours). To ensure a good progress during the semester, we will have mandatory meetings (every two weeks) where the students report their current state. In the end of the course, the students are asked to give a talk about their project and results.&lt;/p&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, 3D scanning &amp;amp; Motion Capture, Introduction to Deep Learning, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp;amp; Motion Capture as well as by the Introduction to Deep Learning lecture. In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field. Specifically, we will have projects on: human motion capturing (e.g., Fusion4D, BodyFusion) real-time facial motion capturing (spare and dense approaches) 3D scene reconstruction (e.g., BundleFusion) scan refinement (e.g., ShapeFromShading) neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering) scene completion (e.g., ScanComplete) 3D object retrieval and alignment (e.g., Scan2CAD) scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS) neural scene representations (DeepSDF, OccupancyNets, NeRF,…)</summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21/" rel="alternate" type="text/html" title="Lecture&amp;#58; 3D Scanning and Motion Capture" /><published>2020-08-24T23:00:00+02:00</published><updated>2020-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic concepts of geometry
    &lt;ul&gt;
      &lt;li&gt;Meshes (polygonal), Point Clouds, Pixels &amp;amp; Voxels&lt;/li&gt;
      &lt;li&gt;RGB and Depth Cameras&lt;/li&gt;
      &lt;li&gt;Extrinsics and Intrinsics&lt;/li&gt;
      &lt;li&gt;Capture devices&lt;/li&gt;
      &lt;li&gt;RGB and Multi-view&lt;/li&gt;
      &lt;li&gt;RGB-D cameras&lt;/li&gt;
      &lt;li&gt;Stereo&lt;/li&gt;
      &lt;li&gt;Time of Flight (ToF)&lt;/li&gt;
      &lt;li&gt;Structured Light&lt;/li&gt;
      &lt;li&gt;Laser Scanner, Lidar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Surface Representations
    &lt;ul&gt;
      &lt;li&gt;Polygonal meshes (trimeshes, etc.)&lt;/li&gt;
      &lt;li&gt;Parametric surfaces: splines, nurbs&lt;/li&gt;
      &lt;li&gt;Implicit surfaces&lt;/li&gt;
      &lt;li&gt;Ridge-based surfaces&lt;/li&gt;
      &lt;li&gt;Radial basis functions&lt;/li&gt;
      &lt;li&gt;Signed distance functions (volumetric, Curless &amp;amp; Levoy)&lt;/li&gt;
      &lt;li&gt;Indicator function (Poisson Surface Reconstruction)&lt;/li&gt;
      &lt;li&gt;More general: level sets&lt;/li&gt;
      &lt;li&gt;Marching cubes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;High-level overview of reconstructions
    &lt;ul&gt;
      &lt;li&gt;Structure from Motion (SfM)&lt;/li&gt;
      &lt;li&gt;Multi-view Stereo (MVS)&lt;/li&gt;
      &lt;li&gt;SLAM&lt;/li&gt;
      &lt;li&gt;Bundle Adjustment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimization
    &lt;ul&gt;
      &lt;li&gt;Non-linear least squares&lt;/li&gt;
      &lt;li&gt;Gauss-Newton LM&lt;/li&gt;
      &lt;li&gt;Examples in Ceres&lt;/li&gt;
      &lt;li&gt;Symbolic diff vs auto-diff&lt;/li&gt;
      &lt;li&gt;Auto-diff with dual numbers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Pose alignment&lt;/li&gt;
      &lt;li&gt;ICP (point cloud alignment; depth-to-model alignment; rigid ‘fitting’)&lt;/li&gt;
      &lt;li&gt;Online surface reconstruction pipeline: KinectFusion&lt;/li&gt;
      &lt;li&gt;Scalable surface representations: VoxelHashing, OctTrees&lt;/li&gt;
      &lt;li&gt;Loop closures and global optimization&lt;/li&gt;
      &lt;li&gt;Robust optimization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Non-rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Surface deformation for modeling&lt;/li&gt;
      &lt;li&gt;Regularizers: ARAP, ED, etc.&lt;/li&gt;
      &lt;li&gt;Non-rigid surface fitting: e.g., non-rigid ICP&lt;/li&gt;
      &lt;li&gt;Non-rigid reconstruction: DynamicFusion/VolumeDeform/KillingFusion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Body Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Skeleton Tracking and Inverse Kinematics&lt;/li&gt;
      &lt;li&gt;Learning-based approaches from RGB and RGB-D&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Face Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Keypoint detection &amp;amp; tracking&lt;/li&gt;
      &lt;li&gt;Parametric / Statistical Models -&amp;gt; BlendShapes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hand Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Parametric Models&lt;/li&gt;
      &lt;li&gt;Some DeepLearning-based things&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminative vs generative tracking
    &lt;ul&gt;
      &lt;li&gt;Random forests&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Motion Capture in Movies
    &lt;ul&gt;
      &lt;li&gt;Marker-based motion capture&lt;/li&gt;
      &lt;li&gt;LightStage -&amp;gt; movies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BRDF and Material Capture&lt;/li&gt;
  &lt;li&gt;Open research questions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).</summary></entry><entry><title type="html">Learning Adaptive Sampling and Reconstruction for Volume Visualization</title><link href="https://justusthies.github.io/posts/learningadaptivesampling/" rel="alternate" type="text/html" title="Learning Adaptive Sampling and Reconstruction for Volume Visualization" /><published>2020-07-22T11:00:00+02:00</published><updated>2020-07-22T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/learningadaptivesampling</id><content type="html" xml:base="https://justusthies.github.io/posts/learningadaptivesampling/">&lt;p&gt;A central challenge in data visualization is to understand which data samples are required to generate an image of a data set in which the relevant information is encoded.
In this work, we make a first step towards answering the question of whether an artificial neural network can predict where to sample the data with higher or lower density, by learning of correspondences between the data, the sampling patterns and the generated images.
We introduce a novel neural rendering pipeline, which is trained end-to-end to generate a sparse adaptive sampling structure from a given low-resolution input image, and reconstructs a high-resolution image from the sparse set of samples.
For the first time, to the best of our knowledge, we demonstrate that the selection of structures that are relevant for the final visual representation can be jointly learned together with the reconstruction of this representation from these structures.
Therefore, we introduce differentiable sampling and reconstruction stages, which can leverage back-propagation based on supervised losses solely on the final image.
We shed light on the adaptive sampling patterns generated by the network pipeline and analyze its use for volume visualization including isosurface and direct volume rendering.&lt;/p&gt;

&lt;h2 id=&quot;source-code&quot;&gt;&lt;a href=&quot;https://github.com/shamanDevel/AdaptiveSampling&quot;&gt;Source Code&lt;/a&gt;&lt;/h2&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Sebastian Weiss&quot;, &quot;website_link&quot;=&gt;&quot;https://www.in.tum.de/cg/people/weiss/&quot;}</name></author><category term="ArXiv" /><summary type="html">A central challenge in data visualization is to understand which data samples are required to generate an image of a data set in which the relevant information is encoded. In this work, we make a first step towards answering the question of whether an artificial neural network can predict where to sample the data with higher or lower density, by learning of correspondences between the data, the sampling patterns and the generated images. We introduce a novel neural rendering pipeline, which is trained end-to-end to generate a sparse adaptive sampling structure from a given low-resolution input image, and reconstructs a high-resolution image from the sparse set of samples. For the first time, to the best of our knowledge, we demonstrate that the selection of structures that are relevant for the final visual representation can be jointly learned together with the reconstruction of this representation from these structures. Therefore, we introduce differentiable sampling and reconstruction stages, which can leverage back-propagation based on supervised losses solely on the final image. We shed light on the adaptive sampling patterns generated by the network pipeline and analyze its use for volume visualization including isosurface and direct volume rendering.</summary></entry><entry><title type="html">Self-Supervised Photometric Scene Generation from RGB-D Scans</title><link href="https://justusthies.github.io/posts/spsg/" rel="alternate" type="text/html" title="Self-Supervised Photometric Scene Generation from RGB-D Scans" /><published>2020-06-24T11:00:00+02:00</published><updated>2020-06-24T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/spsg</id><content type="html" xml:base="https://justusthies.github.io/posts/spsg/">&lt;p&gt;We present Self-Supervised Photometric Scene Generation (SPSG), a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion.
Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan.
Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes.
This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies.
Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Angela Dai&quot;, &quot;website_link&quot;=&gt;&quot;https://angeladai.github.io&quot;}</name></author><category term="ArXiv" /><summary type="html">We present Self-Supervised Photometric Scene Generation (SPSG), a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion. Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan. Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes. This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies. Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.</summary></entry><entry><title type="html">Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition</title><link href="https://justusthies.github.io/posts/intrinsicautoencoder/" rel="alternate" type="text/html" title="Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition" /><published>2020-06-23T12:00:00+02:00</published><updated>2020-06-23T12:00:00+02:00</updated><id>https://justusthies.github.io/posts/intrinsicautoencoder</id><content type="html" xml:base="https://justusthies.github.io/posts/intrinsicautoencoder/">&lt;p&gt;Neural rendering techniques promise efficient photo-realistic image synthesis while at the same time providing rich control over scene parameters by learning the physical image formation process.
While several supervised methods have been proposed for this task, acquiring a dataset of images with accurately aligned 3D models is very difficult.
The main contribution of this work is to lift this restriction by training a neural rendering algorithm from unpaired data.
More specifically, we propose an autoencoder for joint generation of realistic images from synthetic 3D models while simultaneously decomposing real images into their intrinsic shape and appearance properties.
In contrast to a traditional graphics pipeline, our approach does not require to specify all scene properties, such as material parameters and lighting by hand.
Instead, we learn photo-realistic deferred rendering from a small set of 3D models and a larger set of unaligned real images, both of which are easy to acquire in practice.
Simultaneously, we obtain accurate intrinsic decompositions of real images while not requiring paired ground truth.
Our experiments confirm that a joint treatment of rendering and decomposition is indeed beneficial and that our approach outperforms state-of-the-art image-toimage translation baselines both qualitatively and quantitatively.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Hassan Abu Alhaija&quot;, &quot;website_link&quot;=&gt;&quot;https://hassanhaija.github.io/&quot;}</name></author><category term="ArXiv" /><summary type="html">Neural rendering techniques promise efficient photo-realistic image synthesis while at the same time providing rich control over scene parameters by learning the physical image formation process. While several supervised methods have been proposed for this task, acquiring a dataset of images with accurately aligned 3D models is very difficult. The main contribution of this work is to lift this restriction by training a neural rendering algorithm from unpaired data. More specifically, we propose an autoencoder for joint generation of realistic images from synthetic 3D models while simultaneously decomposing real images into their intrinsic shape and appearance properties. In contrast to a traditional graphics pipeline, our approach does not require to specify all scene properties, such as material parameters and lighting by hand. Instead, we learn photo-realistic deferred rendering from a small set of 3D models and a larger set of unaligned real images, both of which are easy to acquire in practice. Simultaneously, we obtain accurate intrinsic decompositions of real images while not requiring paired ground truth. Our experiments confirm that a joint treatment of rendering and decomposition is indeed beneficial and that our approach outperforms state-of-the-art image-toimage translation baselines both qualitatively and quantitatively.</summary></entry><entry><title type="html">CVPR 2020 - Tutorial on Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrenderingtutorial_cvpr/" rel="alternate" type="text/html" title="CVPR 2020 - Tutorial on Neural Rendering" /><published>2020-04-08T11:00:00+02:00</published><updated>2020-04-08T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuralrenderingtutorial_cvpr</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrenderingtutorial_cvpr/">&lt;p&gt;Neural rendering is a new class of deep image and video generation approaches that enable explicit or implicit control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure.
It combines generative machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic outputs.
This tutorial teaches the fundamentals of neural rendering and summarizes recent trends and applications.
Starting with an overview of the underlying graphics, vision and machine learning concepts, we discuss critical aspects of neural rendering approaches.
Specifically, our emphasis is on what aspects of the generated imagery can be controlled, which parts of the pipeline are learned, explicit vs.~implicit control, generalization, and stochastic vs.~deterministic synthesis.
The second half of this tutorial is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence.
Finally, we conclude with a discussion of the social implications of this technology and investigate open research problems.&lt;/p&gt;

&lt;p&gt;##&lt;a href=&quot;https://www.neuralrender.com/&quot;&gt;Tutorial Website&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="CVPR" /><summary type="html">Neural rendering is a new class of deep image and video generation approaches that enable explicit or implicit control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure. It combines generative machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic outputs. This tutorial teaches the fundamentals of neural rendering and summarizes recent trends and applications. Starting with an overview of the underlying graphics, vision and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on what aspects of the generated imagery can be controlled, which parts of the pipeline are learned, explicit vs.~implicit control, generalization, and stochastic vs.~deterministic synthesis. The second half of this tutorial is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of this technology and investigate open research problems.</summary></entry></feed>