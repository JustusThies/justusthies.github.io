<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2021-11-29T14:58:15+01:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">3DV 2021 - Tutorial on the Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrenderingtutorial_3dv/" rel="alternate" type="text/html" title="3DV 2021 - Tutorial on the Advances in Neural Rendering" /><published>2021-11-29T10:00:00+01:00</published><updated>2021-11-29T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/neuralrenderingtutorial_3dv</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrenderingtutorial_3dv/">&lt;p&gt;Neural rendering is a fast evolving field at the intersection of computer graphics and computer vision. In this tutorial, we will talk about the advances in neural rendering, especially the underlying 2D and 3D representations that allow for novel viewpoint synthesis, controllability and editability. Specifically, we will discuss neural rendering methods based on 2D GANs, techniques using 3D Neural Radiance Fields or learnable sphere proxies. Besides methods that handle static content, we will talk about dynamic content as well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://3dv2021.surrey.ac.uk/tutorials/#Tutorial2&quot;&gt;3DV Tutorial Website&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The tutorial is based on our two state-of-the-art reports:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://justusthies.github.io/posts/advancedneuralrenderingstar/&quot;&gt;Advanced Neural Rendering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://justusthies.github.io/posts/neuralrenderingstar/&quot;&gt;Neural Rendering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Michael Zollh\u00F6fer&quot;, &quot;website_link&quot;=&gt;&quot;https://zollhoefer.com&quot;}</name></author><category term="3DV" /><summary type="html">Neural rendering is a fast evolving field at the intersection of computer graphics and computer vision. In this tutorial, we will talk about the advances in neural rendering, especially the underlying 2D and 3D representations that allow for novel viewpoint synthesis, controllability and editability. Specifically, we will discuss neural rendering methods based on 2D GANs, techniques using 3D Neural Radiance Fields or learnable sphere proxies. Besides methods that handle static content, we will talk about dynamic content as well.</summary></entry><entry><title type="html">Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/advancedneuralrenderingstar/" rel="alternate" type="text/html" title="Advances in Neural Rendering" /><published>2021-11-11T09:00:00+01:00</published><updated>2021-11-11T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/advancedneuralrenderingstar</id><content type="html" xml:base="https://justusthies.github.io/posts/advancedneuralrenderingstar/">&lt;p&gt;Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using  differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from  classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline.&lt;/p&gt;

&lt;p&gt;This state-of-the-art report on advances in neural rendering  focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects and scene editing and composition. While most of these approaches are scene-specific, we also discuss techniques that generalize across object classes and can be used for generative tasks. In addition to reviewing these state-of-the-art methods, we provide an overview of fundamental concepts and definitions used in the current literature. We conclude with a discussion on open challenges and social implications.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="ArXiv" /><summary type="html">Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline.</summary></entry><entry><title type="html">Expert Interview on DeepFakes</title><link href="https://justusthies.github.io/posts/eu_report/" rel="alternate" type="text/html" title="Expert Interview on DeepFakes" /><published>2021-08-10T09:00:00+02:00</published><updated>2021-08-10T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/eu_report</id><content type="html" xml:base="https://justusthies.github.io/posts/eu_report/">&lt;p&gt;Expert interview for the European Parliamentary Research Service report on ‘Tackling DeepFakes in European Policy’.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2021)690039&quot;&gt;Weblink to the official website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="EU-PARLIAMENT" /><summary type="html">Expert interview for the European Parliamentary Research Service report on ‘Tackling DeepFakes in European Policy’.</summary></entry><entry><title type="html">SIGGRAPH 2021 - Course on the Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrendering_course_siggraph/" rel="alternate" type="text/html" title="SIGGRAPH 2021 - Course on the Advances in Neural Rendering" /><published>2021-08-08T11:00:00+02:00</published><updated>2021-08-08T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuralrendering_course_siggraph</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrendering_course_siggraph/">&lt;p&gt;Neural rendering is an emerging class of deep image and video generation approaches that enable control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure.
It combines machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic models of scenes.
This course covers the advances in neural rendering over the last year.
We will first cover the fundamentals of machine learning and computer graphics relevant for neural rendering. 
Next, we will present state of the art techniques for the many important neural rendering methods for applications such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence.
Finally, we will conclude with a discussion on the ethical implications of this technology and open research problems.&lt;/p&gt;

&lt;p&gt;##&lt;a href=&quot;https://www.neuralrender.com/&quot;&gt;Tutorial Website&lt;/a&gt;
##&lt;a href=&quot;https://www.youtube.com/watch?v=otly9jcZ0Jg&amp;amp;ab_channel=NeuralRendering&quot;&gt;Video Part 1&lt;/a&gt;
##&lt;a href=&quot;https://www.youtube.com/watch?v=aboFl5ozImM&amp;amp;ab_channel=NeuralRendering&quot;&gt;Video Part 2&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Ayush Tewari&quot;, &quot;website_link&quot;=&gt;&quot;https://people.mpi-inf.mpg.de/~atewari/&quot;}</name></author><category term="SIGGRAPH" /><summary type="html">Neural rendering is an emerging class of deep image and video generation approaches that enable control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure. It combines machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic models of scenes. This course covers the advances in neural rendering over the last year. We will first cover the fundamentals of machine learning and computer graphics relevant for neural rendering. Next, we will present state of the art techniques for the many important neural rendering methods for applications such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we will conclude with a discussion on the ethical implications of this technology and open research problems.</summary></entry><entry><title type="html">TransformerFusion: Monocular RGB Scene Reconstruction using Transformers</title><link href="https://justusthies.github.io/posts/transfusion/" rel="alternate" type="text/html" title="TransformerFusion&amp;#58; Monocular RGB Scene Reconstruction using Transformers" /><published>2021-07-12T09:30:00+02:00</published><updated>2021-07-12T09:30:00+02:00</updated><id>https://justusthies.github.io/posts/transfusion</id><content type="html" xml:base="https://justusthies.github.io/posts/transfusion/">&lt;p&gt;We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="NeurIPS 2021" /><summary type="html">We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.</summary></entry><entry><title type="html">EG Junior Fellow</title><link href="https://justusthies.github.io/posts/eg_junior_fellow/" rel="alternate" type="text/html" title="EG Junior Fellow" /><published>2021-05-14T11:00:00+02:00</published><updated>2021-05-14T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/eg_junior_fellow</id><content type="html" xml:base="https://justusthies.github.io/posts/eg_junior_fellow/">&lt;p&gt;The Eurographics Junior Fellows have elected me to join the EG Junior Fellows.&lt;/p&gt;

&lt;p&gt;‘Eurographics Junior Fellows are young researchers (PhD degree obtained no more than eight years ago) from all over the world who have already made significant contributions to the field of computer graphics, or to the Eurographics Association.’&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.eg.org/wp/junior-fellows/&quot;&gt;Eurographics Website&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="EG" /><summary type="html">The Eurographics Junior Fellows have elected me to join the EG Junior Fellows.</summary></entry><entry><title type="html">Dynamic Surface Function Networks for Clothed Human Bodies</title><link href="https://justusthies.github.io/posts/dsfn/" rel="alternate" type="text/html" title="Dynamic Surface Function Networks for Clothed Human Bodies" /><published>2021-04-12T10:00:00+02:00</published><updated>2021-04-12T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/dsfn</id><content type="html" xml:base="https://justusthies.github.io/posts/dsfn/">&lt;p&gt;We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Andrei Burov&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/andrei_burov/profile.html&quot;}</name></author><category term="ICCV" /><summary type="html">We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.</summary></entry><entry><title type="html">Neural Parametric Models for 3D Deformable Shapes</title><link href="https://justusthies.github.io/posts/npm/" rel="alternate" type="text/html" title="Neural Parametric Models for 3D Deformable Shapes" /><published>2021-04-12T09:30:00+02:00</published><updated>2021-04-12T09:30:00+02:00</updated><id>https://justusthies.github.io/posts/npm</id><content type="html" xml:base="https://justusthies.github.io/posts/npm/">&lt;p&gt;Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Pablo Palafox&quot;, &quot;website_link&quot;=&gt;&quot;https://pablorpalafox.github.io&quot;}</name></author><category term="ICCV" /><summary type="html">Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs.</summary></entry><entry><title type="html">Neural 3D Scene Reconstruction with a Database</title><link href="https://justusthies.github.io/posts/retrievalfuse/" rel="alternate" type="text/html" title="Neural 3D Scene Reconstruction with a Database" /><published>2021-04-12T09:00:00+02:00</published><updated>2021-04-12T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/retrievalfuse</id><content type="html" xml:base="https://justusthies.github.io/posts/retrievalfuse/">&lt;p&gt;3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In contrast to traditional generative learned models which encode the full generative process into a neural network and can struggle with maintaining local details at the scene level, we introduce a new method that directly leverages scene geometry from the training database. First, we learn to synthesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can effectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neural scene reconstruction with a database for the tasks of 3D super-resolution and surface reconstruction from sparse point clouds, showing that our approach enables generation of more coherent, accurate 3D scenes, improving on average by over 8% in IoU over state-of-the-art scene reconstruction.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Yawar Siddiqui&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/yawar_siddiqui/profile.html&quot;}</name></author><category term="ICCV" /><summary type="html">3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In contrast to traditional generative learned models which encode the full generative process into a neural network and can struggle with maintaining local details at the scene level, we introduce a new method that directly leverages scene geometry from the training database. First, we learn to synthesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can effectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neural scene reconstruction with a database for the tasks of 3D super-resolution and surface reconstruction from sparse point clouds, showing that our approach enables generation of more coherent, accurate 3D scenes, improving on average by over 8% in IoU over state-of-the-art scene reconstruction.</summary></entry><entry><title type="html">Neural RGB-D Surface Reconstruction</title><link href="https://justusthies.github.io/posts/rgbdnerf/" rel="alternate" type="text/html" title="Neural RGB-D Surface Reconstruction" /><published>2021-04-12T08:30:00+02:00</published><updated>2021-04-12T08:30:00+02:00</updated><id>https://justusthies.github.io/posts/rgbdnerf</id><content type="html" xml:base="https://justusthies.github.io/posts/rgbdnerf/">&lt;p&gt;In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dejan Azinovic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/dejan_azinovic/profile.html&quot;}</name></author><category term="ARXIV" /><summary type="html">In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.</summary></entry></feed>