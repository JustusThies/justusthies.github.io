<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2023-06-05T11:58:46+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Text-guided Editing of Textured 3D Morphable Models</title><link href="https://justusthies.github.io/posts/clipface/" rel="alternate" type="text/html" title="Text-guided Editing of Textured 3D Morphable Models" /><published>2023-03-30T11:00:00+02:00</published><updated>2023-03-30T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/clipface</id><content type="html" xml:base="https://justusthies.github.io/posts/clipface/"><![CDATA[<p>We propose ClipFace, a novel self-supervised approach for text-guided editing of textured 3D morphable model of faces. Specifically, we employ user-friendly language prompts to enable control of the expressions as well as appearance of 3D faces. We leverage the geometric expressiveness of 3D morphable models, which inherently possess limited controllability and texture expressivity, and develop a self-supervised generative model to jointly synthesize expressive, textured, and articulated faces in 3D. We enable high-quality texture generation for 3D faces by adversarial self-supervised training, guided by differentiable rendering against collections of real RGB images. Controllable editing and manipulation are given by language prompts to adapt texture and expression of the 3D morphable model. To this end, we propose a neural network that predicts both texture and expression latent codes of the morphable model. Our model is trained in a self-supervised fashion by exploiting differentiable rendering and losses based on a pre-trained CLIP model. Once trained, our model jointly predicts face textures in UV-space, along with expression parameters to capture both geometry and texture changes in facial expressions in a single forward pass. We further show the applicability of our method to generate temporally changing textures for a given animation sequence.
<a href="https://shivangi-aneja.github.io/projects/clipface/">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Shivangi Aneja&quot;, &quot;website_link&quot;=&gt;&quot;https://niessnerlab.org/members/shivangi_aneja/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="SIGGRAPH" /><summary type="html"><![CDATA[ClipFace is a novel self-supervised approach for text-guided editing of textured 3D morphable model of faces. Controllable editing and manipulation are given by language prompts to adapt texture and expression of the 3D morphable model.]]></summary></entry><entry><title type="html">High-Res Facial Appearance Capture from Polarized Smartphone Images</title><link href="https://justusthies.github.io/posts/polface/" rel="alternate" type="text/html" title="High-Res Facial Appearance Capture from Polarized Smartphone Images" /><published>2023-03-30T11:00:00+02:00</published><updated>2023-03-30T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/polface</id><content type="html" xml:base="https://justusthies.github.io/posts/polface/"><![CDATA[<p>We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil. Specifically, we turn the flashlight into a polarized light source and add a polarization filter on top of the camera. Leveraging this setup, we capture the face of a subject with cross-polarized and parallel-polarized light. For each subject, we record two short sequences in a dark environment under flash illumination with different light polarization using the modified smartphone. Based on these observations, we reconstruct an explicit surface mesh of the face using structure from motion. We then exploit the camera and light co-location within a differentiable renderer to optimize the facial textures using an analysis-by-synthesis approach.</p>

<p><a href="https://dazinovic.github.io/polface/">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Dejan Azinovic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/dejan_azinovic/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil.]]></summary></entry><entry><title type="html">Human-Aware 3D Scene Generation</title><link href="https://justusthies.github.io/posts/mime/" rel="alternate" type="text/html" title="Human-Aware 3D Scene Generation" /><published>2023-03-29T11:00:00+02:00</published><updated>2023-03-29T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/mime</id><content type="html" xml:base="https://justusthies.github.io/posts/mime/"><![CDATA[<p>Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a “scanner” of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement.</p>

<p><a href="https://mime.is.tue.mpg.de">Code and data</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Hongwei Yi&quot;, &quot;website_link&quot;=&gt;&quot;https://xyyhw.top/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[Humans constantly interact with their environment. They walk through a room, touch objects, rest on a chair, or sleep in a bed. All these interactions contain information about the scene layout and object placement which we leverage to generate scenes from human motion.]]></summary></entry><entry><title type="html">Instant Volumetric Head Avatars</title><link href="https://justusthies.github.io/posts/insta/" rel="alternate" type="text/html" title="Instant Volumetric Head Avatars" /><published>2023-03-28T10:00:00+02:00</published><updated>2023-03-28T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/insta</id><content type="html" xml:base="https://justusthies.github.io/posts/insta/"><![CDATA[<p>For immersive telepresence in AR or VR, we aim for digital humans (avatars) that mimic the motions and facial expressions of the actual subjects participating in a meeting. Besides the motion, these avatars should reflect the human’s shape and appearance. Instead of prerecorded, old avatars, we aim to instantaneously reconstruct the subject’s look to capture the actual appearance during a meeting. To this end, we propose INSTA, which enables the reconstruction of an avatar within a few minutes (~10 min) and can be driven at interactive frame rates.</p>

<p><a href="https://zielon.github.io/insta/">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Wojciech Zielonka&quot;, &quot;website_link&quot;=&gt;&quot;https://zielon.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[Instead of prerecorded, old avatars, we aim to instantaneously reconstruct the subject's look to capture the actual appearance during a meeting. To this end, we propose INSTA, which enables the reconstruction of an avatar within a few minutes (~10 min) and can be driven at interactive frame rates.]]></summary></entry><entry><title type="html">Depth-aware Image-based NEural Radiance Fields</title><link href="https://justusthies.github.io/posts/diner/" rel="alternate" type="text/html" title="Depth-aware Image-based NEural Radiance Fields" /><published>2023-03-28T09:00:00+02:00</published><updated>2023-03-28T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/diner</id><content type="html" xml:base="https://justusthies.github.io/posts/diner/"><![CDATA[<p>We present Depth-aware Image-based NEural Radiance fields (DINER). Given a sparse set of RGB input views, we predict depth and feature maps to guide the reconstruction of a volumetric scene representation that allows us to render 3D objects under novel views. Specifically, we propose novel techniques to incorporate depth information into feature fusion and efficient scene sampling. In comparison to the previous state of the art, DINER achieves higher synthesis quality and can process input views with greater disparity. This allows us to capture scenes more completely without changing capturing hardware requirements and ultimately enables larger viewpoint changes during novel view synthesis. We evaluate our method by synthesizing novel views, both for human heads and for general objects, and observe significantly improved qualitative results and increased perceptual metrics compared to the previous state of the art. The code will be made publicly available for research purposes.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Malte Prinzler&quot;, &quot;website_link&quot;=&gt;&quot;https://ncs.is.mpg.de/person/mprinzler&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[Given a sparse set of RGB input views, we predict depth and feature maps to guide the reconstruction of a volumetric scene representation that allows us to render 3D objects under novel views.]]></summary></entry><entry><title type="html">Neural Deformation Priors</title><link href="https://justusthies.github.io/posts/defprior/" rel="alternate" type="text/html" title="Neural Deformation Priors" /><published>2022-12-01T08:00:00+01:00</published><updated>2022-12-01T08:00:00+01:00</updated><id>https://justusthies.github.io/posts/defprior</id><content type="html" xml:base="https://justusthies.github.io/posts/defprior/"><![CDATA[<p>We present Neural Shape Deformation Priors, a novel method for shape manipulation that predicts mesh deformations of non-rigid objects from user-provided handle movements. State-of-the-art methods cast this problem as an optimization task, where the input source mesh is iteratively deformed to minimize an objective function according to hand-crafted regularizers such as ARAP. In this work, we learn the deformation behavior based on the underlying geometric properties of a shape, while leveraging a large-scale dataset containing a diverse set of non-rigid deformations. Specifically, given a source mesh and desired target locations of handles that describe the partial surface deformation, we predict a continuous deformation field that is defined in 3D space to describe the space deformation. To this end, we introduce transformer-based deformation networks that represent a shape deformation as a composition of local surface deformations. It learns a set of local latent codes anchored in 3D space, from which we can learn a set of continuous deformation functions for local surfaces. Our method can be applied to challenging deformations and generalizes well to unseen deformations. We validate our approach in experiments using the DeformingThing4D dataset, and compare to both classic optimization-based and recent neural network-based methods.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Jiapeng Tang&quot;, &quot;website_link&quot;=&gt;&quot;https://tangjiapeng.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="NeurIPS" /><summary type="html"><![CDATA[We present Neural Shape Deformation Priors, a novel method for shape manipulation that predicts mesh deformations of non-rigid objects from user-provided handle movements.]]></summary></entry><entry><title type="html">Towards Metrical Reconstruction of Human Faces</title><link href="https://justusthies.github.io/posts/mica/" rel="alternate" type="text/html" title="Towards Metrical Reconstruction of Human Faces" /><published>2022-07-04T09:33:00+02:00</published><updated>2022-07-04T09:33:00+02:00</updated><id>https://justusthies.github.io/posts/mica</id><content type="html" xml:base="https://justusthies.github.io/posts/mica/"><![CDATA[<p>Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric (15% and 24% lower average error on NoW, respectively).</p>

<p><a href="https://zielon.github.io/mica/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Wojciech Zielonka&quot;, &quot;website_link&quot;=&gt;&quot;https://zielon.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context. Thus, we present MICA, a novel metrical face reconstruction method that combines face recognition with supervised face shape learning.]]></summary></entry><entry><title type="html">Generating Textures on 3D Shape Surfaces</title><link href="https://justusthies.github.io/posts/texturify/" rel="alternate" type="text/html" title="Generating Textures on 3D Shape Surfaces" /><published>2022-07-04T09:00:00+02:00</published><updated>2022-07-04T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/texturify</id><content type="html" xml:base="https://justusthies.github.io/posts/texturify/"><![CDATA[<p>Texture cues on 3D objects are key to compelling visual representations, with the possibility to create high visual fidelity with inherent spatial consistency across different views. Since the availability of textured 3D shapes remains very limited, learning a 3D-supervised data-driven method that predicts a texture based on the 3D input is very challenging. We thus propose Texurify, a GAN-based method that leverages a 3D shape dataset of an object class and learns to reproduce the distribution of appearances observed in real images by generating high-quality textures. In particular, our method does not require any 3D color supervision or correspondence between shape geometry and images to learn the texturing of 3D objects. Texurify operates directly on the surface of the 3D objects by introducing face convolutional operators on a hierarchical 4-RoSy parameterization to generate plausible object-specific textures. Employing differentiable rendering and adversarial losses that critique individual views and consistency across views, we effectively learn the high-quality surface texturing distribution from real-world images. Experiments on car and chair shape collections show that our approach outperforms state of the art by an average of 22% in FID score.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Yawar Siddiqui&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/yawar_siddiqui/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[Texturify learns to generate geometry-aware textures for untextured collections of 3D objects. Our method trains from only a collection of images and a collection of untextured shapes, which are both often available, without requiring any explicit 3D color supervision or shape-image correspondence. Textures are created directly on the surface of a given 3D shape, enabling generation of high-quality, compelling textured 3D shapes.]]></summary></entry><entry><title type="html">CLIPme if you can!</title><link href="https://justusthies.github.io/posts/clipme/" rel="alternate" type="text/html" title="CLIPme if you can!" /><published>2022-05-12T09:00:00+02:00</published><updated>2022-05-12T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/clipme</id><content type="html" xml:base="https://justusthies.github.io/posts/clipme/"><![CDATA[<p><a href="https://openai.com/blog/clip/">CLIP</a> is a powerful tool to match images to text descriptions.
It can be used for classification of ImageNet classes as shown in the original paper, but it can also be used to assign names of (famous) people to images.
Specifically, we assume that we have an image and a list of names of people in the image as input (also called queries below).</p>

<p>In a first step, we detect faces in the input image which can give us the regions of interest:
<img src="solvay_conference_1927_detections.jpg" alt="Face Detections" style="width:600px;height:450px;" /></p>

<p>Using these detections, we can run the CLIP model on the cropped regions of interest and compute the matching score w.r.t. the names of people we provide as input.
We annotate the original image with the results that match best with the names:</p>

<p><img src="solvay_conference_1927_annotations.jpg" alt="Face Annotations" style="width:600px;height:450px;" /></p>

<p>As can be seen, it works surprisingly well. Except for Hendrik Lorentz (which actually sits between Marie Curie and Albert Einstein), it matches all queries.</p>

<p>It also works well for other famous people, like state leaders:
<img src="G7_annotations.jpg" alt="Face Annotations" style="width:600px;height:400px;" /></p>

<p>Interestingly, CLIP also knows the relationship of the countries they represent:
<img src="G7_annotations_countries.jpg" alt="Face Annotations" style="width:600px;height:400px;" /></p>

<p>Obviously, I had to try it with an image of myself and it turns out that it can also annotate my face:
<img src="berlin_annotations.jpg" alt="Face Annotations" style="width:600px;height:450px;" /></p>

<p><img src="fmx_annotations.jpg" alt="Face Annotations" style="width:600px;height:450px;" /></p>

<h1 id="conclusion">Conclusion:</h1>
<p>CLIP not only stores general concepts of text and images, to some extent it is also storing information about individual people.
We can also see this in the results of <a href="https://openai.com/dall-e-2/">DALL-E-2</a>, where we can ask to render a cat in an outfit like Napoleon.</p>

<h1 id="code">Code:</h1>
<p><a href="https://github.com/JustusThies/CLIPme">GITHUB</a></p>

<h1 id="image-credits">Image Credits:</h1>
<ul>
  <li><a href="https://i.redd.it/okqe57386di51.jpg">Solvay conference 1927</a> - image colorized by Sanna Dullaway</li>
  <li><a href="https://www.tagesschau.de/multimedia/bilder/g7-121~_v-gross20x9.jpg">G7</a></li>
  <li><a href="https://justusthies.github.io/posts/face_berlin/thumb.jpg">Cabinet Meeting</a></li>
  <li><a href="https://pbs.twimg.com/media/D5fal0cWsAQxpCT?format=jpg">FMX2019</a></li>
</ul>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="tutorial" /><category term="PLAYGROUND" /><summary type="html"><![CDATA[CLIP is a powerful tool to match images to text descriptions. It can be used for classification of ImageNet classes as shown in the original paper, but it can also be used to assign names of (famous) people to images.]]></summary></entry><entry><title type="html">Neural Head Avatars from Monocular RGB Videos</title><link href="https://justusthies.github.io/posts/neuralhead/" rel="alternate" type="text/html" title="Neural Head Avatars from Monocular RGB Videos" /><published>2022-03-22T08:30:00+01:00</published><updated>2022-03-22T08:30:00+01:00</updated><id>https://justusthies.github.io/posts/neuralhead</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralhead/"><![CDATA[<p>We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be used for teleconferencing in AR/VR or other applications in the movie or games industry that rely on a digital human. Our representation can be learned from a monocular RGB portrait video that features a range of different expressions and views. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture. We demonstrate that this representation is able to accurately extrapolate to unseen poses and view points, and generates natural expressions while providing sharp texture details. Compared to previous works on head avatars, our method provides a disentangled shape and appearance model of the complete human head (including hair) that is compatible with the standard graphics pipeline. Moreover, it quantitatively and qualitatively outperforms current state of the art in terms of reconstruction quality and novel-view synthesis.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Philip-William Grassal&quot;, &quot;website_link&quot;=&gt;&quot;https://hci.iwr.uni-heidelberg.de/vislearn/people&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar using a deep neural network. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture.]]></summary></entry></feed>