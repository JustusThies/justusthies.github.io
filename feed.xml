<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2020-06-30T10:42:55+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Neural Non-Rigid Tracking</title><link href="https://justusthies.github.io/posts/neuraltracking/" rel="alternate" type="text/html" title="Neural Non-Rigid Tracking" /><published>2020-06-24T11:00:00+02:00</published><updated>2020-06-24T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuraltracking</id><content type="html" xml:base="https://justusthies.github.io/posts/neuraltracking/">&lt;p&gt;We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction.
Given two input RGB-D frames of a non-rigidly moving object, we employ a convolutional neural network to predict dense correspondences.
These correspondences are used as constraints in an as-rigid-as-possible (ARAP) optimization problem.
By enabling gradient back-propagation through the non-rigid optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking.
Furthermore, this formulation allows for learning correspondence weights in a self-supervised manner.
Thus, outliers and wrong correspondences are down-weighted to enable robust tracking.
Compared to state-of-the-art approaches, our algorithm shows improved reconstruction performance, while simultaneously achieving 85x faster correspondence prediction than comparable deep-learning based methods.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="ArXiv" /><summary type="html">We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction. Given two input RGB-D frames of a non-rigidly moving object, we employ a convolutional neural network to predict dense correspondences. These correspondences are used as constraints in an as-rigid-as-possible (ARAP) optimization problem. By enabling gradient back-propagation through the non-rigid optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking. Furthermore, this formulation allows for learning correspondence weights in a self-supervised manner. Thus, outliers and wrong correspondences are down-weighted to enable robust tracking. Compared to state-of-the-art approaches, our algorithm shows improved reconstruction performance, while simultaneously achieving 85x faster correspondence prediction than comparable deep-learning based methods.</summary></entry><entry><title type="html">Self-Supervised Photometric Scene Generation from RGB-D Scans</title><link href="https://justusthies.github.io/posts/spsg/" rel="alternate" type="text/html" title="Self-Supervised Photometric Scene Generation from RGB-D Scans" /><published>2020-06-24T11:00:00+02:00</published><updated>2020-06-24T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/spsg</id><content type="html" xml:base="https://justusthies.github.io/posts/spsg/">&lt;p&gt;We present Self-Supervised Photometric Scene Generation (SPSG), a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion.
Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan.
Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes.
This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies.
Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Angela Dai&quot;, &quot;website_link&quot;=&gt;&quot;https://angeladai.github.io&quot;}</name></author><category term="ArXiv" /><summary type="html">We present Self-Supervised Photometric Scene Generation (SPSG), a novel approach to generate high-quality, colored 3D models of scenes from RGB-D scan observations by learning to infer unobserved scene geometry and color in a self-supervised fashion. Our self-supervised approach learns to jointly inpaint geometry and color by correlating an incomplete RGB-D scan with a more complete version of that scan. Notably, rather than relying on 3D reconstruction losses to inform our 3D geometry and color reconstruction, we propose adversarial and perceptual losses operating on 2D renderings in order to achieve high-resolution, high-quality colored reconstructions of scenes. This exploits the high-resolution, self-consistent signal from individual raw RGB-D frames, in contrast to fused 3D reconstructions of the frames which exhibit inconsistencies from view-dependent effects, such as color balancing or pose inconsistencies. Thus, by informing our 3D scene generation directly through 2D signal, we produce high-quality colored reconstructions of 3D scenes, outperforming state of the art on both synthetic and real data.</summary></entry><entry><title type="html">Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition</title><link href="https://justusthies.github.io/posts/intrinsicautoencoder/" rel="alternate" type="text/html" title="Intrinsic Autoencoders for Joint Neural Rendering and Intrinsic Image Decomposition" /><published>2020-06-23T12:00:00+02:00</published><updated>2020-06-23T12:00:00+02:00</updated><id>https://justusthies.github.io/posts/intrinsicautoencoder</id><content type="html" xml:base="https://justusthies.github.io/posts/intrinsicautoencoder/">&lt;p&gt;Neural rendering techniques promise efficient photo-realistic image synthesis while at the same time providing rich control over scene parameters by learning the physical image formation process.
While several supervised methods have been proposed for this task, acquiring a dataset of images with accurately aligned 3D models is very difficult.
The main contribution of this work is to lift this restriction by training a neural rendering algorithm from unpaired data.
More specifically, we propose an autoencoder for joint generation of realistic images from synthetic 3D models while simultaneously decomposing real images into their intrinsic shape and appearance properties.
In contrast to a traditional graphics pipeline, our approach does not require to specify all scene properties, such as material parameters and lighting by hand.
Instead, we learn photo-realistic deferred rendering from a small set of 3D models and a larger set of unaligned real images, both of which are easy to acquire in practice.
Simultaneously, we obtain accurate intrinsic decompositions of real images while not requiring paired ground truth.
Our experiments confirm that a joint treatment of rendering and decomposition is indeed beneficial and that our approach outperforms state-of-the-art image-toimage translation baselines both qualitatively and quantitatively.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Hassan Abu Alhaija&quot;, &quot;website_link&quot;=&gt;&quot;https://hassanhaija.github.io/&quot;}</name></author><category term="ArXiv" /><summary type="html">Neural rendering techniques promise efficient photo-realistic image synthesis while at the same time providing rich control over scene parameters by learning the physical image formation process. While several supervised methods have been proposed for this task, acquiring a dataset of images with accurately aligned 3D models is very difficult. The main contribution of this work is to lift this restriction by training a neural rendering algorithm from unpaired data. More specifically, we propose an autoencoder for joint generation of realistic images from synthetic 3D models while simultaneously decomposing real images into their intrinsic shape and appearance properties. In contrast to a traditional graphics pipeline, our approach does not require to specify all scene properties, such as material parameters and lighting by hand. Instead, we learn photo-realistic deferred rendering from a small set of 3D models and a larger set of unaligned real images, both of which are easy to acquire in practice. Simultaneously, we obtain accurate intrinsic decompositions of real images while not requiring paired ground truth. Our experiments confirm that a joint treatment of rendering and decomposition is indeed beneficial and that our approach outperforms state-of-the-art image-toimage translation baselines both qualitatively and quantitatively.</summary></entry><entry><title type="html">CVPR 2020 - Tutorial on Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrenderingtutorial_cvpr/" rel="alternate" type="text/html" title="CVPR 2020 - Tutorial on Neural Rendering" /><published>2020-04-08T11:00:00+02:00</published><updated>2020-04-08T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuralrenderingtutorial_cvpr</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrenderingtutorial_cvpr/">&lt;p&gt;Neural rendering is a new class of deep image and video generation approaches that enable explicit or implicit control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure.
It combines generative machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic outputs.
This tutorial teaches the fundamentals of neural rendering and summarizes recent trends and applications.
Starting with an overview of the underlying graphics, vision and machine learning concepts, we discuss critical aspects of neural rendering approaches.
Specifically, our emphasis is on what aspects of the generated imagery can be controlled, which parts of the pipeline are learned, explicit vs.~implicit control, generalization, and stochastic vs.~deterministic synthesis.
The second half of this tutorial is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence.
Finally, we conclude with a discussion of the social implications of this technology and investigate open research problems.&lt;/p&gt;

&lt;p&gt;##&lt;a href=&quot;https://www.neuralrender.com/&quot;&gt;Tutorial Website&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="CVPR" /><summary type="html">Neural rendering is a new class of deep image and video generation approaches that enable explicit or implicit control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure. It combines generative machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic outputs. This tutorial teaches the fundamentals of neural rendering and summarizes recent trends and applications. Starting with an overview of the underlying graphics, vision and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on what aspects of the generated imagery can be controlled, which parts of the pipeline are learned, explicit vs.~implicit control, generalization, and stochastic vs.~deterministic synthesis. The second half of this tutorial is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of this technology and investigate open research problems.</summary></entry><entry><title type="html">State of the Art on Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrenderingstar/" rel="alternate" type="text/html" title="State of the Art on Neural Rendering" /><published>2020-04-08T10:00:00+02:00</published><updated>2020-04-08T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuralrenderingstar</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrenderingstar/">&lt;p&gt;Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations.
However, the automatic generation of shape, materials, lighting, and other aspects of scenes  remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible.
Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models.
Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training.
With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists.
This state-of-the-art report summarizes the recent trends and applications of neural rendering.
We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs.
Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches.
Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs.~implicit control, generalization, and stochastic vs.~deterministic synthesis.
The second half of this state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence.
Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="Eurographics" /><summary type="html">Efficient rendering of photo-realistic virtual worlds is a long standing effort of computer graphics. Modern graphics techniques have succeeded in synthesizing photo-realistic images from hand-crafted scene representations. However, the automatic generation of shape, materials, lighting, and other aspects of scenes remains a challenging problem that, if solved, would make photo-realistic computer graphics more widely accessible. Concurrently, progress in computer vision and machine learning have given rise to a new approach to image synthesis and editing, namely deep generative models. Neural rendering is a new and rapidly emerging field that combines generative machine learning techniques with physical knowledge from computer graphics, e.g., by the integration of differentiable rendering into network training. With a plethora of applications in computer graphics and vision, neural rendering is poised to become a new area in the graphics community, yet no survey of this emerging field exists. This state-of-the-art report summarizes the recent trends and applications of neural rendering. We focus on approaches that combine classic computer graphics techniques with deep generative models to obtain controllable and photo-realistic outputs. Starting with an overview of the underlying computer graphics and machine learning concepts, we discuss critical aspects of neural rendering approaches. Specifically, our emphasis is on the type of control, i.e., how the control is provided, which parts of the pipeline are learned, explicit vs.~implicit control, generalization, and stochastic vs.~deterministic synthesis. The second half of this state-of-the-art report is focused on the many important use cases for the described algorithms such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we conclude with a discussion of the social implications of such technology and investigate open research problems.</summary></entry><entry><title type="html">CVPR 2020 - Workshop on Media Forensics</title><link href="https://justusthies.github.io/posts/media_forensics_workshop/" rel="alternate" type="text/html" title="CVPR 2020 - Workshop on Media Forensics" /><published>2020-04-08T08:00:00+02:00</published><updated>2020-04-08T08:00:00+02:00</updated><id>https://justusthies.github.io/posts/media_forensics_workshop</id><content type="html" xml:base="https://justusthies.github.io/posts/media_forensics_workshop/">&lt;p&gt;&lt;a href=&quot;https://sites.google.com/view/wmediaforensics2020/&quot;&gt;Workshop Website&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The recent advent of techniques that generate photo-realistic fully synthetic images and videos, and the increasing prevalence of misinformation associated with such fabricated media have raised the level of interest of media forensics within the computer vision community.
Both academia and industry have addressed this topic in the past, but only recently, with the emergence of more sophisticated ML and CV techniques, has multimedia forensics has become a broad and prominent area of research.
In the recent years, there has been a number of events (Media Forensics@CVPR2019 or WOCM@ECCV2018 among others) that touched on this topic but now, more than ever, it critical to keep advancing on all fronts of media forensics: from detection of manipulations, biometric implications, misrepresentation/spoofing, etc.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Cristian Canton&quot;, &quot;website_link&quot;=&gt;&quot;https://www.linkedin.com/in/cristiancanton/&quot;}</name></author><category term="CVPR" /><summary type="html">Workshop Website</summary></entry><entry><title type="html">Seminar: 3D Vision</title><link href="https://justusthies.github.io/posts/3D-Vision-SS20/" rel="alternate" type="text/html" title="Seminar&amp;#58; 3D Vision" /><published>2020-04-07T23:00:00+02:00</published><updated>2020-04-07T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Vision-SS20</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Vision-SS20/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).&lt;/p&gt;

&lt;p&gt;The talks have to cover the following topics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;face tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;facial reenactment (audio driven, video driven animation)&lt;/li&gt;
  &lt;li&gt;body tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;hand tracking&lt;/li&gt;
  &lt;li&gt;static / dynamic scene reconstruction&lt;/li&gt;
  &lt;li&gt;scene understanding&lt;/li&gt;
  &lt;li&gt;inverse rendering, material estimation, light estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;organization--requirements&quot;&gt;Organization / Requirements&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Lecture: 3D Scanning &amp;amp; Motion Capturing&lt;/li&gt;
  &lt;li&gt;The participants have to present their topics in a talk (in English), which should last 30 minutes.&lt;/li&gt;
  &lt;li&gt;The semi-final slides (PDF) should be sent one week before the talk; otherwise, the talk will be canceled.&lt;/li&gt;
  &lt;li&gt;A short report (approximately 2-3 pages in the ACM SIGGRAPH TOG format (acmtog)) should be prepared and sent within two weeks after the talk. When you send the report, please send the final slides (PDF) together.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).</summary></entry><entry><title type="html">Practical Course: 3D Scanning and Spatial Learning</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-SS20/" rel="alternate" type="text/html" title="Practical Course&amp;#58; 3D Scanning and Spatial Learning" /><published>2020-04-07T23:00:00+02:00</published><updated>2020-04-07T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-SS20</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-SS20/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp;amp; Motion Capture as well as by the Introduction to Deep Learning lecture.
In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field.
Specifically, we will have projects on:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;human motion capturing (e.g., Fusion4D, BodyFusion)&lt;/li&gt;
  &lt;li&gt;real-time facial motion capturing (spare and dense approaches)&lt;/li&gt;
  &lt;li&gt;3D scene reconstruction (e.g., BundleFusion)&lt;/li&gt;
  &lt;li&gt;scan refinement (e.g., ShapeFromShading)&lt;/li&gt;
  &lt;li&gt;neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering)&lt;/li&gt;
  &lt;li&gt;scene completion (e.g., ScanComplete)&lt;/li&gt;
  &lt;li&gt;3D object retrieval and alignment (e.g., Scan2CAD)&lt;/li&gt;
  &lt;li&gt;scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;objective&quot;&gt;Objective&lt;/h3&gt;
&lt;p&gt;Upon completion of this module, students will have acquired extensive theoretical concepts behind state-of-the art 3D reconstruction methods, in particular in the context of human motion capturing, static object scanning, scene understanding and synthesis of captured scenes. Besides the theoretical foundations, a significant aspect lies on the practical realization and implementation of such algorithms.&lt;/p&gt;

&lt;h3 id=&quot;organization&quot;&gt;Organization&lt;/h3&gt;
&lt;p&gt;In the practical course students shall get familiar with state-of-the-art 3D scanning. They will be assisted by current PhD students working in this field (regular office hours). To ensure a good progress during the semester, we will have mandatory meetings (every two weeks) where the students report their current state. In the end of the course, the students are asked to give a talk about their project and results.&lt;/p&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, 3D scanning &amp;amp; Motion Capture, Introduction to Deep Learning, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp;amp; Motion Capture as well as by the Introduction to Deep Learning lecture. In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field. Specifically, we will have projects on: human motion capturing (e.g., Fusion4D, BodyFusion) real-time facial motion capturing (spare and dense approaches) 3D scene reconstruction (e.g., BundleFusion) scan refinement (e.g., ShapeFromShading) neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering) scene completion (e.g., ScanComplete) 3D object retrieval and alignment (e.g., Scan2CAD) scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS)</summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS20/" rel="alternate" type="text/html" title="Lecture&amp;#58; 3D Scanning and Motion Capture" /><published>2020-04-07T23:00:00+02:00</published><updated>2020-04-07T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS20</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS20/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic concepts of geometry
    &lt;ul&gt;
      &lt;li&gt;Meshes (polygonal), Point Clouds, Pixels &amp;amp; Voxels&lt;/li&gt;
      &lt;li&gt;RGB and Depth Cameras&lt;/li&gt;
      &lt;li&gt;Extrinsics and Intrinsics&lt;/li&gt;
      &lt;li&gt;Capture devices&lt;/li&gt;
      &lt;li&gt;RGB and Multi-view&lt;/li&gt;
      &lt;li&gt;RGB-D cameras&lt;/li&gt;
      &lt;li&gt;Stereo&lt;/li&gt;
      &lt;li&gt;Time of Flight (ToF)&lt;/li&gt;
      &lt;li&gt;Structured Light&lt;/li&gt;
      &lt;li&gt;Laser Scanner, Lidar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Surface Representations
    &lt;ul&gt;
      &lt;li&gt;Polygonal meshes (trimeshes, etc.)&lt;/li&gt;
      &lt;li&gt;Parametric surfaces: splines, nurbs&lt;/li&gt;
      &lt;li&gt;Implicit surfaces&lt;/li&gt;
      &lt;li&gt;Ridge-based surfaces&lt;/li&gt;
      &lt;li&gt;Radial basis functions&lt;/li&gt;
      &lt;li&gt;Signed distance functions (volumetric, Curless &amp;amp; Levoy)&lt;/li&gt;
      &lt;li&gt;Indicator function (Poisson Surface Reconstruction)&lt;/li&gt;
      &lt;li&gt;More general: level sets&lt;/li&gt;
      &lt;li&gt;Marching cubes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;High-level overview of reconstructions
    &lt;ul&gt;
      &lt;li&gt;Structure from Motion (SfM)&lt;/li&gt;
      &lt;li&gt;Multi-view Stereo (MVS)&lt;/li&gt;
      &lt;li&gt;SLAM&lt;/li&gt;
      &lt;li&gt;Bundle Adjustment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimization
    &lt;ul&gt;
      &lt;li&gt;Non-linear least squares&lt;/li&gt;
      &lt;li&gt;Gauss-Newton LM&lt;/li&gt;
      &lt;li&gt;Examples in Ceres&lt;/li&gt;
      &lt;li&gt;Symbolic diff vs auto-diff&lt;/li&gt;
      &lt;li&gt;Auto-diff with dual numbers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Pose alignment&lt;/li&gt;
      &lt;li&gt;ICP (point cloud alignment; depth-to-model alignment; rigid ‘fitting’)&lt;/li&gt;
      &lt;li&gt;Online surface reconstruction pipeline: KinectFusion&lt;/li&gt;
      &lt;li&gt;Scalable surface representations: VoxelHashing, OctTrees&lt;/li&gt;
      &lt;li&gt;Loop closures and global optimization&lt;/li&gt;
      &lt;li&gt;Robust optimization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Non-rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Surface deformation for modeling&lt;/li&gt;
      &lt;li&gt;Regularizers: ARAP, ED, etc.&lt;/li&gt;
      &lt;li&gt;Non-rigid surface fitting: e.g., non-rigid ICP&lt;/li&gt;
      &lt;li&gt;Non-rigid reconstruction: DynamicFusion/VolumeDeform/KillingFusion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Body Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Skeleton Tracking and Inverse Kinematics&lt;/li&gt;
      &lt;li&gt;Learning-based approaches from RGB and RGB-D&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Face Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Keypoint detection &amp;amp; tracking&lt;/li&gt;
      &lt;li&gt;Parametric / Statistical Models -&amp;gt; BlendShapes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hand Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Parametric Models&lt;/li&gt;
      &lt;li&gt;Some DeepLearning-based things&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminative vs generative tracking
    &lt;ul&gt;
      &lt;li&gt;Random forests&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Motion Capture in Movies
    &lt;ul&gt;
      &lt;li&gt;Marker-based motion capture&lt;/li&gt;
      &lt;li&gt;LightStage -&amp;gt; movies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BRDF and Material Capture&lt;/li&gt;
  &lt;li&gt;Open research questions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).</summary></entry><entry><title type="html">Adversarial Texture Optimization from RGB-D Scans</title><link href="https://justusthies.github.io/posts/advtex/" rel="alternate" type="text/html" title="Adversarial Texture Optimization from RGB-D Scans" /><published>2020-03-19T10:00:00+01:00</published><updated>2020-03-19T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/advtex</id><content type="html" xml:base="https://justusthies.github.io/posts/advtex/">&lt;p&gt;Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera poses, and view-dependent imaging artifacts.
In this work, we present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views.
Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors.
The key idea of our approach is to learn a patch-based conditional discriminator which guides the texture optimization to be tolerant to misalignments.
Our discriminator takes a synthesized view and a real image, and evaluates whether the synthesized one is realistic, under a broadened definition of realism.
We train the discriminator by providing as ‘real’ examples pairs of input views and their misaligned versions – so that the learned adversarial loss will tolerate errors from the scans.
Experiments on synthetic and real data under quantitative or qualitative evaluation demonstrate the advantage of our approach in comparison to state of the art.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Jingwei Huang&quot;, &quot;website_link&quot;=&gt;&quot;http://stanford.edu/~jingweih/&quot;}</name></author><category term="CVPR" /><summary type="html">Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera poses, and view-dependent imaging artifacts. In this work, we present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views. Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors. The key idea of our approach is to learn a patch-based conditional discriminator which guides the texture optimization to be tolerant to misalignments. Our discriminator takes a synthesized view and a real image, and evaluates whether the synthesized one is realistic, under a broadened definition of realism. We train the discriminator by providing as ‘real’ examples pairs of input views and their misaligned versions – so that the learned adversarial loss will tolerate errors from the scans. Experiments on synthetic and real data under quantitative or qualitative evaluation demonstrate the advantage of our approach in comparison to state of the art.</summary></entry></feed>