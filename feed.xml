<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2021-08-27T18:11:41+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Expert Interview on DeepFakes</title><link href="https://justusthies.github.io/posts/eu_report/" rel="alternate" type="text/html" title="Expert Interview on DeepFakes" /><published>2021-08-10T09:00:00+02:00</published><updated>2021-08-10T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/eu_report</id><content type="html" xml:base="https://justusthies.github.io/posts/eu_report/">&lt;p&gt;Expert interview for the European Parliamentary Research Service report on ‘Tackling DeepFakes in European Policy’.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2021)690039&quot;&gt;Weblink to the official website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="EU-PARLIAMENT" /><summary type="html">Expert interview for the European Parliamentary Research Service report on ‘Tackling DeepFakes in European Policy’.</summary></entry><entry><title type="html">SIGGRAPH 2021 - Course on the Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrendering_course_siggraph/" rel="alternate" type="text/html" title="SIGGRAPH 2021 - Course on the Advances in Neural Rendering" /><published>2021-08-08T11:00:00+02:00</published><updated>2021-08-08T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuralrendering_course_siggraph</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrendering_course_siggraph/">&lt;p&gt;Neural rendering is an emerging class of deep image and video generation approaches that enable control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure.
It combines machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic models of scenes.
This course covers the advances in neural rendering over the last year.
We will first cover the fundamentals of machine learning and computer graphics relevant for neural rendering. 
Next, we will present state of the art techniques for the many important neural rendering methods for applications such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence.
Finally, we will conclude with a discussion on the ethical implications of this technology and open research problems.&lt;/p&gt;

&lt;p&gt;##&lt;a href=&quot;https://www.neuralrender.com/&quot;&gt;Tutorial Website&lt;/a&gt;
##&lt;a href=&quot;https://www.youtube.com/watch?v=otly9jcZ0Jg&amp;amp;ab_channel=NeuralRendering&quot;&gt;Video Part 1&lt;/a&gt;
##&lt;a href=&quot;https://www.youtube.com/watch?v=aboFl5ozImM&amp;amp;ab_channel=NeuralRendering&quot;&gt;Video Part 2&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Ayush Tewari&quot;, &quot;website_link&quot;=&gt;&quot;https://people.mpi-inf.mpg.de/~atewari/&quot;}</name></author><category term="SIGGRAPH" /><summary type="html">Neural rendering is an emerging class of deep image and video generation approaches that enable control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure. It combines machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic models of scenes. This course covers the advances in neural rendering over the last year. We will first cover the fundamentals of machine learning and computer graphics relevant for neural rendering. Next, we will present state of the art techniques for the many important neural rendering methods for applications such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we will conclude with a discussion on the ethical implications of this technology and open research problems.</summary></entry><entry><title type="html">TransformerFusion: Monocular RGB Scene Reconstruction using Transformers</title><link href="https://justusthies.github.io/posts/transfusion/" rel="alternate" type="text/html" title="TransformerFusion&amp;#58; Monocular RGB Scene Reconstruction using Transformers" /><published>2021-07-12T09:30:00+02:00</published><updated>2021-07-12T09:30:00+02:00</updated><id>https://justusthies.github.io/posts/transfusion</id><content type="html" xml:base="https://justusthies.github.io/posts/transfusion/">&lt;p&gt;We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="ARXIV" /><summary type="html">We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.</summary></entry><entry><title type="html">EG Junior Fellow</title><link href="https://justusthies.github.io/posts/eg_junior_fellow/" rel="alternate" type="text/html" title="EG Junior Fellow" /><published>2021-05-14T11:00:00+02:00</published><updated>2021-05-14T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/eg_junior_fellow</id><content type="html" xml:base="https://justusthies.github.io/posts/eg_junior_fellow/">&lt;p&gt;The Eurographics Junior Fellows have elected me to join the EG Junior Fellows.&lt;/p&gt;

&lt;p&gt;‘Eurographics Junior Fellows are young researchers (PhD degree obtained no more than eight years ago) from all over the world who have already made significant contributions to the field of computer graphics, or to the Eurographics Association.’&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.eg.org/wp/junior-fellows/&quot;&gt;Eurographics Website&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="EG" /><summary type="html">The Eurographics Junior Fellows have elected me to join the EG Junior Fellows.</summary></entry><entry><title type="html">Dynamic Surface Function Networks for Clothed Human Bodies</title><link href="https://justusthies.github.io/posts/dsfn/" rel="alternate" type="text/html" title="Dynamic Surface Function Networks for Clothed Human Bodies" /><published>2021-04-12T10:00:00+02:00</published><updated>2021-04-12T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/dsfn</id><content type="html" xml:base="https://justusthies.github.io/posts/dsfn/">&lt;p&gt;We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Andrei Burov&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/andrei_burov/profile.html&quot;}</name></author><category term="ICCV" /><summary type="html">We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.</summary></entry><entry><title type="html">Neural Parametric Models for 3D Deformable Shapes</title><link href="https://justusthies.github.io/posts/npm/" rel="alternate" type="text/html" title="Neural Parametric Models for 3D Deformable Shapes" /><published>2021-04-12T09:30:00+02:00</published><updated>2021-04-12T09:30:00+02:00</updated><id>https://justusthies.github.io/posts/npm</id><content type="html" xml:base="https://justusthies.github.io/posts/npm/">&lt;p&gt;Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Pablo Palafox&quot;, &quot;website_link&quot;=&gt;&quot;https://pablorpalafox.github.io&quot;}</name></author><category term="ICCV" /><summary type="html">Parametric 3D models have enabled a wide variety of tasks in computer graphics and vision, such as modeling human bodies, faces, and hands. However, the construction of these parametric models is often tedious, as it requires heavy manual tweaking, and they struggle to represent additional complexity and details such as wrinkles or clothing. To this end, we propose Neural Parametric Models (NPMs), a novel, learned alternative to traditional, parametric 3D models, which does not require hand-crafted, object-specific constraints. In particular, we learn to disentangle 4D dynamics into latent-space representations of shape and pose, leveraging the flexibility of recent developments in learned implicit functions. Crucially, once learned, our neural parametric models of shape and pose enable optimization over the learned spaces to fit to new observations, similar to the fitting of a traditional parametric model, e.g., SMPL. This enables NPMs to achieve a significantly more accurate and detailed representation of observed deformable sequences. We show that NPMs improve notably over both parametric and non-parametric state of the art in reconstruction and tracking of monocular depth sequences of clothed humans and hands. Latent-space interpolation as well as shape/pose transfer experiments further demonstrate the usefulness of NPMs.</summary></entry><entry><title type="html">Neural 3D Scene Reconstruction with a Database</title><link href="https://justusthies.github.io/posts/retrievalfuse/" rel="alternate" type="text/html" title="Neural 3D Scene Reconstruction with a Database" /><published>2021-04-12T09:00:00+02:00</published><updated>2021-04-12T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/retrievalfuse</id><content type="html" xml:base="https://justusthies.github.io/posts/retrievalfuse/">&lt;p&gt;3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In contrast to traditional generative learned models which encode the full generative process into a neural network and can struggle with maintaining local details at the scene level, we introduce a new method that directly leverages scene geometry from the training database. First, we learn to synthesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can effectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neural scene reconstruction with a database for the tasks of 3D super-resolution and surface reconstruction from sparse point clouds, showing that our approach enables generation of more coherent, accurate 3D scenes, improving on average by over 8% in IoU over state-of-the-art scene reconstruction.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Yawar Siddiqui&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/yawar_siddiqui/profile.html&quot;}</name></author><category term="ICCV" /><summary type="html">3D reconstruction of large scenes is a challenging problem due to the high-complexity nature of the solution space, in particular for generative neural networks. In contrast to traditional generative learned models which encode the full generative process into a neural network and can struggle with maintaining local details at the scene level, we introduce a new method that directly leverages scene geometry from the training database. First, we learn to synthesize an initial estimate for a 3D scene, constructed by retrieving a top-k set of volumetric chunks from the scene database. These candidates are then refined to a final scene generation with an attention-based refinement that can effectively select the most consistent set of geometry from the candidates and combine them together to create an output scene, facilitating transfer of coherent structures and local detail from train scene geometry. We demonstrate our neural scene reconstruction with a database for the tasks of 3D super-resolution and surface reconstruction from sparse point clouds, showing that our approach enables generation of more coherent, accurate 3D scenes, improving on average by over 8% in IoU over state-of-the-art scene reconstruction.</summary></entry><entry><title type="html">Neural RGB-D Surface Reconstruction</title><link href="https://justusthies.github.io/posts/rgbdnerf/" rel="alternate" type="text/html" title="Neural RGB-D Surface Reconstruction" /><published>2021-04-12T08:30:00+02:00</published><updated>2021-04-12T08:30:00+02:00</updated><id>https://justusthies.github.io/posts/rgbdnerf</id><content type="html" xml:base="https://justusthies.github.io/posts/rgbdnerf/">&lt;p&gt;In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dejan Azinovic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/dejan_azinovic/profile.html&quot;}</name></author><category term="ARXIV" /><summary type="html">In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.</summary></entry><entry><title type="html">NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction</title><link href="https://justusthies.github.io/posts/nerface/" rel="alternate" type="text/html" title="NerFACE&amp;#58; Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction" /><published>2021-03-03T08:00:00+01:00</published><updated>2021-03-03T08:00:00+01:00</updated><id>https://justusthies.github.io/posts/nerface</id><content type="html" xml:base="https://justusthies.github.io/posts/nerface/">&lt;p&gt;We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face.
Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications.
Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoint or head-poses is required.
In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks.
To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions.
We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup.
In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gafniguy.github.io/4D-Facial-Avatars&quot;&gt;Project Page&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Guy Gafni&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/guy_gafni/profile.html&quot;}</name></author><category term="CVPR" /><summary type="html">We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoint or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.</summary></entry><entry><title type="html">Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction</title><link href="https://justusthies.github.io/posts/neuraldeformationgraphs/" rel="alternate" type="text/html" title="Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction" /><published>2021-03-03T07:00:00+01:00</published><updated>2021-03-03T07:00:00+01:00</updated><id>https://justusthies.github.io/posts/neuraldeformationgraphs</id><content type="html" xml:base="https://justusthies.github.io/posts/neuraldeformationgraphs/">&lt;p&gt;We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. 
Specifically, we implicitly model a deformation graph via a deep neural network.
This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking.
Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object.
Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints,  the underlying network is trained in a self-supervised fashion.
We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation.
Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings.
Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="CVPR" /><summary type="html">We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.</summary></entry></feed>