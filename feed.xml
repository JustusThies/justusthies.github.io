<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2025-05-09T15:38:08+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">GEM - Gaussian Eigen Models for Human Heads</title><link href="https://justusthies.github.io/posts/gem/" rel="alternate" type="text/html" title="GEM - Gaussian Eigen Models for Human Heads" /><published>2025-05-01T10:00:00+02:00</published><updated>2025-05-01T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/gem</id><content type="html" xml:base="https://justusthies.github.io/posts/gem/"><![CDATA[<p>Current personalized neural head avatars face a trade-off: lightweight models lack detail and realism, while high-quality, animatable avatars require significant computational resources, making them unsuitable for commodity devices. To address this gap, we introduce Gaussian Eigen Models (GEM), which provide high-quality, lightweight, and easily controllable head avatars. GEM utilizes 3D Gaussian primitives for representing the appearance combined with Gaussian splatting for rendering. Building on the success of mesh-based 3D morphable face models (3DMM), we define GEM as an ensemble of linear eigenbases for representing the head appearance of a specific subject. In particular, we construct linear bases to represent the position, scale, rotation, and opacity of the 3D Gaussians. This allows us to efficiently generate Gaussian primitives of a specific head shape by a linear combination of the basis vectors, only requiring a low-dimensional parameter vector that contains the respective coefficients. We propose to construct these linear bases (GEM) by distilling high-quality compute-intense CNN-based Gaussian avatar models that can generate expression-dependent appearance changes like wrinkles. These high-quality models are trained on multi-view videos of a subject and are distilled using a series of principle component analyses. Once we have obtained the bases that represent the animatable appearance space of a specific human, we learn a regressor that takes a single RGB image as input and predicts the low-dimensional parameter vector that corresponds to the shown facial expression. We demonstrate that this regressor can be trained such that it effectively supports self- and cross-person reenactment from monocular videos without requiring prior mesh-based tracking. In a series of experiments, we compare GEM’s self-reenactment and cross-person reenactment results to state-of-the-art 3D avatar methods, demonstrating GEM’s higher visual quality and better generalization to new expressions. As our distilled linear model is highly efficient in generating novel animation states, we also show a real-time demo of GEMs driven by monocular webcam videos. The code and model will be released for research purposes.</p>

<p><a href="https://github.com/Zielon/GEM">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Wojciech Zielonka&quot;, &quot;website_link&quot;=&gt;&quot;https://zielon.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We introduce Gaussian Eigen Models (GEM), which provide high-quality, lightweight, and easily controllable head avatars, making them suitable for real-time applications on commodity devices.]]></summary></entry><entry><title type="html">SynShot - Synthetic Prior for Few-Shot Drivable Head Avatar Inversion</title><link href="https://justusthies.github.io/posts/synshot/" rel="alternate" type="text/html" title="SynShot - Synthetic Prior for Few-Shot Drivable Head Avatar Inversion" /><published>2025-05-01T10:00:00+02:00</published><updated>2025-05-01T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/synshot</id><content type="html" xml:base="https://justusthies.github.io/posts/synshot/"><![CDATA[<p>We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior. We tackle two major challenges. First, state-of-the-art monocular avatar models struggle to generalize to new views and expressions, lacking a strong prior and often overfitting to a specific viewpoint distribution. Second, training a controllable 3D generative network requires a large number of diverse sequences, for which pairs of images and high-quality tracked meshes are not always available. Moreover, under General Data Protection Regulation, the storage and use of real datasets are highly regulated. Users are often required to delete trained models and remove any derivatives of the real data within a specified time frame if a subject from the dataset withdraws their consent, as guaranteed by law. This process is very cumbersome from a data management perspective, whereas synthetic datasets are not subject to these regulations. With few input images (usually 3 or 5), SynShot fine-tunes the pretrained synthetic prior to bridge the domain gap, modeling a photorealistic head avatar that generalizes to novel expressions and viewpoints.</p>

<p><a href="https://github.com/Zielon/synshot">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Wojciech Zielonka&quot;, &quot;website_link&quot;=&gt;&quot;https://zielon.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We present SynShot, a novel method for the few-shot inversion of a drivable head avatar based on a synthetic prior.]]></summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS25/" rel="alternate" type="text/html" title="Lecture: 3D Scanning and Motion Capture" /><published>2025-04-24T23:00:00+02:00</published><updated>2025-04-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS25</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-SS25/"><![CDATA[<h3 id="content">Content</h3>
<p>The lecture and exercises will cover 3D reconstruction from various input
modalities (Webcams, RGB-D cameras (Kinect, Realsense, …). It will start with
basic concepts of what is 3D, the different representations, how to capture 3D
and how the devices and sensors function. Based on this introduction, rigid and
non-rigid tracking and reconstruction will be discussed. Specialized face and
body tracking methods will be covered and the applications of the 3D
reconstruction and tracking will be shown. In addition to the 3D surface
reconstruction, techniques for appearance modelling and material estimation
will be shown.</p>

<p>++ Basic concepts of geometry (Meshes, Point Clouds, Pixels &amp; Voxels)
++ RGB and Depth Cameras (Calibration, active/passive stereo, Time of
Flight (ToF), Structured Light, Laser Scanner, Lidar)
++ Surface Representations (Polygonal meshes, parametric surfaces,
implicit surfaces (Radial basis functions, signed distance functions,
indicator function), Marching cubes)
++ Overview of reconstruction methods (Structure from Motion (SfM),
Multi-view Stereo (MVS), SLAM, Bundle Adjustment)
++ Rigid Surface Tracking &amp; Reconstruction (Pose alignment, ICP, online
surface reconstruction pipeline (KinectFusion), scalable surface
representations (VoxelHashing, OctTrees), loop closures and global
optimization)
++ Non-rigid Surface Tracking &amp; Reconstruction (Surface deformation for
modeling, Regularizers: ARAP, ED, etc., Non-rigid surface fitting: e.g.,
non-rigid ICP. Non-rigid reconstruction:
DynamicFusion/VolumeDeform/KillingFusion)
++ Face Tracking &amp; Reconstruction (Keypoint detection &amp; tracking,
Parametric / Statistical Models -&gt; BlendShapes)
++ Body Tracking &amp; Reconstruction (Skeleton Tracking and Inverse
Kinematics, Marker-based motion capture)
++ Material capture (Lightstage, BRDF estimation)
++ Outlook DeepLearning-based tracking</p>

<h3 id="aim">Aim</h3>
<p>Basic understanding of 3D capturing devices and underlying principles (active vs.
passive stereo, ToF etc.), modelling of geometry and conversion between
different representations, principles of static reconstruction (fusion, ICP) and
non-rigid reconstruction using deformation priors. Basic understanding of
specialized class-specific tracking (face, body, hands) and their applications.</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[This hands-on lecture focuses on cutting-edge 3D reconstruction approaches, including volumetric fusion approaches on implicit functions, face tracking, hand tracking, and much more! We also cover essential optimization techniques such as Gauss-Newton and Levenberg-Marquardt.]]></summary></entry><entry><title type="html">Lecture: AI-based 3D Graphics and Vision</title><link href="https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-WS25/" rel="alternate" type="text/html" title="Lecture: AI-based 3D Graphics and Vision" /><published>2025-04-24T23:00:00+02:00</published><updated>2025-04-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-WS25</id><content type="html" xml:base="https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-WS25/"><![CDATA[<h3 id="content">Content</h3>
<p>The lecture will cover AI-based 3D reconstruction from various input modalities (Webcams, RGB-D cameras (Kinect, Realsense, …). The lecture builds upon the classical 3D reconstruction methods discussed in the ‘3D Scanning &amp; Motion Capture’ lecture and shows how components and data structures of those methods can be replaced or extended by methods from AI. It will start with basic concepts of 2D neural rendering, including methods like Pix2Pix, Deferred Neural Rendering and alike. Then, more advanced topics like 3D/4D neural scene representations are discussed. To train those representations, differentiable rendering needs to be understood. The lecture will introduce methods for static and dynamic reconstruction with different levels of controllability and editability.
++ Concept of 2D Neural Rendering
++ Deferred Neural Rendering, AI-based Image-based Rendering.
++ 3D Neural Rendering and Neural Scene Representations
++ Neural Radiance Fields (NeRFs)
++ Neural Point-based Graphics, Gaussian Splatting
++ Differentiable Rendering (Rasterization, Volume Rendering, Shading)
++ Relighting and Material Reconstruction
++ DeepFakes
++ Outlook: detection of synthetic media</p>

<h3 id="aim">Aim</h3>
<p>Basic understanding of how methods of AI can be used for 3D capturing of objects, scenes, and humans. It includes the principles of 2D and 3D neural rendering, as well as the differentiable rendering (Inverse Graphics).</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[The lecture will cover AI-based 3D reconstruction, synthesis, and rendering from various input modalities (image, text, audio). It covers topics like 2D/3D neural rendering, deepfakes, natural image synthesis with different controls, as well as motion and video generation.]]></summary></entry><entry><title type="html">Practical Course: 3D Scanning and Spatial Learning</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-SS2025/" rel="alternate" type="text/html" title="Practical Course: 3D Scanning and Spatial Learning" /><published>2025-04-24T23:00:00+02:00</published><updated>2025-04-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-SS2025</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-SS2025/"><![CDATA[<h3 id="content">Content</h3>
<p>3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp; Motion Capture as well as by the Introduction to Deep Learning lecture.
In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field.
Specifically, we will have projects on:</p>
<ul>
  <li>human motion capturing (e.g., Fusion4D, BodyFusion)</li>
  <li>real-time facial motion capturing (spare and dense approaches)</li>
  <li>3D scene reconstruction (e.g., BundleFusion)</li>
  <li>scan refinement (e.g., ShapeFromShading)</li>
  <li>neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering)</li>
  <li>scene completion (e.g., ScanComplete)</li>
  <li>3D object retrieval and alignment (e.g., Scan2CAD)</li>
  <li>scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS)</li>
  <li>neural scene representations (DeepSDF, OccupancyNets, NeRF,…)</li>
  <li>distilling image diffusion models for 3D reconstructions</li>
</ul>

<h3 id="objective">Objective</h3>
<p>Upon completion of this module, students will have acquired extensive theoretical concepts behind state-of-the art 3D reconstruction methods, in particular in the context of human motion capturing, static object scanning, scene understanding and synthesis of captured scenes. Besides the theoretical foundations, a significant aspect lies on the practical realization and implementation of such algorithms.</p>

<h3 id="organization">Organization</h3>
<p>In the practical course students shall get familiar with state-of-the-art 3D scanning. They will be assisted by current PhD students working in this field (regular office hours). To ensure a good progress during the semester, we will have mandatory meetings (every two weeks) where the students report their current state. In the end of the course, the students are asked to give a talk about their project and results.</p>

<h3 id="prerequisites">Prerequisites</h3>
<p>Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, 3D scanning &amp; Motion Capture, Introduction to Deep Learning, C++</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[3D scanning and motion capture is of paramount importance for content creation, human-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning & Motion Capture as well as by the Introduction to Deep Learning lecture.]]></summary></entry><entry><title type="html">Joker - Conditional 3D Head Synthesis with Extreme Facial Expressions</title><link href="https://justusthies.github.io/posts/joker/" rel="alternate" type="text/html" title="Joker - Conditional 3D Head Synthesis with Extreme Facial Expressions" /><published>2024-11-25T09:00:00+01:00</published><updated>2024-11-25T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/joker</id><content type="html" xml:base="https://justusthies.github.io/posts/joker/"><![CDATA[<p>We introduce Joker, a new method for the conditional synthesis of 3D human heads with extreme expressions. Given a single reference image of a person, we synthesize a volumetric human head with the reference’s identity and a new expression. We offer control over the expression via a 3D morphable model (3DMM) and textual inputs. This multi-modal conditioning signal is essential since 3DMMs alone fail to define subtle emotional changes and extreme expressions, including those involving the mouth cavity and tongue articulation. Our method is built upon a 2D diffusion-based prior that generalizes well to out-of-domain samples, such as sculptures, heavy makeup, and paintings while achieving high levels of expressiveness. To improve view consistency, we propose a new 3D distillation technique that converts predictions of our 2D prior into a neural radiance field (NeRF). Both the 2D prior and our distillation technique produce state-of-the-art results, which are confirmed by our extensive evaluations. Also, to the best of our knowledge, our method is the first to achieve view-consistent extreme tongue articulation.</p>

<p><a href="https://malteprinzler.github.io/projects/joker/index.html">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Malte Prinzler&quot;, &quot;website_link&quot;=&gt;&quot;https://ncs.is.mpg.de/person/mprinzler&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[Joker uses one reference image to generate a 3D reconstruction with a novel extreme expression. The target expression is defined through 3DMM parameters and text prompts. The text prompts effectively resolve ambiguities in the 3DMM input and can control emotion-related expression subtleties and tongue articulation.]]></summary></entry><entry><title type="html">VMV2024 - Program Chair</title><link href="https://justusthies.github.io/posts/vmv/" rel="alternate" type="text/html" title="VMV2024 - Program Chair" /><published>2024-11-12T08:00:00+01:00</published><updated>2024-11-12T08:00:00+01:00</updated><id>https://justusthies.github.io/posts/vmv</id><content type="html" xml:base="https://justusthies.github.io/posts/vmv/"><![CDATA[<p>The Symposium on Vision, Modeling, and Visualization (VMV) will take place for the 29th time in 2024. It is the annual symposium of the German research community in computer graphics and visualization, and it serves the annual gathering of the members of the “Fachbereich Graphische Datenverarbeitung” of the German “Gesellschaft für Informatik”.</p>

<p><a href="https://diglib.eg.org/handle/10.2312/3607041">VMV2024 Collection</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Lars Linsen&quot;}</name></author><category term="services" /><category term="VMV" /><summary type="html"><![CDATA[The Symposium on Vision, Modeling, and Visualization (VMV) is the annual symposium of the German research community in computer graphics and visualization.]]></summary></entry><entry><title type="html">SPRIN-D Funke - DeepFake Detection and Prevention</title><link href="https://justusthies.github.io/posts/sprind_challenge/" rel="alternate" type="text/html" title="SPRIN-D Funke - DeepFake Detection and Prevention" /><published>2024-11-12T08:00:00+01:00</published><updated>2024-11-12T08:00:00+01:00</updated><id>https://justusthies.github.io/posts/sprind_challenge</id><content type="html" xml:base="https://justusthies.github.io/posts/sprind_challenge/"><![CDATA[<p>The rapid development of digital technologies in recent years has opened up both fascinating and unsettling possibilities, particularly in the area of synthetic content and deepfakes. This sophisticated AI-generated audio, image and video content is now so realistic that it is almost indistinguishable from real footage.
With the exponential increase in the performance of systems and language models based on artificial intelligence (AI) and machine learning (ML), not only the quantity but the quality of deepfakes is improving rapidly. This poses dangers – for trust, security and the perception of reality in our society.
Efforts to combat deepfakes are currently focused on two major areas:</p>
<ul>
  <li>Detection: Development of AI algorithms to identify deepfakes</li>
  <li>Prevention: Implementation of authentication mechanisms for digital content. Despite significant progress in these areas, major challenges remain, such as the generalizability and scalability of AI deepfake detection systems and the establishment of a manipulation-resistant standard for image metadata.</li>
</ul>

<p><em>The task: Develop a prototype that reliably detects deepfakes images from various media content and/or protects existing infrastructures from the use of deepfakes with preventive measures.</em></p>

<p>The prototype must demonstrate how deepfakes images can be reliably detected and authenticated. It can be AI-supported and should be able to continuously adapt to new deepfake techniques. At least three different use cases (e.g. social media, news portals, video conferencing systems) will be demonstrated by the end of the process. Scalability and adaptability to different digital platforms are essential.</p>

<p>The SPRIND Funke runs over a period of 13 months. At the end of October 2024, our expert jury  selected twelve teams as participants for the first stage of the Funke.
SPRIND provides intensive and individualized support, which includes funding of up to €350,000 for each team in the first phase of the Funke. After seven months, the jury reconvenes to evaluate the progress and decide which approaches have the greatest potential for breakthrough innovation. The selected teams will then have the opportunity to prove themselves in a second phase of the Funke, which provides up to €375,000 per team in additional funding.</p>

<p>Jury:  Isabelle Sonnenfeld, Carola Plesch, Johannes Otterbach, Johannes Otterbach, Felix Kartte, Justus Thies, Ahmad-Reza Sadeghi, Stefan Kirschnick</p>

<p><a href="https://www.sprind.org/impulse/challenges/funke-deepfake#funkebrdeepfake-detection-and-prevention">SPRIN-D Challenge Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="services" /><category term="SPRIND" /><summary type="html"><![CDATA[Synthetic Media can be an opportunity, but also a thread. In an exhibition at the Schloss Museum in Tübingen, vistors were able to try out recent facial reenactment methods and learn more about spotting manipulated media.]]></summary></entry><entry><title type="html">3DiFACE - Diffusion-based Speech-driven 3D Facial Animation and Editing</title><link href="https://justusthies.github.io/posts/3diface/" rel="alternate" type="text/html" title="3DiFACE - Diffusion-based Speech-driven 3D Facial Animation and Editing" /><published>2024-11-02T09:00:00+01:00</published><updated>2024-11-02T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/3diface</id><content type="html" xml:base="https://justusthies.github.io/posts/3diface/"><![CDATA[<p>Creating an animation of a specific person with audio-synced lip motions, realistic head motion and editing via artist-defined keyframes are a set of tasks that challenge existing speech-driven 3D facial animation methods. Especially, editing 3D facial animation is a complex and time-consuming task carried out by highly skilled animators. Also, most existing works overlook the inherent one-to-many relationship between speech and facial motion, where multiple plausible lip and head animations could sync with the audio input. To this end, we present 3DiFACE, a novel method for holistic speech-driven 3D facial animation, which produces diverse plausible lip and head motions for a single audio input, while also allowing editing via keyframing and interpolation. 3DiFACE is a lightweight audio-conditioned diffusion model, which can be fine-tuned to generate personalized 3D facial animation requiring only a short video of the subject. Specifically, we leverage the viseme-level diversity in our training corpus to train a fully-convolutional diffusion model that produces diverse sequences for single audio input. Additionally, we employ a modified guided motion diffusion to enable head-motion synthesis and editing using masking. Through quantitative and qualitative evaluations, we demonstrate that our method is capable of generating and editing diverse holistic 3D facial animations given a single audio input, with control between high fidelity and diversity.</p>

<p><a href="https://balamuruganthambiraja.github.io/3DiFACE/">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Balamurugan Thambiraja&quot;, &quot;website_link&quot;=&gt;&quot;https://balamuruganthambiraja.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[We present 3DiFACE, a novel audio-conditioned diffusion model for holistic speech-driven 3D facial animation, which produces diverse plausible lip and head motions for a single audio input, while also allowing editing via keyframing and interpolation.]]></summary></entry><entry><title type="html">D3GA - Drivable 3D Gaussian Avatars</title><link href="https://justusthies.github.io/posts/dega/" rel="alternate" type="text/html" title="D3GA - Drivable 3D Gaussian Avatars" /><published>2024-11-01T09:00:00+01:00</published><updated>2024-11-01T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/dega</id><content type="html" xml:base="https://justusthies.github.io/posts/dega/"><![CDATA[<p>We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats. Current photorealistic drivable avatars require either accurate 3D registrations during training, dense input images during testing, or both. The ones based on neural radiance fields also tend to be prohibitively slow for telepresence applications. This work uses the recently presented 3D Gaussian Splatting (3DGS) technique to render realistic humans at real-time framerates, using dense calibrated multi-view videos as input. To deform those primitives, we depart from the commonly used point deformation method of linear blend skinning (LBS) and use a classic volumetric deformation method: cage deformations. Given their smaller size, we drive these deformations with joint angles and keypoints, which are more suitable for communication applications. Our experiments on nine subjects with varied body shapes, clothes, and motions obtain higher-quality results than state-of-the-art methods when using the same training and test data.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Wojciech Zielonka&quot;, &quot;website_link&quot;=&gt;&quot;https://zielon.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="3DV" /><summary type="html"><![CDATA[We present Drivable 3D Gaussian Avatars (D3GA), the first 3D controllable model for human bodies rendered with Gaussian splats.]]></summary></entry></feed>