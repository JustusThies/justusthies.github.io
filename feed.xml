<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2020-03-19T19:14:41+01:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Adversarial Texture Optimization from RGB-D Scans</title><link href="https://justusthies.github.io/posts/advtex/" rel="alternate" type="text/html" title="Adversarial Texture Optimization from RGB-D Scans" /><published>2020-03-19T10:00:00+01:00</published><updated>2020-03-19T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/advtex</id><content type="html" xml:base="https://justusthies.github.io/posts/advtex/">&lt;p&gt;Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera poses, and view-dependent imaging artifacts.
In this work, we present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views.
Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors.
The key idea of our approach is to learn a patch-based conditional discriminator which guides the texture optimization to be tolerant to misalignments.
Our discriminator takes a synthesized view and a real image, and evaluates whether the synthesized one is realistic, under a broadened definition of realism.
We train the discriminator by providing as ‘real’ examples pairs of input views and their misaligned versions – so that the learned adversarial loss will tolerate errors from the scans.
Experiments on synthetic and real data under quantitative or qualitative evaluation demonstrate the advantage of our approach in comparison to state of the art.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Jingwei Huang&quot;, &quot;website_link&quot;=&gt;&quot;http://stanford.edu/~jingweih/&quot;}</name></author><category term="CVPR" /><summary type="html">Realistic color texture generation is an important step in RGB-D surface reconstruction, but remains challenging in practice due to inaccuracies in reconstructed geometry, misaligned camera poses, and view-dependent imaging artifacts. In this work, we present a novel approach for color texture generation using a conditional adversarial loss obtained from weakly-supervised views. Specifically, we propose an approach to produce photorealistic textures for approximate surfaces, even from misaligned images, by learning an objective function that is robust to these errors. The key idea of our approach is to learn a patch-based conditional discriminator which guides the texture optimization to be tolerant to misalignments. Our discriminator takes a synthesized view and a real image, and evaluates whether the synthesized one is realistic, under a broadened definition of realism. We train the discriminator by providing as ‘real’ examples pairs of input views and their misaligned versions – so that the learned adversarial loss will tolerate errors from the scans. Experiments on synthetic and real data under quantitative or qualitative evaluation demonstrate the advantage of our approach in comparison to state of the art.</summary></entry><entry><title type="html">Image-guided Neural Object Rendering</title><link href="https://justusthies.github.io/posts/ignor/" rel="alternate" type="text/html" title="Image-guided Neural Object Rendering" /><published>2020-01-15T10:00:00+01:00</published><updated>2020-01-15T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/ignor</id><content type="html" xml:base="https://justusthies.github.io/posts/ignor/">&lt;p&gt;We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis.
The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours and sightseeing, the digital inspection of historical artifacts).
A core component of our work is the handling of view-dependent effects.
Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object.
As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo.
Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering.
This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts.
To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects.
Based on these estimations, we are able to convert observed images to diffuse images.
These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects.
To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results.
Using this image-guided approach, the network does not have to allocate capacity on ``remembering’’ object appearance, instead it learns how to combine the appearance of captured images.
We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="ICLR" /><summary type="html">We propose a learned image-guided rendering technique that combines the benefits of image-based rendering and GAN-based image synthesis. The goal of our method is to generate photo-realistic re-renderings of reconstructed objects for virtual and augmented reality applications (e.g., virtual showrooms, virtual tours and sightseeing, the digital inspection of historical artifacts). A core component of our work is the handling of view-dependent effects. Specifically, we directly train an object-specific deep neural network to synthesize the view-dependent appearance of an object. As input data we are using an RGB video of the object. This video is used to reconstruct a proxy geometry of the object via multi-view stereo. Based on this 3D proxy, the appearance of a captured view can be warped into a new target view as in classical image-based rendering. This warping assumes diffuse surfaces, in case of view-dependent effects, such as specular highlights, it leads to artifacts. To this end, we propose EffectsNet, a deep neural network that predicts view-dependent effects. Based on these estimations, we are able to convert observed images to diffuse images. These diffuse images can be projected into other views. In the target view, our pipeline reinserts the new view-dependent effects. To composite multiple reprojected images to a final output, we learn a composition network that outputs photo-realistic results. Using this image-guided approach, the network does not have to allocate capacity on ``remembering’’ object appearance, instead it learns how to combine the appearance of captured images. We demonstrate the effectiveness of our approach both qualitatively and quantitatively on synthetic as well as on real data.</summary></entry><entry><title type="html">Neural Voice Puppetry: Audio-driven Facial Reenactment</title><link href="https://justusthies.github.io/posts/neural-voice-puppetry/" rel="alternate" type="text/html" title="Neural Voice Puppetry&amp;#58; &lt;br&gt; Audio-driven Facial Reenactment" /><published>2019-12-12T10:00:00+01:00</published><updated>2019-12-12T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/neural-voice-puppetry</id><content type="html" xml:base="https://justusthies.github.io/posts/neural-voice-puppetry/">&lt;p&gt;We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis.
Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input.
This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space.
Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames.
Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches.
Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples.
Our method is not only more general than existing works since we are generic to the input person, but we also show superior visual and lip sync quality compared to photo-realistic audio- and video-driven reenactment techniques.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="ArXiv" /><summary type="html">We present Neural Voice Puppetry, a novel approach for audio-driven facial video synthesis. Given an audio sequence of a source person or digital assistant, we generate a photo-realistic output video of a target person that is in sync with the audio of the source input. This audio-driven facial reenactment is driven by a deep neural network that employs a latent 3D face model space. Through the underlying 3D representation, the model inherently learns temporal stability while we leverage neural rendering to generate photo-realistic output frames. Our approach generalizes across different people, allowing us to synthesize videos of a target actor with the voice of any unknown source actor or even synthetic voices that can be generated utilizing standard text-to-speech approaches. Neural Voice Puppetry has a variety of use-cases, including audio-driven video avatars, video dubbing, and text-driven video synthesis of a talking head. We demonstrate the capabilities of our method in a series of audio- and text-based puppetry examples. Our method is not only more general than existing works since we are generic to the input person, but we also show superior visual and lip sync quality compared to photo-realistic audio- and video-driven reenactment techniques.</summary></entry><entry><title type="html">SpoC: Spoofing Camera Fingerprints</title><link href="https://justusthies.github.io/posts/spoc/" rel="alternate" type="text/html" title="SpoC&amp;#58; Spoofing Camera Fingerprints" /><published>2019-11-26T09:00:00+01:00</published><updated>2019-11-26T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/spoc</id><content type="html" xml:base="https://justusthies.github.io/posts/spoc/">&lt;p&gt;Thanks to the fast progress in synthetic media generation, creating realistic false images has become very easy.
Such images can be used to wrap  “rich” fake news with enhanced credibility, spawning a new wave of high-impact, high-risk misinformation campaigns.
Therefore, there is a fast-growing interest in reliable detectors of manipulated media.
The most powerful detectors, to date, rely on the subtle traces left by any device on all images acquired by it.
In particular, due to proprietary in-camera processes, like demosaicing or compression, each camera model leaves trademark traces that can be exploited for forensic analyses.
The absence or distortion of such traces in the target image is a strong hint of manipulation.&lt;/p&gt;

&lt;p&gt;In this paper, we challenge such detectors to gain better insight into their vulnerabilities.
This is an important study in order to build better forgery detectors able to face malicious attacks. Our proposal consists of a GAN-based approach that injects camera traces into synthetic images.
Given a GANgenerated image, we insert the traces of a specific camera model into it and deceive state-of-the-art detectors into believing the image was acquired by that model.
Likewise, we deceive independent detectors of synthetic GAN images into believing the image is real. Experiments prove the effectiveness of the proposed method in a wide array of conditions.
Moreover, no prior information on the attacked detectors is needed, but only sample images from the target camera.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Davide Cozzolino&quot;, &quot;website_link&quot;=&gt;&quot;http://www.grip.unina.it/people/userprofile/davide_cozzolino.html&quot;}</name></author><category term="ArXiv" /><summary type="html">Thanks to the fast progress in synthetic media generation, creating realistic false images has become very easy. Such images can be used to wrap “rich” fake news with enhanced credibility, spawning a new wave of high-impact, high-risk misinformation campaigns. Therefore, there is a fast-growing interest in reliable detectors of manipulated media. The most powerful detectors, to date, rely on the subtle traces left by any device on all images acquired by it. In particular, due to proprietary in-camera processes, like demosaicing or compression, each camera model leaves trademark traces that can be exploited for forensic analyses. The absence or distortion of such traces in the target image is a strong hint of manipulation.</summary></entry><entry><title type="html">Cabinet Meeting: Synthetic Media - Danger or Chance?</title><link href="https://justusthies.github.io/posts/face_berlin/" rel="alternate" type="text/html" title="Cabinet Meeting&amp;#58; Synthetic Media - Danger or Chance?" /><published>2019-11-18T10:00:00+01:00</published><updated>2019-11-18T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/face_berlin</id><content type="html" xml:base="https://justusthies.github.io/posts/face_berlin/">&lt;p&gt;In this session we showed several demonstrations of our current projects, to inform the Cabinet of Germany about the risks and the chances of synthetic media at Schloss Meseberg.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.bundesregierung.de/breg-de/themen/digitalisierung/deepfakes-mit-kuenstlicher-intelligenz-gegen-gefaelschte-fotos-und-videos-1692244&quot;&gt;Weblink to the official website.&lt;/a&gt;&lt;/p&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;de&quot; dir=&quot;ltr&quot;&gt;Gut, dass wir in Deutschland so hervorragende Forschung und Lehre wie an der &lt;a href=&quot;https://twitter.com/TU_Muenchen?ref_src=twsrc%5Etfw&quot;&gt;@tu_muenchen&lt;/a&gt; haben. Danke &lt;a href=&quot;https://twitter.com/MattNiessner?ref_src=twsrc%5Etfw&quot;&gt;@mattniessner&lt;/a&gt; für den großartigen Vortrag vor dem Kabinett zu Deep Learning und Deepfakes. Da sind wir weltweit mit an der Spitze. @justusnies &lt;a href=&quot;https://twitter.com/ondyari?ref_src=twsrc%5Etfw&quot;&gt;@ondyari&lt;/a&gt; &lt;a href=&quot;https://t.co/Melh8wK9Rj&quot;&gt;pic.twitter.com/Melh8wK9Rj&lt;/a&gt;&lt;/p&gt;&amp;mdash; Dorothee Bär (@DoroBaer) &lt;a href=&quot;https://twitter.com/DoroBaer/status/1196385570781638656?ref_src=twsrc%5Etfw&quot;&gt;November 18, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;Presenting to the cabinet of Germany about &lt;a href=&quot;https://twitter.com/hashtag/AI?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#AI&lt;/a&gt; and &lt;a href=&quot;https://twitter.com/hashtag/SyntheticMedia?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#SyntheticMedia&lt;/a&gt;, and &lt;a href=&quot;https://twitter.com/hashtag/MediaForensics?src=hash&amp;amp;ref_src=twsrc%5Etfw&quot;&gt;#MediaForensics&lt;/a&gt;. Vivid discussion, lots of engagement :)&lt;br /&gt;&lt;br /&gt;Great opportunity to meet chancellor Angela Merkel, &lt;a href=&quot;https://twitter.com/OlafScholz?ref_src=twsrc%5Etfw&quot;&gt;@OlafScholz&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/DoroBaer?ref_src=twsrc%5Etfw&quot;&gt;@DoroBaer&lt;/a&gt;, Horst Seehofer, &lt;a href=&quot;https://twitter.com/HBraun?ref_src=twsrc%5Etfw&quot;&gt;@HBraun&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/AndiScheuer?ref_src=twsrc%5Etfw&quot;&gt;@AndiScheuer&lt;/a&gt;, &lt;a href=&quot;https://twitter.com/RegSprecher?ref_src=twsrc%5Etfw&quot;&gt;@RegSprecher&lt;/a&gt;, etc.&lt;a href=&quot;https://twitter.com/TU_Muenchen?ref_src=twsrc%5Etfw&quot;&gt;@TU_Muenchen&lt;/a&gt; &lt;a href=&quot;https://t.co/iOM3btNreO&quot;&gt;pic.twitter.com/iOM3btNreO&lt;/a&gt;&lt;/p&gt;&amp;mdash; Matthias Niessner (@MattNiessner) &lt;a href=&quot;https://twitter.com/MattNiessner/status/1196463897219358720?ref_src=twsrc%5Etfw&quot;&gt;November 18, 2019&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;https://platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Matthias Nie\u00DFner&quot;, &quot;website_link&quot;=&gt;&quot;https://niessnerlab.org&quot;}</name></author><category term="Bundesregierung" /><summary type="html">In this session we showed several demonstrations of our current projects, to inform the Cabinet of Germany about the risks and the chances of synthetic media at Schloss Meseberg.</summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS19-20/" rel="alternate" type="text/html" title="Lecture&amp;#58; 3D Scanning and Motion Capture" /><published>2019-10-10T23:00:00+02:00</published><updated>2019-10-10T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS19-20</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS19-20/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic concepts of geometry
    &lt;ul&gt;
      &lt;li&gt;Meshes (polygonal), Point Clouds, Pixels &amp;amp; Voxels&lt;/li&gt;
      &lt;li&gt;RGB and Depth Cameras&lt;/li&gt;
      &lt;li&gt;Extrinsics and Intrinsics&lt;/li&gt;
      &lt;li&gt;Capture devices&lt;/li&gt;
      &lt;li&gt;RGB and Multi-view&lt;/li&gt;
      &lt;li&gt;RGB-D cameras&lt;/li&gt;
      &lt;li&gt;Stereo&lt;/li&gt;
      &lt;li&gt;Time of Flight (ToF)&lt;/li&gt;
      &lt;li&gt;Structured Light&lt;/li&gt;
      &lt;li&gt;Laser Scanner, Lidar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Surface Representations
    &lt;ul&gt;
      &lt;li&gt;Polygonal meshes (trimeshes, etc.)&lt;/li&gt;
      &lt;li&gt;Parametric surfaces: splines, nurbs&lt;/li&gt;
      &lt;li&gt;Implicit surfaces&lt;/li&gt;
      &lt;li&gt;Ridge-based surfaces&lt;/li&gt;
      &lt;li&gt;Radial basis functions&lt;/li&gt;
      &lt;li&gt;Signed distance functions (volumetric, Curless &amp;amp; Levoy)&lt;/li&gt;
      &lt;li&gt;Indicator function (Poisson Surface Reconstruction)&lt;/li&gt;
      &lt;li&gt;More general: level sets&lt;/li&gt;
      &lt;li&gt;Marching cubes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;High-level overview of reconstructions
    &lt;ul&gt;
      &lt;li&gt;Structure from Motion (SfM)&lt;/li&gt;
      &lt;li&gt;Multi-view Stereo (MVS)&lt;/li&gt;
      &lt;li&gt;SLAM&lt;/li&gt;
      &lt;li&gt;Bundle Adjustment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimization
    &lt;ul&gt;
      &lt;li&gt;Non-linear least squares&lt;/li&gt;
      &lt;li&gt;Gauss-Newton LM&lt;/li&gt;
      &lt;li&gt;Examples in Ceres&lt;/li&gt;
      &lt;li&gt;Symbolic diff vs auto-diff&lt;/li&gt;
      &lt;li&gt;Auto-diff with dual numbers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Pose alignment&lt;/li&gt;
      &lt;li&gt;ICP (point cloud alignment; depth-to-model alignment; rigid ‘fitting’)&lt;/li&gt;
      &lt;li&gt;Online surface reconstruction pipeline: KinectFusion&lt;/li&gt;
      &lt;li&gt;Scalable surface representations: VoxelHashing, OctTrees&lt;/li&gt;
      &lt;li&gt;Loop closures and global optimization&lt;/li&gt;
      &lt;li&gt;Robust optimization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Non-rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Surface deformation for modeling&lt;/li&gt;
      &lt;li&gt;Regularizers: ARAP, ED, etc.&lt;/li&gt;
      &lt;li&gt;Non-rigid surface fitting: e.g., non-rigid ICP&lt;/li&gt;
      &lt;li&gt;Non-rigid reconstruction: DynamicFusion/VolumeDeform/KillingFusion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Body Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Skeleton Tracking and Inverse Kinematics&lt;/li&gt;
      &lt;li&gt;Learning-based approaches from RGB and RGB-D&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Face Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Keypoint detection &amp;amp; tracking&lt;/li&gt;
      &lt;li&gt;Parametric / Statistical Models -&amp;gt; BlendShapes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hand Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Parametric Models&lt;/li&gt;
      &lt;li&gt;Some DeepLearning-based things&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminative vs generative tracking
    &lt;ul&gt;
      &lt;li&gt;Random forests&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Motion Capture in Movies
    &lt;ul&gt;
      &lt;li&gt;Marker-based motion capture&lt;/li&gt;
      &lt;li&gt;LightStage -&amp;gt; movies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BRDF and Material Capture&lt;/li&gt;
  &lt;li&gt;Open research questions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).</summary></entry><entry><title type="html">Practical Course: 3D Scanning and Spatial Learning</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS19-20/" rel="alternate" type="text/html" title="Practical Course&amp;#58; 3D Scanning and Spatial Learning" /><published>2019-10-07T23:00:00+02:00</published><updated>2019-10-07T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS19-20</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS19-20/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp;amp; Motion Capture as well as by the Introduction to Deep Learning lecture.
In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field.
Specifically, we will have projects on:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;human motion capturing (e.g., Fusion4D, BodyFusion)&lt;/li&gt;
  &lt;li&gt;real-time facial motion capturing (spare and dense approaches)&lt;/li&gt;
  &lt;li&gt;3D scene reconstruction (e.g., BundleFusion)&lt;/li&gt;
  &lt;li&gt;scan refinement (e.g., ShapeFromShading)&lt;/li&gt;
  &lt;li&gt;neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering)&lt;/li&gt;
  &lt;li&gt;scene completion (e.g., ScanComplete)&lt;/li&gt;
  &lt;li&gt;3D object retrieval and alignment (e.g., Scan2CAD)&lt;/li&gt;
  &lt;li&gt;scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;objective&quot;&gt;Objective&lt;/h3&gt;
&lt;p&gt;Upon completion of this module, students will have acquired extensive theoretical concepts behind state-of-the art 3D reconstruction methods, in particular in the context of human motion capturing, static object scanning, scene understanding and synthesis of captured scenes. Besides the theoretical foundations, a significant aspect lies on the practical realization and implementation of such algorithms.&lt;/p&gt;

&lt;h3 id=&quot;organization&quot;&gt;Organization&lt;/h3&gt;
&lt;p&gt;In the practical course students shall get familiar with state-of-the-art 3D scanning. They will be assisted by current PhD students working in this field (regular office hours). To ensure a good progress during the semester, we will have mandatory meetings (every two weeks) where the students report their current state. In the end of the course, the students are asked to give a talk about their project and results.&lt;/p&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, 3D scanning &amp;amp; Motion Capture, Introduction to Deep Learning, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp;amp; Motion Capture as well as by the Introduction to Deep Learning lecture. In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field. Specifically, we will have projects on: human motion capturing (e.g., Fusion4D, BodyFusion) real-time facial motion capturing (spare and dense approaches) 3D scene reconstruction (e.g., BundleFusion) scan refinement (e.g., ShapeFromShading) neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering) scene completion (e.g., ScanComplete) 3D object retrieval and alignment (e.g., Scan2CAD) scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS)</summary></entry><entry><title type="html">Seminar: 3D Vision</title><link href="https://justusthies.github.io/posts/3D-Vision-WS19-20/" rel="alternate" type="text/html" title="Seminar&amp;#58; 3D Vision" /><published>2019-10-07T23:00:00+02:00</published><updated>2019-10-07T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Vision-WS19-20</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Vision-WS19-20/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).&lt;/p&gt;

&lt;p&gt;The talks have to cover the following topics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;face tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;facial reenactment (audio driven, video driven animation)&lt;/li&gt;
  &lt;li&gt;body tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;hand tracking&lt;/li&gt;
  &lt;li&gt;static / dynamic scene reconstruction&lt;/li&gt;
  &lt;li&gt;scene understanding&lt;/li&gt;
  &lt;li&gt;inverse rendering, material estimation, light estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;organization--requirements&quot;&gt;Organization / Requirements&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Lecture: 3D Scanning &amp;amp; Motion Capturing&lt;/li&gt;
  &lt;li&gt;The participants have to present their topics in a talk (in English), which should last 30 minutes.&lt;/li&gt;
  &lt;li&gt;The semi-final slides (PDF) should be sent one week before the talk; otherwise, the talk will be canceled.&lt;/li&gt;
  &lt;li&gt;A short report (approximately 2-3 pages in the ACM SIGGRAPH TOG format (acmtog)) should be prepared and sent within two weeks after the talk. When you send the report, please send the final slides (PDF) together.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).</summary></entry><entry><title type="html">FaceForensics++: Learning to Detect Manipulated Facial Images</title><link href="https://justusthies.github.io/posts/faceforensics++/" rel="alternate" type="text/html" title="FaceForensics++&amp;#58; &lt;br&gt; Learning to Detect Manipulated Facial Images" /><published>2019-08-26T11:00:00+02:00</published><updated>2019-08-26T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/faceforensics++</id><content type="html" xml:base="https://justusthies.github.io/posts/faceforensics++/">&lt;p&gt;The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.&lt;/p&gt;

&lt;h2 id=&quot;dataset-access&quot;&gt;Dataset Access&lt;/h2&gt;
&lt;p&gt;If you would like to download the FaceForensics and FaceForensics++ datasets, please fill out this &lt;a href=&quot;https://docs.google.com/forms/d/e/1FAIpQLSdRRR3L5zAv6tQ_CKxmK4W96tAab_pfBu2EKAgQbeDVhmXagg/viewform&quot;&gt;google form&lt;/a&gt; and, once accepted, we will send you the link to our download script.&lt;/p&gt;

&lt;p&gt;&lt;b&gt;Update:&lt;/b&gt; We are also hosting the DeepFakes Detection Dataset which includes various high quality scenes of multiple actors which have been manipulated using DeepFakes. The dataset was donated by Google/Jigsaw to support the community effort on detecting manipulated faces. To obtain this dataset please use the google form provided above.&lt;/p&gt;

&lt;h2 id=&quot;benchmark&quot;&gt;Benchmark&lt;/h2&gt;
&lt;p&gt;We are offering an automated benchmark for facial manipulation detection on the presence of compression based on our manipulation methods. If you are interested to test your approach on unseen data, visit it &lt;a href=&quot;http://kaldir.vc.in.tum.de/faceforensics_benchmark/&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;source-code--contact&quot;&gt;Source Code &amp;amp; Contact&lt;/h2&gt;
&lt;p&gt;For more information about our code, visit our &lt;a href=&quot;https://github.com/ondyari/FaceForensics&quot;&gt;github&lt;/a&gt; or contact us under &lt;a href=&quot;mailto:faceforensics@googlegroups.com&quot;&gt;faceforensics@googlegroups.com&lt;/a&gt;.&lt;/p&gt;

&lt;h2 id=&quot;links&quot;&gt;Links&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;http://www.grip.unina.it/research/83-multimedia_forensics/106-deepfake_detection.html&quot;&gt;Results on Youtube Videos&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Andreas R\u00F6ssler&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/andreas_roessler/profile.html&quot;}</name></author><category term="ICCV" /><summary type="html">The rapid progress in synthetic image generation and manipulation has now come to a point where it raises significant concerns for the implications towards society. At best, this leads to a loss of trust in digital content, but could potentially cause further harm by spreading false information or fake news. This paper examines the realism of state-of-the-art image manipulations, and how difficult it is to detect them, either automatically or by humans. To standardize the evaluation of detection methods, we propose an automated benchmark for facial manipulation detection. In particular, the benchmark is based on DeepFakes, Face2Face, FaceSwap and NeuralTextures as prominent representatives for facial manipulations at random compression level and size. The benchmark is publicly available and contains a hidden test set as well as a database of over 1.8 million manipulated images. This dataset is over an order of magnitude larger than comparable, publicly available, forgery datasets. Based on this data, we performed a thorough analysis of data-driven forgery detectors. We show that the use of additional domain specific knowledge improves forgery detection to unprecedented accuracy, even in the presence of strong compression, and clearly outperforms human observers.</summary></entry><entry><title type="html">Deferred Neural Rendering: Image Synthesis using Neural Textures</title><link href="https://justusthies.github.io/posts/deferred-neural-rendering/" rel="alternate" type="text/html" title="Deferred Neural Rendering&amp;#58; &lt;br&gt; Image Synthesis using Neural Textures" /><published>2019-04-28T11:00:00+02:00</published><updated>2019-04-28T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/deferred-neural-rendering</id><content type="html" xml:base="https://justusthies.github.io/posts/deferred-neural-rendering/">&lt;p&gt;The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input.
In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings.
To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components.
Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process.
Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline.
Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect.
In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains.
For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space.
This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates.
We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="SIGGRAPH" /><summary type="html">The modern computer graphics pipeline can synthesize images at remarkable visual quality; however, it requires well-defined, high-quality 3D content as input. In this work, we explore the use of imperfect 3D content, for instance, obtained from photo-metric reconstructions with noisy and incomplete surface geometry, while still aiming to produce photo-realistic (re-)renderings. To address this challenging problem, we introduce Deferred Neural Rendering, a new paradigm for image synthesis that combines the traditional graphics pipeline with learnable components. Specifically, we propose Neural Textures, which are learned feature maps that are trained as part of the scene capture process. Similar to traditional textures, neural textures are stored as maps on top of 3D mesh proxies; however, the high-dimensional feature maps contain significantly more information, which can be interpreted by our new deferred neural rendering pipeline. Both neural textures and deferred neural renderer are trained end-to-end, enabling us to synthesize photo-realistic images even when the original 3D content was imperfect. In contrast to traditional, black-box 2D generative neural networks, our 3D representation gives us explicit control over the generated output, and allows for a wide range of application domains. For instance, we can synthesize temporally-consistent video re-renderings of recorded 3D scenes as our representation is inherently embedded in 3D space. This way, neural textures can be utilized to coherently re-render or manipulate existing video content in both static and dynamic environments at real-time rates. We show the effectiveness of our approach in several experiments on novel view synthesis, scene editing, and facial reenactment, and compare to state-of-the-art approaches that leverage the standard graphics pipeline as well as conventional generative neural networks.</summary></entry></feed>