<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.3">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2023-09-11T11:01:42+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS23-24/" rel="alternate" type="text/html" title="Lecture: 3D Scanning and Motion Capture" /><published>2023-08-24T23:00:00+02:00</published><updated>2023-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS23-24</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS23-24/"><![CDATA[<h3 id="content">Content</h3>
<p>The lecture and exercises will cover 3D reconstruction from various input
modalities (Webcams, RGB-D cameras (Kinect, Realsense, …). It will start with
basic concepts of what is 3D, the different representations, how to capture 3D
and how the devices and sensors function. Based on this introduction, rigid and
non-rigid tracking and reconstruction will be discussed. Specialized face and
body tracking methods will be covered and the applications of the 3D
reconstruction and tracking will be shown. In addition to the 3D surface
reconstruction, techniques for appearance modelling and material estimation
will be shown.</p>

<p>++ Basic concepts of geometry (Meshes, Point Clouds, Pixels &amp; Voxels)
++ RGB and Depth Cameras (Calibration, active/passive stereo, Time of
Flight (ToF), Structured Light, Laser Scanner, Lidar)
++ Surface Representations (Polygonal meshes, parametric surfaces,
implicit surfaces (Radial basis functions, signed distance functions,
indicator function), Marching cubes)
++ Overview of reconstruction methods (Structure from Motion (SfM),
Multi-view Stereo (MVS), SLAM, Bundle Adjustment)
++ Rigid Surface Tracking &amp; Reconstruction (Pose alignment, ICP, online
surface reconstruction pipeline (KinectFusion), scalable surface
representations (VoxelHashing, OctTrees), loop closures and global
optimization)
++ Non-rigid Surface Tracking &amp; Reconstruction (Surface deformation for
modeling, Regularizers: ARAP, ED, etc., Non-rigid surface fitting: e.g.,
non-rigid ICP. Non-rigid reconstruction:
DynamicFusion/VolumeDeform/KillingFusion)
++ Face Tracking &amp; Reconstruction (Keypoint detection &amp; tracking,
Parametric / Statistical Models -&gt; BlendShapes)
++ Body Tracking &amp; Reconstruction (Skeleton Tracking and Inverse
Kinematics, Marker-based motion capture)
++ Material capture (Lightstage, BRDF estimation)
++ Outlook DeepLearning-based tracking</p>

<h3 id="aim">Aim</h3>
<p>Basic understanding of 3D capturing devices and underlying principles (active vs.
passive stereo, ToF etc.), modelling of geometry and conversion between
different representations, principles of static reconstruction (fusion, ICP) and
non-rigid reconstruction using deformation priors. Basic understanding of
specialized class-specific tracking (face, body, hands) and their applications.</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[This hands-on lecture focuses on cutting-edge 3D reconstruction approaches, including volumetric fusion approaches on implicit functions, face tracking, hand tracking, and much more! We also cover essential optimization techniques such as Gauss-Newton and Levenberg-Marquardt.]]></summary></entry><entry><title type="html">Seminar: Digital Humans</title><link href="https://justusthies.github.io/posts/Digital-Humans-WS23-24/" rel="alternate" type="text/html" title="Seminar: Digital Humans" /><published>2023-08-24T23:00:00+02:00</published><updated>2023-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/Digital-Humans-WS23-24</id><content type="html" xml:base="https://justusthies.github.io/posts/Digital-Humans-WS23-24/"><![CDATA[<h3 id="content">Content</h3>
<p>The goal of the seminar is to get the students in touch with recent publications
in the research field. They will learn how publications are structured and
presented. They will learn how methods are validated / evaluated.
In addition, the students will train their presentation skills by preparing a talk on
a selected topic (recent publication of their choice).</p>

<p>The talks have to cover the following topics:</p>
<ul>
  <li>face tracking / reconstruction</li>
  <li>facial reenactment (audio driven, video driven animation)</li>
  <li>body tracking / reconstruction</li>
  <li>hand tracking</li>
  <li>inverse rendering, material estimation, light estimation</li>
</ul>

<h3 id="organization--requirements">Organization / Requirements</h3>
<ul>
  <li>Lecture: 3D Scanning &amp; Motion Capturing (optional)</li>
  <li>The participants have to present their topics in a talk (in English), which should last 30 minutes.</li>
  <li>The semi-final slides (PDF) should be sent one week before the talk; otherwise, the talk will be canceled.</li>
  <li>A short report (approximately 2-3 pages in the ACM SIGGRAPH TOG format (acmtog)) should be prepared and sent within two weeks after the talk. When you send the report, please send the final slides (PDF) together.</li>
</ul>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[In this course, students will autonomously investigate recent research about Digital Humans. Independent investigation for further reading, critical analysis, and evaluation of the topic is required.]]></summary></entry><entry><title type="html">Imitator - Personalized Speech-driven 3D Facial Animation</title><link href="https://justusthies.github.io/posts/imitator/" rel="alternate" type="text/html" title="Imitator - Personalized Speech-driven 3D Facial Animation" /><published>2023-08-14T11:00:00+02:00</published><updated>2023-08-14T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/imitator</id><content type="html" xml:base="https://justusthies.github.io/posts/imitator/"><![CDATA[<p>Speech-driven 3D facial animation has been widely explored, with applications in gaming, character animation, virtual reality, and telepresence systems. State-of-the-art methods deform the face topology of the target actor to sync the input audio without considering the identity-specific speaking style and facial idiosyncrasies of the target actor, thus, resulting in unrealistic and inaccurate lip movements. To address this, we present Imitator, a speech-driven facial expression synthesis method, which learns identity-specific details from a short input video and produces novel facial expressions matching the identity-specific speaking style and facial idiosyncrasies of the target actor. Specifically, we train a style-agnostic transformer on a large facial expression dataset which we use as a prior for audio-driven facial expressions. Based on this prior, we optimize for identity-specific speaking style based on a short reference video. To train the prior, we introduce a novel loss function based on detected bilabial consonants to ensure plausible lip closures and consequently improve the realism of the generated expressions. Through detailed experiments and a user study, we show that our approach produces temporally coherent facial expressions from input audio while preserving the speaking style of the target actors.</p>

<p><a href="https://balamuruganthambiraja.github.io/Imitator/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Balamurugan Thambiraja&quot;, &quot;website_link&quot;=&gt;&quot;https://balamuruganthambiraja.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ICCV" /><summary type="html"><![CDATA[We present Imitator, a speech-driven facial expression synthesis method, which learns identity-specific details from a short input video and produces novel facial expressions matching the identity-specific speaking style and facial idiosyncrasies of the target actor. Specifically, we train a style-agnostic transformer on a large facial expression dataset which we use as a prior for audio-driven facial expressions. Based on this prior, we optimize for identity-specific speaking style based on a short reference video.]]></summary></entry><entry><title type="html">3D Graphics and Vision at the Technical University of Darmstadt</title><link href="https://justusthies.github.io/posts/tuda/" rel="alternate" type="text/html" title="3D Graphics and Vision at the Technical University of Darmstadt" /><published>2023-08-14T11:00:00+02:00</published><updated>2023-08-14T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/tuda</id><content type="html" xml:base="https://justusthies.github.io/posts/tuda/"><![CDATA[<p> </p>
<div style="text-align: justify">
I am happy to announce that I am joining the Technical University Darmstadt as a full professor for 3D Graphics and Vision in September 2023.
My group will work at the intersection of computer graphics, computer vision and machine learning.
</div>

<div style="text-align: justify"> 
The main theme of my work is to capture and to (re-)synthesize the real world using commodity hardware.
It includes the modeling of the human body, tracking, as well as the reconstruction and interaction with the environment.
The digitization is needed for various applications in AR/VR as well as in movie (post-)production.
Teleconferencing and working in VR is of high interest for many companies ranging from social media platforms to car manufacturer.
It enables the remote interaction in VR, e.g., the inspection of 3D content like CAD models or scans from real objects.
A realistic reproduction of appearances and motions is key for such applications. 
Capturing natural motions and expressions as well as the photorealistic reproduction of images under novel views are challenging.
With the rise of deep learning methods and, especially, neural rendering, we see immense progress to succeed in these challenges.
The goal of my work is to develop methods for AI-based image synthesis of humans, the underlying representation of appearance, geometry and motion to allow for explicit and implicit control over the synthesis process.
My work on 3D reconstruction, tracking and rendering does not focus exclusively on humans but also on the environment and objects we interact with, thus, enabling applications like 3d telepresence or collaborative working in VR.
In both areas, reconstruction and rendering, hybrid approaches that combine novel findings in machine learning with classical computer graphics and computer vision approaches show promising results.
Nevertheless, these methods still suffer from limitations like generalizability, controllability and editability which I will tackle in my ongoing and future work.
</div>
<p> </p>

<p><img src="face_overview.jpg" alt="Face Projects" />  </p>

<p><img src="scene_reco.jpg" alt="Scene Reconstruction Projects" />  </p>

<p><img src="non-rigid-tracking.jpg" alt="Neural Non-rigid Tracking Projects" />  </p>

<p>On my <a href="https://justusthies.github.io/publications/">website</a> and on my <a href="https://www.youtube.com/channel/UCwmSTvnV-sjtlIlNvWYC6Ow">YouTube page</a> you will find a couple of interesting projects I have been working on in the past:</p>

<ul>
  <li><a href="https://www.youtube.com/watch?v=p7UIF1Mw5GI&amp;list=PLVOhpVSqYrEYMhjQ0MQ62JZgtRGTW5hPW&amp;ab_channel=JustusThies">Facial Reenactment / Reconstruction</a></li>
  <li><a href="https://www.youtube.com/watch?v=ofVgAEb1FiE&amp;list=PLVOhpVSqYrEbIFkFsvdtO8FUkqbmn7cUQ&amp;ab_channel=JustusThies">Neural Rendering</a></li>
  <li><a href="https://www.youtube.com/watch?v=Kj_P-lHtWkU&amp;list=PLVOhpVSqYrEYqHZIEfY4R6Lgp90LJFUd2&amp;ab_channel=JustusThies">Neural Non-rigid Tracking</a></li>
  <li><a href="https://www.youtube.com/watch?v=52xlRn0ESek&amp;list=PLVOhpVSqYrEZ6D3FFabhQolw5ZRCNVfiI&amp;ab_channel=MatthiasNiessner">3D Reconstruction / Texturing</a></li>
  <li><a href="https://www.youtube.com/watch?v=Tle7YaPkO_k&amp;list=PLVOhpVSqYrEaZUh3ZktMfXwWuiQTMvuV8&amp;ab_channel=MatthiasNiessner">Multi-media Forensics</a></li>
</ul>

<p><br /><br /></p>

<hr />]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="TUDa" /><summary type="html"><![CDATA[I am happy to announce that I am joining the Technical University Darmstadt as a full professor for 3D Graphics and Vision in September 2023. My group will work at the intersection of computer graphics, computer vision and machine learning.]]></summary></entry><entry><title type="html">CaPhy - Capturing Physical Properties for Animatable Human Avatars</title><link href="https://justusthies.github.io/posts/caphy/" rel="alternate" type="text/html" title="CaPhy - Capturing Physical Properties for Animatable Human Avatars" /><published>2023-08-14T10:00:00+02:00</published><updated>2023-08-14T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/caphy</id><content type="html" xml:base="https://justusthies.github.io/posts/caphy/"><![CDATA[<p>We present CaPhy, a novel method for reconstructing animatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geometric and physical properties of the clothing from real observations. This allows us to apply novel poses to the human avatar with physically correct deformations and wrinkles of the clothing. To this end, we combine unsupervised training with physics-based losses and 3D-supervised training using scanned data to reconstruct a dynamic model of clothing that is physically realistic and conforms to the human scans. We also optimize the physical parameters of the underlying physical model from the scans by introducing gradient constraints of the physics-based losses. In contrast to previous work on 3D avatar reconstruction, our method is able to generalize to novel poses with realistic dynamic cloth deformations. Experiments on several subjects demonstrate that our method can estimate the physical properties of the garments, resulting in superior quantitative and qualitative results compared with previous methods.</p>

<p><a href="https://suzhaoqi.github.io/projects/CaPhy/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Zhaoqi Su&quot;, &quot;website_link&quot;=&gt;&quot;https://suzhaoqi.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ICCV" /><summary type="html"><![CDATA[We present CaPhy, a novel method for reconstructing animatable human avatars with realistic dynamic properties for clothing. Specifically, we aim for capturing the geometric and physical properties of the clothing from real observations. This allows us to apply novel poses to the human avatar with physically correct deformations and wrinkles of the clothing.]]></summary></entry><entry><title type="html">TADA! Text to Animatable Digital Avatars</title><link href="https://justusthies.github.io/posts/tada/" rel="alternate" type="text/html" title="TADA! Text to Animatable Digital Avatars" /><published>2023-08-13T11:00:00+02:00</published><updated>2023-08-13T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/tada</id><content type="html" xml:base="https://justusthies.github.io/posts/tada/"><![CDATA[<p>We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines. Existing text-based character generation methods are limited in terms of geometry and texture quality, and cannot be realistically animated due to inconsistent align-007 ment between the geometry and the texture, particularly in the face region. To overcome these limitations, TADA leverages the synergy of a 2D diffusion model and an animatable parametric body model. Specifically, we derive an optimizable high-resolution body model from SMPL-X with 3D displacements and a texture map, and use hierarchical rendering with score distillation sampling (SDS) to create high-quality, detailed, holistic 3D avatars from text. To ensure alignment between the geometry and texture, we render normals and RGB images of the generated character and exploit their latent embeddings in the SDS training process. We further introduce various expression parameters to deform the generated character during training, ensuring that the semantics of our generated character remain consistent with the original SMPL-X model, resulting in an animatable character. Comprehensive evaluations demonstrate that TADA significantly surpasses existing approaches on both qualitative and quantitative measures. TADA enables creation of large-scale digital character assets that are ready for animation and rendering, while also being easily editable through natural language. The code will be public for research purposes.</p>

<p><a href="https://tada.is.tue.mpg.de/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Tingting Liao&quot;, &quot;website_link&quot;=&gt;&quot;https://github.com/tingtingliao&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ARXIV" /><summary type="html"><![CDATA[We introduce TADA, a simple-yet-effective approach that takes textual descriptions and produces expressive 3D avatars with high-quality geometry and lifelike textures, that can be animated and rendered with traditional graphics pipelines.]]></summary></entry><entry><title type="html">TeCH - Text-guided Reconstruction of Lifelike Clothed Humans</title><link href="https://justusthies.github.io/posts/tech/" rel="alternate" type="text/html" title="TeCH - Text-guided Reconstruction of Lifelike Clothed Humans" /><published>2023-08-13T11:00:00+02:00</published><updated>2023-08-13T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/tech</id><content type="html" xml:base="https://justusthies.github.io/posts/tech/"><![CDATA[<p>Despite recent research advancements in reconstructing clothed humans from a single image, accurately restoring the “unseen regions” with high-level details remains an unsolved challenge that lacks attention. Existing methods often generate overly smooth back-side surfaces with a blurry texture. But how to effectively capture all visual attributes of an individual from a single image, which are sufficient to reconstruct unseen areas (e.g., the back view)? Motivated by the power of foundation models, TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the “indescribable” appearance. To represent high-resolution 3D clothed humans at an affordable cost, we propose a hybrid 3D representation based on DMTet, which consists of an explicit body shape grid and an implicit distance field. Guided by the descriptive prompts + personalized T2I diffusion model, the geometry and texture of the 3D humans are optimized through multi-view Score Distillation Sampling (SDS) and reconstruction losses based on the original observation. TeCH produces high-fidelity 3D clothed humans with consistent &amp; delicate texture, and detailed full-body geometry. Quantitative and qualitative experiments demonstrate that TeCH outperforms the state-of-the-art methods in terms of reconstruction accuracy and rendering quality.</p>

<p><a href="https://huangyangyi.github.io/TeCH/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Yangyi Huang&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ARXIV" /><summary type="html"><![CDATA[TeCH reconstructs the 3D human by leveraging 1) descriptive text prompts (e.g., garments, colors, hairstyles) which are automatically generated via a garment parsing model and Visual Question Answering (VQA), 2) a personalized fine-tuned Text-to-Image diffusion model (T2I) which learns the "indescribable" appearance.]]></summary></entry><entry><title type="html">Text-guided Editing of Textured 3D Morphable Models</title><link href="https://justusthies.github.io/posts/clipface/" rel="alternate" type="text/html" title="Text-guided Editing of Textured 3D Morphable Models" /><published>2023-03-30T11:00:00+02:00</published><updated>2023-03-30T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/clipface</id><content type="html" xml:base="https://justusthies.github.io/posts/clipface/"><![CDATA[<p>We propose ClipFace, a novel self-supervised approach for text-guided editing of textured 3D morphable model of faces. Specifically, we employ user-friendly language prompts to enable control of the expressions as well as appearance of 3D faces. We leverage the geometric expressiveness of 3D morphable models, which inherently possess limited controllability and texture expressivity, and develop a self-supervised generative model to jointly synthesize expressive, textured, and articulated faces in 3D. We enable high-quality texture generation for 3D faces by adversarial self-supervised training, guided by differentiable rendering against collections of real RGB images. Controllable editing and manipulation are given by language prompts to adapt texture and expression of the 3D morphable model. To this end, we propose a neural network that predicts both texture and expression latent codes of the morphable model. Our model is trained in a self-supervised fashion by exploiting differentiable rendering and losses based on a pre-trained CLIP model. Once trained, our model jointly predicts face textures in UV-space, along with expression parameters to capture both geometry and texture changes in facial expressions in a single forward pass. We further show the applicability of our method to generate temporally changing textures for a given animation sequence.
<a href="https://shivangi-aneja.github.io/projects/clipface/">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Shivangi Aneja&quot;, &quot;website_link&quot;=&gt;&quot;https://niessnerlab.org/members/shivangi_aneja/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="SIGGRAPH" /><summary type="html"><![CDATA[ClipFace is a novel self-supervised approach for text-guided editing of textured 3D morphable model of faces. Controllable editing and manipulation are given by language prompts to adapt texture and expression of the 3D morphable model.]]></summary></entry><entry><title type="html">High-Res Facial Appearance Capture from Polarized Smartphone Images</title><link href="https://justusthies.github.io/posts/polface/" rel="alternate" type="text/html" title="High-Res Facial Appearance Capture from Polarized Smartphone Images" /><published>2023-03-30T11:00:00+02:00</published><updated>2023-03-30T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/polface</id><content type="html" xml:base="https://justusthies.github.io/posts/polface/"><![CDATA[<p>We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil. Specifically, we turn the flashlight into a polarized light source and add a polarization filter on top of the camera. Leveraging this setup, we capture the face of a subject with cross-polarized and parallel-polarized light. For each subject, we record two short sequences in a dark environment under flash illumination with different light polarization using the modified smartphone. Based on these observations, we reconstruct an explicit surface mesh of the face using structure from motion. We then exploit the camera and light co-location within a differentiable renderer to optimize the facial textures using an analysis-by-synthesis approach.</p>

<p><a href="https://dazinovic.github.io/polface/">Project Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Dejan Azinovic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/dejan_azinovic/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We propose a novel method for high-quality facial texture reconstruction from RGB images using a novel capturing routine based on a single smartphone which we equip with an inexpensive polarization foil.]]></summary></entry><entry><title type="html">Human-Aware 3D Scene Generation</title><link href="https://justusthies.github.io/posts/mime/" rel="alternate" type="text/html" title="Human-Aware 3D Scene Generation" /><published>2023-03-29T11:00:00+02:00</published><updated>2023-03-29T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/mime</id><content type="html" xml:base="https://justusthies.github.io/posts/mime/"><![CDATA[<p>Generating realistic 3D worlds occupied by moving humans has many applications in games, architecture, and synthetic data creation. But generating such scenes is expensive and labor intensive. Recent work generates human poses and motions given a 3D scene. Here, we take the opposite approach and generate 3D indoor scenes given 3D human motion. Such motions can come from archival motion capture or from IMU sensors worn on the body, effectively turning human movement in a “scanner” of the 3D world. Intuitively, human movement indicates the free-space in a room and human contact indicates surfaces or objects that support activities such as sitting, lying or touching. We propose MIME (Mining Interaction and Movement to infer 3D Environments), which is a generative model of indoor scenes that produces furniture layouts that are consistent with the human movement. MIME uses an auto-regressive transformer architecture that takes the already generated objects in the scene as well as the human motion as input, and outputs the next plausible object. To train MIME, we build a dataset by populating the 3D FRONT scene dataset with 3D humans. Our experiments show that MIME produces more diverse and plausible 3D scenes than a recent generative scene method that does not know about human movement.</p>

<p><a href="https://mime.is.tue.mpg.de">Code and data</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Hongwei Yi&quot;, &quot;website_link&quot;=&gt;&quot;https://xyyhw.top/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[Humans constantly interact with their environment. They walk through a room, touch objects, rest on a chair, or sleep in a bed. All these interactions contain information about the scene layout and object placement which we leverage to generate scenes from human motion.]]></summary></entry></feed>