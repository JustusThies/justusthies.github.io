<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2022-07-04T15:50:17+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Towards Metrical Reconstruction of Human Faces</title><link href="https://justusthies.github.io/posts/mica/" rel="alternate" type="text/html" title="Towards Metrical Reconstruction of Human Faces" /><published>2022-07-04T09:33:00+02:00</published><updated>2022-07-04T09:33:00+02:00</updated><id>https://justusthies.github.io/posts/mica</id><content type="html" xml:base="https://justusthies.github.io/posts/mica/"><![CDATA[<p>Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context (i.e., when there is a reference object of known size). A metrical reconstruction is also needed for any application that measures distances and dimensions of the subject (e.g., to virtually fit a glasses frame). State-of-the-art methods for face reconstruction from a single image are trained on large 2D image datasets in a self-supervised fashion. However, due to the nature of a perspective projection they are not able to reconstruct the actual face dimensions, and even predicting the average human face outperforms some of these methods in a metrical sense. To learn the actual shape of a face, we argue for a supervised training scheme. Since there exists no large-scale 3D dataset for this task, we annotated and unified small- and medium-scale databases. The resulting unified dataset is still a medium-scale dataset with more than 2k identities and training purely on it would lead to overfitting. To this end, we take advantage of a face recognition network pretrained on a large-scale 2D image dataset, which provides distinct features for different faces and is robust to expression, illumination, and camera changes. Using these features, we train our face shape estimator in a supervised fashion, inheriting the robustness and generalization of the face recognition network. Our method, which we call MICA (MetrIC fAce), outperforms the state-of-the-art reconstruction methods by a large margin, both on current non-metric benchmarks as well as on our metric (15% and 24% lower average error on NoW, respectively).</p>

<p><a href="https://zielon.github.io/mica/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Wojciech Zielonka&quot;, &quot;website_link&quot;=&gt;&quot;https://zielon.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[Face reconstruction and tracking is a building block of numerous applications in AR/VR, human-machine interaction, as well as medical applications. Most of these applications rely on a metrically correct prediction of the shape, especially, when the reconstructed subject is put into a metrical context. Thus, we present MICA, a novel metrical face reconstruction method that combines face recognition with supervised face shape learning.]]></summary></entry><entry><title type="html">Generating Textures on 3D Shape Surfaces</title><link href="https://justusthies.github.io/posts/texturify/" rel="alternate" type="text/html" title="Generating Textures on 3D Shape Surfaces" /><published>2022-07-04T09:00:00+02:00</published><updated>2022-07-04T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/texturify</id><content type="html" xml:base="https://justusthies.github.io/posts/texturify/"><![CDATA[<p>Texture cues on 3D objects are key to compelling visual representations, with the possibility to create high visual fidelity with inherent spatial consistency across different views. Since the availability of textured 3D shapes remains very limited, learning a 3D-supervised data-driven method that predicts a texture based on the 3D input is very challenging. We thus propose Texurify, a GAN-based method that leverages a 3D shape dataset of an object class and learns to reproduce the distribution of appearances observed in real images by generating high-quality textures. In particular, our method does not require any 3D color supervision or correspondence between shape geometry and images to learn the texturing of 3D objects. Texurify operates directly on the surface of the 3D objects by introducing face convolutional operators on a hierarchical 4-RoSy parameterization to generate plausible object-specific textures. Employing differentiable rendering and adversarial losses that critique individual views and consistency across views, we effectively learn the high-quality surface texturing distribution from real-world images. Experiments on car and chair shape collections show that our approach outperforms state of the art by an average of 22% in FID score.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Yawar Siddiqui&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/yawar_siddiqui/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[Texturify learns to generate geometry-aware textures for untextured collections of 3D objects. Our method trains from only a collection of images and a collection of untextured shapes, which are both often available, without requiring any explicit 3D color supervision or shape-image correspondence. Textures are created directly on the surface of a given 3D shape, enabling generation of high-quality, compelling textured 3D shapes.]]></summary></entry><entry><title type="html">CLIPme if you can!</title><link href="https://justusthies.github.io/posts/clipme/" rel="alternate" type="text/html" title="CLIPme if you can!" /><published>2022-05-12T09:00:00+02:00</published><updated>2022-05-12T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/clipme</id><content type="html" xml:base="https://justusthies.github.io/posts/clipme/"><![CDATA[<p><a href="https://openai.com/blog/clip/">CLIP</a> is a powerful tool to match images to text descriptions.
It can be used for classification of ImageNet classes as shown in the original paper, but it can also be used to assign names of (famous) people to images.
Specifically, we assume that we have an image and a list of names of people in the image as input (also called queries below).</p>

<p>In a first step, we detect faces in the input image which can give us the regions of interest:
<img src="solvay_conference_1927_detections.jpg" alt="Face Detections" style="width:600px;height:450px;" /></p>

<p>Using these detections, we can run the CLIP model on the cropped regions of interest and compute the matching score w.r.t. the names of people we provide as input.
We annotate the original image with the results that match best with the names:</p>

<p><img src="solvay_conference_1927_annotations.jpg" alt="Face Annotations" style="width:600px;height:450px;" /></p>

<p>As can be seen, it works surprisingly well. Except for Hendrik Lorentz (which actually sits between Marie Curie and Albert Einstein), it matches all queries.</p>

<p>It also works well for other famous people, like state leaders:
<img src="G7_annotations.jpg" alt="Face Annotations" style="width:600px;height:400px;" /></p>

<p>Interestingly, CLIP also knows the relationship of the countries they represent:
<img src="G7_annotations_countries.jpg" alt="Face Annotations" style="width:600px;height:400px;" /></p>

<p>Obviously, I had to try it with an image of myself and it turns out that it can also annotate my face:
<img src="berlin_annotations.jpg" alt="Face Annotations" style="width:600px;height:450px;" /></p>

<p><img src="fmx_annotations.jpg" alt="Face Annotations" style="width:600px;height:450px;" /></p>

<h1 id="conclusion">Conclusion:</h1>
<p>CLIP not only stores general concepts of text and images, to some extent it is also storing information about individual people.
We can also see this in the results of <a href="https://openai.com/dall-e-2/">DALL-E-2</a>, where we can ask to render a cat in an outfit like Napoleon.</p>

<h1 id="code">Code:</h1>
<p><a href="https://github.com/JustusThies/CLIPme">GITHUB</a></p>

<h1 id="image-credits">Image Credits:</h1>
<ul>
  <li><a href="https://i.redd.it/okqe57386di51.jpg">Solvay conference 1927</a> - image colorized by Sanna Dullaway</li>
  <li><a href="https://www.tagesschau.de/multimedia/bilder/g7-121~_v-gross20x9.jpg">G7</a></li>
  <li><a href="https://justusthies.github.io/posts/face_berlin/thumb.jpg">Cabinet Meeting</a></li>
  <li><a href="https://pbs.twimg.com/media/D5fal0cWsAQxpCT?format=jpg">FMX2019</a></li>
</ul>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="tutorial" /><category term="PLAYGROUND" /><summary type="html"><![CDATA[CLIP is a powerful tool to match images to text descriptions. It can be used for classification of ImageNet classes as shown in the original paper, but it can also be used to assign names of (famous) people to images.]]></summary></entry><entry><title type="html">Neural Head Avatars from Monocular RGB Videos</title><link href="https://justusthies.github.io/posts/neuralhead/" rel="alternate" type="text/html" title="Neural Head Avatars from Monocular RGB Videos" /><published>2022-03-22T08:30:00+01:00</published><updated>2022-03-22T08:30:00+01:00</updated><id>https://justusthies.github.io/posts/neuralhead</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralhead/"><![CDATA[<p>We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be used for teleconferencing in AR/VR or other applications in the movie or games industry that rely on a digital human. Our representation can be learned from a monocular RGB portrait video that features a range of different expressions and views. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture. We demonstrate that this representation is able to accurately extrapolate to unseen poses and view points, and generates natural expressions while providing sharp texture details. Compared to previous works on head avatars, our method provides a disentangled shape and appearance model of the complete human head (including hair) that is compatible with the standard graphics pipeline. Moreover, it quantitatively and qualitatively outperforms current state of the art in terms of reconstruction quality and novel-view synthesis.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Philip-William Grassal&quot;, &quot;website_link&quot;=&gt;&quot;https://hci.iwr.uni-heidelberg.de/vislearn/people&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar using a deep neural network. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture.]]></summary></entry><entry><title type="html">Mover: Human-Aware Object Placement for Visual Environment Reconstruction</title><link href="https://justusthies.github.io/posts/mover/" rel="alternate" type="text/html" title="Mover: Human-Aware Object Placement for Visual Environment Reconstruction" /><published>2022-03-22T07:30:00+01:00</published><updated>2022-03-22T07:30:00+01:00</updated><id>https://justusthies.github.io/posts/mover</id><content type="html" xml:base="https://justusthies.github.io/posts/mover/"><![CDATA[<p>Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects, (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Hongwei Yi&quot;, &quot;website_link&quot;=&gt;&quot;https://xyyhw.top/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We demonstrate that human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout.]]></summary></entry><entry><title type="html">Neural RGB-D Surface Reconstruction</title><link href="https://justusthies.github.io/posts/rgbdnerf/" rel="alternate" type="text/html" title="Neural RGB-D Surface Reconstruction" /><published>2022-03-22T07:30:00+01:00</published><updated>2022-03-22T07:30:00+01:00</updated><id>https://justusthies.github.io/posts/rgbdnerf</id><content type="html" xml:base="https://justusthies.github.io/posts/rgbdnerf/"><![CDATA[<p>In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Dejan Azinovic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/dejan_azinovic/profile.html&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="CVPR" /><summary type="html"><![CDATA[We demonstrate how depth measurements can be incorporated into the neural radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone.]]></summary></entry><entry><title type="html">Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/advancedneuralrenderingstar/" rel="alternate" type="text/html" title="Advances in Neural Rendering" /><published>2022-01-01T09:00:00+01:00</published><updated>2022-01-01T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/advancedneuralrenderingstar</id><content type="html" xml:base="https://justusthies.github.io/posts/advancedneuralrenderingstar/"><![CDATA[<p>Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using  differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from  classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline.</p>

<p>This state-of-the-art report on advances in neural rendering  focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects and scene editing and composition. While most of these approaches are scene-specific, we also discuss techniques that generalize across object classes and can be used for generative tasks. In addition to reviewing these state-of-the-art methods, we provide an overview of fundamental concepts and definitions used in the current literature. We conclude with a discussion on open challenges and social implications.</p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="Eurographics" /><summary type="html"><![CDATA[This state-of-the-art report on advances in neural rendering focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene.]]></summary></entry><entry><title type="html">3DV 2021 - Tutorial on the Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrenderingtutorial_3dv/" rel="alternate" type="text/html" title="3DV 2021 - Tutorial on the Advances in Neural Rendering" /><published>2021-11-29T10:00:00+01:00</published><updated>2021-11-29T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/neuralrenderingtutorial_3dv</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrenderingtutorial_3dv/"><![CDATA[<p>Neural rendering is a fast evolving field at the intersection of computer graphics and computer vision. In this tutorial, we will talk about the advances in neural rendering, especially the underlying 2D and 3D representations that allow for novel viewpoint synthesis, controllability and editability. Specifically, we will discuss neural rendering methods based on 2D GANs, techniques using 3D Neural Radiance Fields or learnable sphere proxies. Besides methods that handle static content, we will talk about dynamic content as well.</p>

<p><a href="https://3dv2021.surrey.ac.uk/tutorials/#Tutorial2">3DV Tutorial Website</a></p>

<p><a href="https://slideslive.com/38972960/9th-international-conference-on-3d-vision-3011-day-0">SlidesLive</a></p>

<p>The tutorial is based on our two state-of-the-art reports:</p>
<ul>
  <li><a href="https://justusthies.github.io/posts/advancedneuralrenderingstar/">Advanced Neural Rendering</a></li>
  <li><a href="https://justusthies.github.io/posts/neuralrenderingstar/">Neural Rendering</a></li>
</ul>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Michael Zollhöfer&quot;, &quot;website_link&quot;=&gt;&quot;https://zollhoefer.com&quot;}</name></author><category term="tutorial" /><category term="reconstruction" /><category term="services" /><category term="3DV" /><summary type="html"><![CDATA[In this tutorial, we will talk about the advances in neural rendering, especially the underlying 2D and 3D representations that allow for novel viewpoint synthesis, controllability and editability. Specifically, we will discuss neural rendering methods based on 2D GANs, techniques using 3D Neural Radiance Fields or learnable sphere proxies. Besides methods that handle static content, we will talk about dynamic content as well.]]></summary></entry><entry><title type="html">Expert Interview on DeepFakes</title><link href="https://justusthies.github.io/posts/eu_report/" rel="alternate" type="text/html" title="Expert Interview on DeepFakes" /><published>2021-08-10T09:00:00+02:00</published><updated>2021-08-10T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/eu_report</id><content type="html" xml:base="https://justusthies.github.io/posts/eu_report/"><![CDATA[<p>Expert interview for the European Parliamentary Research Service report on ‘Tackling DeepFakes in European Policy’.</p>

<p><a href="https://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2021)690039">Weblink to the official website.</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="tutorial" /><category term="forensics" /><category term="EU-PARLIAMENT" /><summary type="html"><![CDATA[Expert interview for the European Parliamentary Research Service report on 'Tackling DeepFakes in European Policy'.]]></summary></entry><entry><title type="html">SIGGRAPH 2021 - Course on the Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrendering_course_siggraph/" rel="alternate" type="text/html" title="SIGGRAPH 2021 - Course on the Advances in Neural Rendering" /><published>2021-08-08T11:00:00+02:00</published><updated>2021-08-08T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuralrendering_course_siggraph</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrendering_course_siggraph/"><![CDATA[<p>Neural rendering is an emerging class of deep image and video generation approaches that enable control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure.
It combines machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic models of scenes.
This course covers the advances in neural rendering over the last year.
We will first cover the fundamentals of machine learning and computer graphics relevant for neural rendering. 
Next, we will present state of the art techniques for the many important neural rendering methods for applications such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence.
Finally, we will conclude with a discussion on the ethical implications of this technology and open research problems.</p>

<p>##<a href="https://www.neuralrender.com/">Tutorial Website</a>
##<a href="https://www.youtube.com/watch?v=otly9jcZ0Jg&amp;ab_channel=NeuralRendering">Video Part 1</a>
##<a href="https://www.youtube.com/watch?v=aboFl5ozImM&amp;ab_channel=NeuralRendering">Video Part 2</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Ayush Tewari&quot;, &quot;website_link&quot;=&gt;&quot;https://people.mpi-inf.mpg.de/~atewari/&quot;}</name></author><category term="tutorial" /><category term="reconstruction" /><category term="services" /><category term="SIGGRAPH" /><summary type="html"><![CDATA[This course covers the advances in neural rendering over the years 2020-2021.]]></summary></entry></feed>