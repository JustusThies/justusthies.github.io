<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2021-02-13T10:06:40+01:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Neural Capture and Synthesis at the Max Planck Institute for Intelligent Systems</title><link href="https://justusthies.github.io/posts/mpi-neural-capture-and-synthesis/" rel="alternate" type="text/html" title="Neural Capture and Synthesis at the Max Planck Institute for Intelligent Systems" /><published>2021-01-31T10:00:00+01:00</published><updated>2021-01-31T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/mpi-neural-capture-and-synthesis</id><content type="html" xml:base="https://justusthies.github.io/posts/mpi-neural-capture-and-synthesis/">&lt;p&gt; &lt;/p&gt;
&lt;div style=&quot;text-align: justify&quot;&gt;
I am happy to announce that I am joining the Max Planck Institute for Intelligent Systems as a research group leader in April 2021.
My Max Planck Research Group 'Neural Capture and Synthesis' will work at the intersection of computer graphics, computer vision and machine learning.
&lt;/div&gt;

&lt;div style=&quot;text-align: justify&quot;&gt; 
The main theme of my work is to capture and to (re-)synthesize the real world using commodity hardware.
It includes the modeling of the human body, tracking, as well as the reconstruction and interaction with the environment.
The digitization is needed for various applications in AR/VR as well as in movie (post-)production.
Teleconferencing and working in VR is of high interest for many companies ranging from social media platforms to car manufacturer.
It enables the remote interaction in VR, e.g., the inspection of 3D content like CAD models or scans from real objects.
A realistic reproduction of appearances and motions is key for such applications. 
Capturing natural motions and expressions as well as the photorealistic reproduction of images under novel views are challenging.
With the rise of deep learning methods and, especially, neural rendering, we see immense progress to succeed in these challenges.
The goal of my work is to develop methods for AI-based image synthesis of humans, the underlying representation of appearance, geometry and motion to allow for explicit and implicit control over the synthesis process.
My work on 3D reconstruction, tracking and rendering does not focus exclusively on humans but also on the environment and objects we interact with, thus, enabling applications like 3d telepresence or collaborative working in VR.
In both areas, reconstruction and rendering, hybrid approaches that combine novel findings in machine learning with classical computer graphics and computer vision approaches show promising results.
Nevertheless, these methods still suffer from limitations like generalizability, controllability and editability which I will tackle in my ongoing and future work.
&lt;/div&gt;
&lt;p&gt; &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;face_overview.jpg&quot; alt=&quot;Face Projects&quot; /&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;scene_reco.jpg&quot; alt=&quot;Scene Reconstruction Projects&quot; /&gt;  &lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;non-rigid-tracking.jpg&quot; alt=&quot;Neural Non-rigid Tracking Projects&quot; /&gt;  &lt;/p&gt;

&lt;p&gt;On my &lt;a href=&quot;https://justusthies.github.io/publications/&quot;&gt;website&lt;/a&gt; and on my &lt;a href=&quot;https://www.youtube.com/channel/UCwmSTvnV-sjtlIlNvWYC6Ow&quot;&gt;YouTube page&lt;/a&gt; you will find a couple of interesting projects I have been working on in the past:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=p7UIF1Mw5GI&amp;amp;list=PLVOhpVSqYrEYMhjQ0MQ62JZgtRGTW5hPW&amp;amp;ab_channel=JustusThies&quot;&gt;Facial Reenactment / Reconstruction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=ofVgAEb1FiE&amp;amp;list=PLVOhpVSqYrEbIFkFsvdtO8FUkqbmn7cUQ&amp;amp;ab_channel=JustusThies&quot;&gt;Neural Rendering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Kj_P-lHtWkU&amp;amp;list=PLVOhpVSqYrEYqHZIEfY4R6Lgp90LJFUd2&amp;amp;ab_channel=JustusThies&quot;&gt;Neural Non-rigid Tracking&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=52xlRn0ESek&amp;amp;list=PLVOhpVSqYrEZ6D3FFabhQolw5ZRCNVfiI&amp;amp;ab_channel=MatthiasNiessner&quot;&gt;3D Reconstruction / Texturing&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=Tle7YaPkO_k&amp;amp;list=PLVOhpVSqYrEaZUh3ZktMfXwWuiQTMvuV8&amp;amp;ab_channel=MatthiasNiessner&quot;&gt;Multi-media Forensics&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;br /&gt;&lt;/p&gt;

&lt;hr /&gt;

&lt;h1 id=&quot;openings&quot;&gt;Openings&lt;/h1&gt;
&lt;div style=&quot;text-align: justify&quot;&gt; 
My group as several open positions for Postdocs and PhD students.
If you are interested and highly motivated to work in the above mentioned research fields, please write me an email (&lt;a href=&quot;mailto:justus.thies@tum.de&quot;&gt;justus.thies@tum.de&lt;/a&gt;)
including the relevant application documents (CV, recommendation letters, transcript of records, etc.) and a short description of your research interests / plans (research statement).
&lt;/div&gt;

&lt;h3 id=&quot;qualification&quot;&gt;Qualification&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Applicants for a PhD must hold a Master’s degree in computer science or equivalent&lt;/li&gt;
  &lt;li&gt;Fluent written and spoken English skills are essential&lt;/li&gt;
  &lt;li&gt;Proficient coding skills (Python, C++ (this is critical and will be tested))&lt;/li&gt;
  &lt;li&gt;Experience with deep learning frameworks (TensorFlow / PyTorch)&lt;/li&gt;
  &lt;li&gt;Excitement, self-motivation, and commitment to revolutionize the field&lt;/li&gt;
  &lt;li&gt;Post Doc Applicants Only: academic track record with publications at top-tier venues in computer vision,
graphics, or machine learning (CVPR, ECCV/ICCV, Siggraph, Siggraph Asia, NeurIPS, ICML, ICLR)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;how-to-apply&quot;&gt;How to Apply&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Documents required: CV, research statement, BA and MA transcripts, MA thesis [optional]&lt;/li&gt;
  &lt;li&gt;Ask two recommenders who know your work to directly email recommendation letters&lt;/li&gt;
  &lt;li&gt;Send all documents (incl. rec. letters) directly to Justus Thies (&lt;a href=&quot;mailto:justus.thies@tum.de&quot;&gt;justus.thies@tum.de&lt;/a&gt;)&lt;/li&gt;
  &lt;li&gt;Please understand that we cannot review incomplete applications&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;additional-doctoral-programs&quot;&gt;Additional doctoral programs&lt;/h3&gt;

&lt;p&gt;The MPI for Intelligent System also offers the following doctoral programs you can apply for:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;div style=&quot;text-align: justify&quot;&gt; The &lt;a href=&quot;https://imprs.is.mpg.de/&quot;&gt;International Max Planck Research School for Intelligent Systems&lt;/a&gt; (IMPRS-IS) brings together the MPI for Intelligent Systems with the University of Stuttgart and the University of Tübingen to form a highly visible and unique graduate school of internationally recognized faculty, working at the leading edge of the field.&lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;div style=&quot;text-align: justify&quot;&gt; The &lt;a href=&quot;https://learning-systems.org/&quot;&gt;Max Planck ETH Center for Learning Systems&lt;/a&gt; (CLS) is a doctoral training program. Ph.D. students are jointly supervised by ETH Professors and MPI Directors and Group Leaders, and obtain their doctoral degree from ETH Zurich.&lt;/div&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="MPIIS" /><summary type="html">  I am happy to announce that I am joining the Max Planck Institute for Intelligent Systems as a research group leader in April 2021. My Max Planck Research Group 'Neural Capture and Synthesis' will work at the intersection of computer graphics, computer vision and machine learning.</summary></entry><entry><title type="html">NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction</title><link href="https://justusthies.github.io/posts/nerface/" rel="alternate" type="text/html" title="NerFACE&amp;#58; Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction" /><published>2020-12-08T08:00:00+01:00</published><updated>2020-12-08T08:00:00+01:00</updated><id>https://justusthies.github.io/posts/nerface</id><content type="html" xml:base="https://justusthies.github.io/posts/nerface/">&lt;p&gt;We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face.
Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications.
Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoint or head-poses is required.
In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks.
To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions.
We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup.
In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gafniguy.github.io/4D-Facial-Avatars&quot;&gt;Project Page&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Guy Gafni&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/guy_gafni/profile.html&quot;}</name></author><category term="ArXiv" /><summary type="html">We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoint or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.</summary></entry><entry><title type="html">Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction</title><link href="https://justusthies.github.io/posts/neuraldeformationgraphs/" rel="alternate" type="text/html" title="Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction" /><published>2020-12-08T07:00:00+01:00</published><updated>2020-12-08T07:00:00+01:00</updated><id>https://justusthies.github.io/posts/neuraldeformationgraphs</id><content type="html" xml:base="https://justusthies.github.io/posts/neuraldeformationgraphs/">&lt;p&gt;We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. 
Specifically, we implicitly model a deformation graph via a deep neural network.
This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking.
Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object.
Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints,  the underlying network is trained in a self-supervised fashion.
We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation.
Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings.
Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="ArXiv" /><summary type="html">We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.</summary></entry><entry><title type="html">ID-Reveal: Identity-aware DeepFake Video Detection</title><link href="https://justusthies.github.io/posts/idreveal/" rel="alternate" type="text/html" title="ID-Reveal&amp;#58; Identity-aware DeepFake Video Detection" /><published>2020-12-08T06:00:00+01:00</published><updated>2020-12-08T06:00:00+01:00</updated><id>https://justusthies.github.io/posts/idreveal</id><content type="html" xml:base="https://justusthies.github.io/posts/idreveal/">&lt;p&gt;State-of-the-art DeepFake forgery detectors are trained in a supervised fashion to answer the question ‘is this video real or fake?’.
Given that their training is typically method-specific, these approaches show poor generalization across different types of facial manipulations, e.g., face swapping or facial reenactment.
In this work, we look at the problem from a different perspective by focusing on the facial characteristics of a specific identity; i.e., we want to answer the question ‘Is this the person who is claimed to be?’.
To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how each person moves while talking, by means of metric learning coupled with an adversarial training strategy. 
ur method is independent of the specific type of manipulation since it is trained only on real videos. Moreover, relying on high-level semantic features, it is robust to widespread and disruptive forms of post-processing.
We performed a thorough experimental analysis on several publicly available benchmarks, such as FaceForensics++, Google’s DFD, and Celeb-DF.
Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks.
In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/grip-unina/id-reveal&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Davide Cozzolino&quot;, &quot;website_link&quot;=&gt;&quot;http://www.grip.unina.it/people/userprofile/davide_cozzolino.html&quot;}</name></author><category term="ArXiv" /><summary type="html">State-of-the-art DeepFake forgery detectors are trained in a supervised fashion to answer the question ‘is this video real or fake?’. Given that their training is typically method-specific, these approaches show poor generalization across different types of facial manipulations, e.g., face swapping or facial reenactment. In this work, we look at the problem from a different perspective by focusing on the facial characteristics of a specific identity; i.e., we want to answer the question ‘Is this the person who is claimed to be?’. To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how each person moves while talking, by means of metric learning coupled with an adversarial training strategy. ur method is independent of the specific type of manipulation since it is trained only on real videos. Moreover, relying on high-level semantic features, it is robust to widespread and disruptive forms of post-processing. We performed a thorough experimental analysis on several publicly available benchmarks, such as FaceForensics++, Google’s DFD, and Celeb-DF. Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks. In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.</summary></entry><entry><title type="html">BIDT: Echt oder?</title><link href="https://justusthies.github.io/posts/bidt/" rel="alternate" type="text/html" title="BIDT&amp;#58; Echt oder?" /><published>2020-10-20T18:00:00+02:00</published><updated>2020-10-20T18:00:00+02:00</updated><id>https://justusthies.github.io/posts/bidt</id><content type="html" xml:base="https://justusthies.github.io/posts/bidt/">&lt;h2 id=&quot;was-ist-ein-deepfake&quot;&gt;Was ist ein DeepFake?&lt;/h2&gt;
&lt;p&gt;Die Bezeichnung DeepFake wird umgangssprachlich für jegliche Art der digitalen Bild- und Videomanipulation genutzt. Der Begriff DeepFake wurde vor allem von der digitalen Manipulation von Portraitbildern geprägt. Da neuere Methoden auf maschinellem Lernen, insbesondere dem sog. Deep Learning, basieren, um photorealistische Inhalte zu erzeugen, kam es zu der Wortneuschöpfung Deep-Fake. DeepFakes sind bereits weit verbreitet und entsprechende Programme werden von einer Vielzahl von Nutzern verwendet. Einige „Bildfilter“ in Apps wie Instagram, die das Aussehen einer Person verändern, können als DeepFakes aufgefasst werden. Insbesondere Face Swapping, also das Vertauschen von zwei Gesichtern, wird als DeepFake bezeichnet. Solche Manipulationen erzeugen virtuelle Personen, die so nicht existieren (das Gesicht passt nicht zum Rest des Körpers). Jedoch gibt es auch Techniken, die die Identität einer Person erhalten und nur die Gesichtszüge verändern - man spricht hierbei vom Facial Reenactment. D.h. der Gesichtsausdruck kann von einer Person auf eine andere übertragen werden.&lt;/p&gt;

&lt;h2 id=&quot;inwieweit-können-deepfakes-zu-einer-gefahr-für-die-gesellschaft-werden&quot;&gt;Inwieweit können DeepFakes zu einer Gefahr für die Gesellschaft werden?&lt;/h2&gt;
&lt;p&gt;DeepFakes sind zurzeit bereits eine Gefahr für einzelne Personen. Cyber-Mobbing (also digitales Mobbing) durch manipulierte Bilder und sogenannte FakePorns (Gesicht einer Person wird in ein Pornovideo eingefügt) werden bereits zur Diskreditierung von Personen erstellt.
Für die Gesellschaft im Allgemeinen ergeben sich auch gewisse Gefahren. Allein die Frage ob ein Inhalt gefälscht wurde oder nicht, hat einen Vertrauensverlust als Folge. Politiker sähen bereits Zweifel an diversen Berichterstattungen mit dem Hinweis auf fake news (siehe USA). Diese Entwicklungen sind besorgniserregend, da es für die Menschen zunehmend schwieriger wird, zu erkennen was der Realität entspricht und was nicht.&lt;/p&gt;

&lt;h2 id=&quot;welche-anwendungsfälle-gibt-es-für-solche-digital-erzeugten-medien&quot;&gt;Welche Anwendungsfälle gibt es für solche digital erzeugten Medien?&lt;/h2&gt;
&lt;p&gt;Die grundlegenden Techniken mit denen DeepFakes erstellt werden, haben diverse Anwendungsfälle. Digitale Inhalte sind omnipräsent, augmented reality (angereicherte Realität) und virtual reality (virtuelle Realität) kurz AR bzw. VR finden immer weitere Anwendungsfelder. Firmen arbeiten mit Hochdruck daran Telekonferenzsysteme weiterzuentwickeln - anstatt einfache 2D Videos ist es das Ziel 3D Inhalte zu erzeugen. 3D Inhalte haben den Vorteil, dass der Nutzer entscheiden kann, wo er hinschauen möchte - Telekonferenzen in 3D werden möglich. Die grundlegenden Algorithmen, um dies zu erreichen, basieren auch auf den Techniken und Methoden, die zu DeepFakes benutzt werden. Es geht darum photorealistische Abbilder von uns Menschen zu erzeugen (einen digitalen Double). Virtuelle Spiegel, um Makeup zu testen, eine virtuelle Anprobe von Kleidung, das Simulieren von chirurgischen Operationen, all dies wird durch einen digitalen Double möglich.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://padlet.com/bidt_Dialog/werkstattdigital_Deepfakes&quot;&gt;Padlet website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="BIDT" /><summary type="html">Was ist ein DeepFake? Die Bezeichnung DeepFake wird umgangssprachlich für jegliche Art der digitalen Bild- und Videomanipulation genutzt. Der Begriff DeepFake wurde vor allem von der digitalen Manipulation von Portraitbildern geprägt. Da neuere Methoden auf maschinellem Lernen, insbesondere dem sog. Deep Learning, basieren, um photorealistische Inhalte zu erzeugen, kam es zu der Wortneuschöpfung Deep-Fake. DeepFakes sind bereits weit verbreitet und entsprechende Programme werden von einer Vielzahl von Nutzern verwendet. Einige „Bildfilter“ in Apps wie Instagram, die das Aussehen einer Person verändern, können als DeepFakes aufgefasst werden. Insbesondere Face Swapping, also das Vertauschen von zwei Gesichtern, wird als DeepFake bezeichnet. Solche Manipulationen erzeugen virtuelle Personen, die so nicht existieren (das Gesicht passt nicht zum Rest des Körpers). Jedoch gibt es auch Techniken, die die Identität einer Person erhalten und nur die Gesichtszüge verändern - man spricht hierbei vom Facial Reenactment. D.h. der Gesichtsausdruck kann von einer Person auf eine andere übertragen werden.</summary></entry><entry><title type="html">Neural Non-Rigid Tracking</title><link href="https://justusthies.github.io/posts/neuraltracking/" rel="alternate" type="text/html" title="Neural Non-Rigid Tracking" /><published>2020-09-29T11:00:00+02:00</published><updated>2020-09-29T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuraltracking</id><content type="html" xml:base="https://justusthies.github.io/posts/neuraltracking/">&lt;p&gt;We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction.
Given two input RGB-D frames of a non-rigidly moving object, we employ a convolutional neural network to predict dense correspondences.
These correspondences are used as constraints in an as-rigid-as-possible (ARAP) optimization problem.
By enabling gradient back-propagation through the non-rigid optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking.
Furthermore, this formulation allows for learning correspondence weights in a self-supervised manner.
Thus, outliers and wrong correspondences are down-weighted to enable robust tracking.
Compared to state-of-the-art approaches, our algorithm shows improved reconstruction performance, while simultaneously achieving 85x faster correspondence prediction than comparable deep-learning based methods.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="NeurIPS" /><summary type="html">We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction. Given two input RGB-D frames of a non-rigidly moving object, we employ a convolutional neural network to predict dense correspondences. These correspondences are used as constraints in an as-rigid-as-possible (ARAP) optimization problem. By enabling gradient back-propagation through the non-rigid optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking. Furthermore, this formulation allows for learning correspondence weights in a self-supervised manner. Thus, outliers and wrong correspondences are down-weighted to enable robust tracking. Compared to state-of-the-art approaches, our algorithm shows improved reconstruction performance, while simultaneously achieving 85x faster correspondence prediction than comparable deep-learning based methods.</summary></entry><entry><title type="html">Egocentric Videoconferencing</title><link href="https://justusthies.github.io/posts/egochat/" rel="alternate" type="text/html" title="Egocentric Videoconferencing" /><published>2020-09-28T11:00:00+02:00</published><updated>2020-09-28T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/egochat</id><content type="html" xml:base="https://justusthies.github.io/posts/egochat/">&lt;p&gt;We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices.
Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera.
Using a frontal camera in a hands-free setting when a person is on the move is impractical.
Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. 
To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses.
Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video.
To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. 
Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods.
We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. 
To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face.
This allows us to synthesis results at different head poses. 
Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator.
We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.&lt;/p&gt;

&lt;video width=&quot;800&quot; height=&quot;450&quot; controls=&quot;&quot;&gt;
	&lt;source src=&quot;http://gvv.mpi-inf.mpg.de/projects/EgoChat/data/Main.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://gvv.mpi-inf.mpg.de/projects/EgoChat/&quot;&gt;Project Page&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Mohamed Elgharib&quot;, &quot;website_link&quot;=&gt;&quot;http://people.mpi-inf.mpg.de/~elgharib/&quot;}</name></author><category term="SIGGRAPH ASIA" /><summary type="html">We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices. Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera. Using a frontal camera in a hands-free setting when a person is on the move is impractical. Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses. Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video. To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods. We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face. This allows us to synthesis results at different head poses. Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator. We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.</summary></entry><entry><title type="html">WIPO: Artifical Intelligence and Intellectual Property</title><link href="https://justusthies.github.io/posts/wipo/" rel="alternate" type="text/html" title="WIPO&amp;#58; Artifical Intelligence and Intellectual Property" /><published>2020-09-18T09:00:00+02:00</published><updated>2020-09-18T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/wipo</id><content type="html" xml:base="https://justusthies.github.io/posts/wipo/">&lt;p&gt;This virtual exhibition presents current approaches of AI-driven media creation.
It raises interesting questions regarding intellectual property.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.wipo.int/about-ip/en/artificial_intelligence/&quot;&gt;Weblink to the official website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="WIPO" /><summary type="html">This virtual exhibition presents current approaches of AI-driven media creation. It raises interesting questions regarding intellectual property.</summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21/" rel="alternate" type="text/html" title="Lecture&amp;#58; 3D Scanning and Motion Capture" /><published>2020-08-24T23:00:00+02:00</published><updated>2020-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic concepts of geometry
    &lt;ul&gt;
      &lt;li&gt;Meshes (polygonal), Point Clouds, Pixels &amp;amp; Voxels&lt;/li&gt;
      &lt;li&gt;RGB and Depth Cameras&lt;/li&gt;
      &lt;li&gt;Extrinsics and Intrinsics&lt;/li&gt;
      &lt;li&gt;Capture devices&lt;/li&gt;
      &lt;li&gt;RGB and Multi-view&lt;/li&gt;
      &lt;li&gt;RGB-D cameras&lt;/li&gt;
      &lt;li&gt;Stereo&lt;/li&gt;
      &lt;li&gt;Time of Flight (ToF)&lt;/li&gt;
      &lt;li&gt;Structured Light&lt;/li&gt;
      &lt;li&gt;Laser Scanner, Lidar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Surface Representations
    &lt;ul&gt;
      &lt;li&gt;Polygonal meshes (trimeshes, etc.)&lt;/li&gt;
      &lt;li&gt;Parametric surfaces: splines, nurbs&lt;/li&gt;
      &lt;li&gt;Implicit surfaces&lt;/li&gt;
      &lt;li&gt;Ridge-based surfaces&lt;/li&gt;
      &lt;li&gt;Radial basis functions&lt;/li&gt;
      &lt;li&gt;Signed distance functions (volumetric, Curless &amp;amp; Levoy)&lt;/li&gt;
      &lt;li&gt;Indicator function (Poisson Surface Reconstruction)&lt;/li&gt;
      &lt;li&gt;More general: level sets&lt;/li&gt;
      &lt;li&gt;Marching cubes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;High-level overview of reconstructions
    &lt;ul&gt;
      &lt;li&gt;Structure from Motion (SfM)&lt;/li&gt;
      &lt;li&gt;Multi-view Stereo (MVS)&lt;/li&gt;
      &lt;li&gt;SLAM&lt;/li&gt;
      &lt;li&gt;Bundle Adjustment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimization
    &lt;ul&gt;
      &lt;li&gt;Non-linear least squares&lt;/li&gt;
      &lt;li&gt;Gauss-Newton LM&lt;/li&gt;
      &lt;li&gt;Examples in Ceres&lt;/li&gt;
      &lt;li&gt;Symbolic diff vs auto-diff&lt;/li&gt;
      &lt;li&gt;Auto-diff with dual numbers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Pose alignment&lt;/li&gt;
      &lt;li&gt;ICP (point cloud alignment; depth-to-model alignment; rigid ‘fitting’)&lt;/li&gt;
      &lt;li&gt;Online surface reconstruction pipeline: KinectFusion&lt;/li&gt;
      &lt;li&gt;Scalable surface representations: VoxelHashing, OctTrees&lt;/li&gt;
      &lt;li&gt;Loop closures and global optimization&lt;/li&gt;
      &lt;li&gt;Robust optimization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Non-rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Surface deformation for modeling&lt;/li&gt;
      &lt;li&gt;Regularizers: ARAP, ED, etc.&lt;/li&gt;
      &lt;li&gt;Non-rigid surface fitting: e.g., non-rigid ICP&lt;/li&gt;
      &lt;li&gt;Non-rigid reconstruction: DynamicFusion/VolumeDeform/KillingFusion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Body Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Skeleton Tracking and Inverse Kinematics&lt;/li&gt;
      &lt;li&gt;Learning-based approaches from RGB and RGB-D&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Face Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Keypoint detection &amp;amp; tracking&lt;/li&gt;
      &lt;li&gt;Parametric / Statistical Models -&amp;gt; BlendShapes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hand Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Parametric Models&lt;/li&gt;
      &lt;li&gt;Some DeepLearning-based things&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminative vs generative tracking
    &lt;ul&gt;
      &lt;li&gt;Random forests&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Motion Capture in Movies
    &lt;ul&gt;
      &lt;li&gt;Marker-based motion capture&lt;/li&gt;
      &lt;li&gt;LightStage -&amp;gt; movies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BRDF and Material Capture&lt;/li&gt;
  &lt;li&gt;Open research questions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).</summary></entry><entry><title type="html">Seminar: 3D Vision</title><link href="https://justusthies.github.io/posts/3D-Vision-WS20-21/" rel="alternate" type="text/html" title="Seminar&amp;#58; 3D Vision" /><published>2020-08-24T23:00:00+02:00</published><updated>2020-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Vision-WS20-21</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Vision-WS20-21/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).&lt;/p&gt;

&lt;p&gt;The talks have to cover the following topics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;face tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;facial reenactment (audio driven, video driven animation)&lt;/li&gt;
  &lt;li&gt;body tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;hand tracking&lt;/li&gt;
  &lt;li&gt;static / dynamic scene reconstruction&lt;/li&gt;
  &lt;li&gt;scene understanding&lt;/li&gt;
  &lt;li&gt;inverse rendering, material estimation, light estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;organization--requirements&quot;&gt;Organization / Requirements&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Lecture: 3D Scanning &amp;amp; Motion Capturing&lt;/li&gt;
  &lt;li&gt;The participants have to present their topics in a talk (in English), which should last 30 minutes.&lt;/li&gt;
  &lt;li&gt;The semi-final slides (PDF) should be sent one week before the talk; otherwise, the talk will be canceled.&lt;/li&gt;
  &lt;li&gt;A short report (approximately 2-3 pages in the ACM SIGGRAPH TOG format (acmtog)) should be prepared and sent within two weeks after the talk. When you send the report, please send the final slides (PDF) together.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).</summary></entry></feed>