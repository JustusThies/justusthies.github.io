<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2022-03-23T21:21:51+01:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Neural Head Avatars from Monocular RGB Videos</title><link href="https://justusthies.github.io/posts/neuralhead/" rel="alternate" type="text/html" title="Neural Head Avatars from Monocular RGB Videos" /><published>2022-03-22T08:30:00+01:00</published><updated>2022-03-22T08:30:00+01:00</updated><id>https://justusthies.github.io/posts/neuralhead</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralhead/">&lt;p&gt;We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be used for teleconferencing in AR/VR or other applications in the movie or games industry that rely on a digital human. Our representation can be learned from a monocular RGB portrait video that features a range of different expressions and views. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture. We demonstrate that this representation is able to accurately extrapolate to unseen poses and view points, and generates natural expressions while providing sharp texture details. Compared to previous works on head avatars, our method provides a disentangled shape and appearance model of the complete human head (including hair) that is compatible with the standard graphics pipeline. Moreover, it quantitatively and qualitatively outperforms current state of the art in terms of reconstruction quality and novel-view synthesis.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Philip-William Grassal&quot;, &quot;website_link&quot;=&gt;&quot;https://hci.iwr.uni-heidelberg.de/vislearn/people&quot;}</name></author><category term="CVPR" /><summary type="html">We present Neural Head Avatars, a novel neural representation that explicitly models the surface geometry and appearance of an animatable human avatar that can be used for teleconferencing in AR/VR or other applications in the movie or games industry that rely on a digital human. Our representation can be learned from a monocular RGB portrait video that features a range of different expressions and views. Specifically, we propose a hybrid representation consisting of a morphable model for the coarse shape and expressions of the face, and two feed-forward networks, predicting vertex offsets of the underlying mesh as well as a view- and expression-dependent texture. We demonstrate that this representation is able to accurately extrapolate to unseen poses and view points, and generates natural expressions while providing sharp texture details. Compared to previous works on head avatars, our method provides a disentangled shape and appearance model of the complete human head (including hair) that is compatible with the standard graphics pipeline. Moreover, it quantitatively and qualitatively outperforms current state of the art in terms of reconstruction quality and novel-view synthesis.</summary></entry><entry><title type="html">Neural RGB-D Surface Reconstruction</title><link href="https://justusthies.github.io/posts/rgbdnerf/" rel="alternate" type="text/html" title="Neural RGB-D Surface Reconstruction" /><published>2022-03-22T07:30:00+01:00</published><updated>2022-03-22T07:30:00+01:00</updated><id>https://justusthies.github.io/posts/rgbdnerf</id><content type="html" xml:base="https://justusthies.github.io/posts/rgbdnerf/">&lt;p&gt;In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Dejan Azinovic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/dejan_azinovic/profile.html&quot;}</name></author><category term="CVPR" /><summary type="html">In this work, we explore how to leverage the success of implicit novel view synthesis methods for surface reconstruction. Methods which learn a neural radiance field have shown amazing image synthesis results, but the underlying geometry representation is only a coarse approximation of the real geometry. We demonstrate how depth measurements can be incorporated into the radiance field formulation to produce more detailed and complete reconstruction results than using methods based on either color or depth data alone. In contrast to a density field as the underlying geometry representation, we propose to learn a deep neural network which stores a truncated signed distance field. Using this representation, we show that one can still leverage differentiable volume rendering to estimate color values of the observed images during training to compute a reconstruction loss. This is beneficial for learning the signed distance field in regions with missing depth measurements. Furthermore, we correct for misalignment errors of the camera, improving the overall reconstruction quality. In several experiments, we show-cast our method and compare to existing works on classical RGB-D fusion and learned representations.</summary></entry><entry><title type="html">Mover: Human-Aware Object Placement for Visual Environment Reconstruction</title><link href="https://justusthies.github.io/posts/mover/" rel="alternate" type="text/html" title="Mover&amp;#58; Human-Aware Object Placement for Visual Environment Reconstruction" /><published>2022-03-22T07:30:00+01:00</published><updated>2022-03-22T07:30:00+01:00</updated><id>https://justusthies.github.io/posts/mover</id><content type="html" xml:base="https://justusthies.github.io/posts/mover/">&lt;p&gt;Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects, (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Hongwei Yi&quot;, &quot;website_link&quot;=&gt;&quot;https://xyyhw.top/&quot;}</name></author><category term="CVPR" /><summary type="html">Humans are in constant contact with the world as they move through it and interact with it. This contact is a vital source of information for understanding 3D humans, 3D scenes, and the interactions between them. In fact, we demonstrate that these human-scene interactions (HSIs) can be leveraged to improve the 3D reconstruction of a scene from a monocular RGB video. Our key idea is that, as a person moves through a scene and interacts with it, we accumulate HSIs across multiple input images, and optimize the 3D scene to reconstruct a consistent, physically plausible and functional 3D scene layout. Our optimization-based approach exploits three types of HSI constraints: (1) humans that move in a scene are occluded or occlude objects, thus, defining the depth ordering of the objects, (2) humans move through free space and do not interpenetrate objects, (3) when humans and objects are in contact, the contact surfaces occupy the same place in space. Using these constraints in an optimization formulation across all observations, we significantly improve the 3D scene layout reconstruction. Furthermore, we show that our scene reconstruction can be used to refine the initial 3D human pose and shape (HPS) estimation. We evaluate the 3D scene layout reconstruction and HPS estimation qualitatively and quantitatively using the PROX and PiGraphs datasets.</summary></entry><entry><title type="html">Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/advancedneuralrenderingstar/" rel="alternate" type="text/html" title="Advances in Neural Rendering" /><published>2022-01-01T09:00:00+01:00</published><updated>2022-01-01T09:00:00+01:00</updated><id>https://justusthies.github.io/posts/advancedneuralrenderingstar</id><content type="html" xml:base="https://justusthies.github.io/posts/advancedneuralrenderingstar/">&lt;p&gt;Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using  differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from  classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline.&lt;/p&gt;

&lt;p&gt;This state-of-the-art report on advances in neural rendering  focuses on methods that combine classical rendering principles with learned 3D scene representations, often now referred to as neural scene representations. A key advantage of these methods is that they are 3D-consistent by design, enabling applications such as novel viewpoint synthesis of a captured scene. In addition to methods that handle static scenes, we cover neural scene representations for modeling non-rigidly deforming objects and scene editing and composition. While most of these approaches are scene-specific, we also discuss techniques that generalize across object classes and can be used for generative tasks. In addition to reviewing these state-of-the-art methods, we provide an overview of fundamental concepts and definitions used in the current literature. We conclude with a discussion on open challenges and social implications.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="Eurographics" /><summary type="html">Synthesizing photo-realistic images and videos is at the heart of computer graphics and has been the focus of decades of research. Traditionally, synthetic images of a scene are generated using rendering algorithms such as rasterization or ray tracing, which take specifically defined representations of geometry and material properties as input. Collectively, these inputs define the actual scene and what is rendered, and are referred to as the scene representation (where a scene consists of one or more objects). Example scene representations are triangle meshes with accompanied textures (e.g., created by an artist), point clouds (e.g., from a depth sensor), volumetric grids (e.g., from a CT scan), or implicit surface functions (e.g., truncated signed distance fields). The reconstruction of such a scene representation from observations using differentiable rendering losses is known as inverse graphics or inverse rendering. Neural rendering is closely related, and combines ideas from classical computer graphics and machine learning to create algorithms for synthesizing images from real-world observations. Neural rendering is a leap forward towards the goal of synthesizing photo-realistic image and video content. In recent years, we have seen immense progress in this field through hundreds of publications that show different ways to inject learnable components into the rendering pipeline.</summary></entry><entry><title type="html">3DV 2021 - Tutorial on the Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrenderingtutorial_3dv/" rel="alternate" type="text/html" title="3DV 2021 - Tutorial on the Advances in Neural Rendering" /><published>2021-11-29T10:00:00+01:00</published><updated>2021-11-29T10:00:00+01:00</updated><id>https://justusthies.github.io/posts/neuralrenderingtutorial_3dv</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrenderingtutorial_3dv/">&lt;p&gt;Neural rendering is a fast evolving field at the intersection of computer graphics and computer vision. In this tutorial, we will talk about the advances in neural rendering, especially the underlying 2D and 3D representations that allow for novel viewpoint synthesis, controllability and editability. Specifically, we will discuss neural rendering methods based on 2D GANs, techniques using 3D Neural Radiance Fields or learnable sphere proxies. Besides methods that handle static content, we will talk about dynamic content as well.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://3dv2021.surrey.ac.uk/tutorials/#Tutorial2&quot;&gt;3DV Tutorial Website&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://slideslive.com/38972960/9th-international-conference-on-3d-vision-3011-day-0&quot;&gt;SlidesLive&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;The tutorial is based on our two state-of-the-art reports:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://justusthies.github.io/posts/advancedneuralrenderingstar/&quot;&gt;Advanced Neural Rendering&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://justusthies.github.io/posts/neuralrenderingstar/&quot;&gt;Neural Rendering&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Michael Zollh\u00F6fer&quot;, &quot;website_link&quot;=&gt;&quot;https://zollhoefer.com&quot;}</name></author><category term="3DV" /><summary type="html">Neural rendering is a fast evolving field at the intersection of computer graphics and computer vision. In this tutorial, we will talk about the advances in neural rendering, especially the underlying 2D and 3D representations that allow for novel viewpoint synthesis, controllability and editability. Specifically, we will discuss neural rendering methods based on 2D GANs, techniques using 3D Neural Radiance Fields or learnable sphere proxies. Besides methods that handle static content, we will talk about dynamic content as well.</summary></entry><entry><title type="html">Expert Interview on DeepFakes</title><link href="https://justusthies.github.io/posts/eu_report/" rel="alternate" type="text/html" title="Expert Interview on DeepFakes" /><published>2021-08-10T09:00:00+02:00</published><updated>2021-08-10T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/eu_report</id><content type="html" xml:base="https://justusthies.github.io/posts/eu_report/">&lt;p&gt;Expert interview for the European Parliamentary Research Service report on ‘Tackling DeepFakes in European Policy’.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.europarl.europa.eu/thinktank/en/document.html?reference=EPRS_STU(2021)690039&quot;&gt;Weblink to the official website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="EU-PARLIAMENT" /><summary type="html">Expert interview for the European Parliamentary Research Service report on ‘Tackling DeepFakes in European Policy’.</summary></entry><entry><title type="html">SIGGRAPH 2021 - Course on the Advances in Neural Rendering</title><link href="https://justusthies.github.io/posts/neuralrendering_course_siggraph/" rel="alternate" type="text/html" title="SIGGRAPH 2021 - Course on the Advances in Neural Rendering" /><published>2021-08-08T11:00:00+02:00</published><updated>2021-08-08T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuralrendering_course_siggraph</id><content type="html" xml:base="https://justusthies.github.io/posts/neuralrendering_course_siggraph/">&lt;p&gt;Neural rendering is an emerging class of deep image and video generation approaches that enable control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure.
It combines machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic models of scenes.
This course covers the advances in neural rendering over the last year.
We will first cover the fundamentals of machine learning and computer graphics relevant for neural rendering. 
Next, we will present state of the art techniques for the many important neural rendering methods for applications such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence.
Finally, we will conclude with a discussion on the ethical implications of this technology and open research problems.&lt;/p&gt;

&lt;p&gt;##&lt;a href=&quot;https://www.neuralrender.com/&quot;&gt;Tutorial Website&lt;/a&gt;
##&lt;a href=&quot;https://www.youtube.com/watch?v=otly9jcZ0Jg&amp;amp;ab_channel=NeuralRendering&quot;&gt;Video Part 1&lt;/a&gt;
##&lt;a href=&quot;https://www.youtube.com/watch?v=aboFl5ozImM&amp;amp;ab_channel=NeuralRendering&quot;&gt;Video Part 2&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Ayush Tewari&quot;, &quot;website_link&quot;=&gt;&quot;https://people.mpi-inf.mpg.de/~atewari/&quot;}</name></author><category term="SIGGRAPH" /><summary type="html">Neural rendering is an emerging class of deep image and video generation approaches that enable control of scene properties such as illumination, camera parameters, pose, geometry, appearance, and semantic structure. It combines machine learning techniques with physical knowledge from computer graphics to obtain controllable and photo-realistic models of scenes. This course covers the advances in neural rendering over the last year. We will first cover the fundamentals of machine learning and computer graphics relevant for neural rendering. Next, we will present state of the art techniques for the many important neural rendering methods for applications such as novel view synthesis, semantic photo manipulation, facial and body reenactment, relighting, free-viewpoint video, and the creation of photo-realistic avatars for virtual and augmented reality telepresence. Finally, we will conclude with a discussion on the ethical implications of this technology and open research problems.</summary></entry><entry><title type="html">TransformerFusion: Monocular RGB Scene Reconstruction using Transformers</title><link href="https://justusthies.github.io/posts/transfusion/" rel="alternate" type="text/html" title="TransformerFusion&amp;#58; Monocular RGB Scene Reconstruction using Transformers" /><published>2021-07-12T09:30:00+02:00</published><updated>2021-07-12T09:30:00+02:00</updated><id>https://justusthies.github.io/posts/transfusion</id><content type="html" xml:base="https://justusthies.github.io/posts/transfusion/">&lt;p&gt;We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="NeurIPS 2021" /><summary type="html">We introduce TransformerFusion, a transformer-based 3D scene reconstruction approach. From an input monocular RGB video, the video frames are processed by a transformer network that fuses the observations into a volumetric feature grid representing the scene; this feature grid is then decoded into an implicit 3D scene representation. Key to our approach is the transformer architecture that enables the network to learn to attend to the most relevant image frames for each 3D location in the scene, supervised only by the scene reconstruction task. Features are fused in a coarse-to-fine fashion, storing fine-level features only where needed, requiring lower memory storage and enabling fusion at interactive rates. The feature grid is then decoded to a higher-resolution scene reconstruction, using an MLP-based surface occupancy prediction from interpolated coarse-to-fine 3D features. Our approach results in an accurate surface reconstruction, outperforming state-of-the-art multi-view stereo depth estimation methods, fully-convolutional 3D reconstruction approaches, and approaches using LSTM- or GRU-based recurrent networks for video sequence fusion.</summary></entry><entry><title type="html">EG Junior Fellow</title><link href="https://justusthies.github.io/posts/eg_junior_fellow/" rel="alternate" type="text/html" title="EG Junior Fellow" /><published>2021-05-14T11:00:00+02:00</published><updated>2021-05-14T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/eg_junior_fellow</id><content type="html" xml:base="https://justusthies.github.io/posts/eg_junior_fellow/">&lt;p&gt;The Eurographics Junior Fellows have elected me to join the EG Junior Fellows.&lt;/p&gt;

&lt;p&gt;‘Eurographics Junior Fellows are young researchers (PhD degree obtained no more than eight years ago) from all over the world who have already made significant contributions to the field of computer graphics, or to the Eurographics Association.’&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.eg.org/wp/junior-fellows/&quot;&gt;Eurographics Website&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="EG" /><summary type="html">The Eurographics Junior Fellows have elected me to join the EG Junior Fellows.</summary></entry><entry><title type="html">Dynamic Surface Function Networks for Clothed Human Bodies</title><link href="https://justusthies.github.io/posts/dsfn/" rel="alternate" type="text/html" title="Dynamic Surface Function Networks for Clothed Human Bodies" /><published>2021-04-12T10:00:00+02:00</published><updated>2021-04-12T10:00:00+02:00</updated><id>https://justusthies.github.io/posts/dsfn</id><content type="html" xml:base="https://justusthies.github.io/posts/dsfn/">&lt;p&gt;We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Andrei Burov&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/andrei_burov/profile.html&quot;}</name></author><category term="ICCV" /><summary type="html">We present a novel method for temporal coherent reconstruction and tracking of clothed humans. Given a monocular RGB-D sequence, we learn a person-specific body model which is based on a dynamic surface function network. To this end, we explicitly model the surface of the person using a multi-layer perceptron (MLP) which is embedded into the canonical space of the SMPL body model. With classical forward rendering, the represented surface can be rasterized using the topology of a template mesh. For each surface point of the template mesh, the MLP is evaluated to predict the actual surface location. To handle pose-dependent deformations, the MLP is conditioned on the SMPL pose parameters. We show that this surface representation as well as the pose parameters can be learned in a self-supervised fashion using the principle of analysis-by-synthesis and differentiable rasterization. As a result, we are able to reconstruct a temporally coherent mesh sequence from the input data. The underlying surface representation can be used to synthesize new animations of the reconstructed person including pose-dependent deformations.</summary></entry></feed>