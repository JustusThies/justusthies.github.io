<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2024-10-16T17:36:02+02:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">Gaussian-Haircut - Human Hair Reconstruction with Strand-Aligned 3D Gaussians</title><link href="https://justusthies.github.io/posts/gaussianhair/" rel="alternate" type="text/html" title="Gaussian-Haircut - Human Hair Reconstruction with Strand-Aligned 3D Gaussians" /><published>2024-10-01T11:00:00+02:00</published><updated>2024-10-01T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/gaussianhair</id><content type="html" xml:base="https://justusthies.github.io/posts/gaussianhair/"><![CDATA[<p>We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data. In contrast to recent approaches that leverage unstructured Gaussians to model human avatars, our method reconstructs the hair using 3D polylines, or strands. This fundamental difference allows the use of the resulting hairstyles out-of-the-box in modern computer graphics engines for editing, rendering, and simulation. Our 3D lifting method relies on unstructured Gaussians to generate multi-view ground truth data to supervise the fitting of hair strands. The hairstyle itself is represented in the form of the so-called strand-aligned 3D Gaussians. This representation allows us to combine strand-based hair priors, which are essential for realistic modeling of the inner structure of hairstyles, with the differentiable rendering capabilities of 3D Gaussian Splatting. Our method, named Gaussian Haircut, is evaluated on synthetic and real scenes and demonstrates state-of-the-art performance in the task of strand-based hair reconstruction.</p>

<p><a href="https://eth-ait.github.io/GaussianHaircut/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Egor Zhakharov&quot;, &quot;website_link&quot;=&gt;&quot;https://egorzakharov.github.io/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[We introduce a new hair modeling method that uses a dual representation of classical hair strands and 3D Gaussians to produce accurate and realistic strand-based reconstructions from multi-view data.]]></summary></entry><entry><title type="html">German Pattern Recognition Award 2024</title><link href="https://justusthies.github.io/posts/german_pattern_recognition_award/" rel="alternate" type="text/html" title="German Pattern Recognition Award 2024" /><published>2024-10-01T11:00:00+02:00</published><updated>2024-10-01T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/german_pattern_recognition_award</id><content type="html" xml:base="https://justusthies.github.io/posts/german_pattern_recognition_award/"><![CDATA[<p>Since 2011, the DAGM has annually granted the German Pattern Recognition Award for outstanding, internationally visible research in the fields of pattern recognition, computer vision, and machine learning. The German Pattern Recognition Award continues the tradition of the Olympus Award for Pattern Recognition (1992-2010).</p>

<p>Its aim is to promote outstanding young scientists in the fields of pattern recognition, computer vision, and machine learning. Internationally visible researchers in these fields who have not yet passed the age of 35 in the year of the awards presentation can be proposed. An association of the candidates with the DAGM is desirable. The German Pattern Recognition Award is currently donated by Daimler AG and consists of a financial award of 5000 euros.</p>

<p>The German Pattern Recognition Award was granted to Prof. Dr. Justus Thies for his outstanding scientific contributions in the area of Real-Time Face Tracking, Facial Reenactment, AI-driven Rendering as well as Digital Multi-Media Forensics.</p>

<p><a href="https://www.dagm.de/award-winners/german-pattern-recognition-award">DAGM Pattern Recogniton Award Website</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="DAGM" /><summary type="html"><![CDATA[I have been awarded with the DAGM German Pattern Recognition Award 2024.]]></summary></entry><entry><title type="html">Stable Video Portraits</title><link href="https://justusthies.github.io/posts/svp/" rel="alternate" type="text/html" title="Stable Video Portraits" /><published>2024-10-01T01:00:00+02:00</published><updated>2024-10-01T01:00:00+02:00</updated><id>https://justusthies.github.io/posts/svp</id><content type="html" xml:base="https://justusthies.github.io/posts/svp/"><![CDATA[<p>Rapid advances in the field of generative AI and text-to-image methods in particular have transformed the way we interact with and perceive computer-generated imagery today. In parallel, much progress has been made in 3D face reconstruction, using 3D Morphable Models (3DMM). In this paper, we present Stable Video Portraits, a novel hybrid 2D/3D generation method that outputs photorealistic videos of talking faces leveraging a large pre-trained text-to-image prior (2D), controlled via a 3DMM (3D). Specifically, we introduce a person-specific fine-tuning of a general 2D stable diffusion model which we lift to a video model by providing temporal 3DMM sequences as conditioning and by introducing a temporal denoising procedure. As an output, this model generates temporally smooth imagery of a person with 3DMM-based controls, i.e., a person-specific avatar. The facial appearance of this person-specific avatar can be edited and morphed to text-defined celebrities, without any test-time fine-tuning. The method is analyzed quantitatively and qualitatively, and we show that our method outperforms state-of-the-art monocular head avatar methods.</p>

<p><a href="https://svp.is.tue.mpg.de/">Project Page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Mirela Ostrek&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[Stable Video Portraits is a novel hybrid 2D/3D generation method that outputs photorealistic videos of talking faces leveraging a large pre-trained text-to-image prior (2D), controlled via a 3DMM (3D). It is based on a personalized image diffusion prior which allows us to generate new videos of the subject, and also to edit the appearance by blending the personalized image prior with a general text-conditioned model.]]></summary></entry><entry><title type="html">ERC Starting Grant 2024</title><link href="https://justusthies.github.io/posts/erc_starting_grant/" rel="alternate" type="text/html" title="ERC Starting Grant 2024" /><published>2024-09-30T11:00:00+02:00</published><updated>2024-09-30T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/erc_starting_grant</id><content type="html" xml:base="https://justusthies.github.io/posts/erc_starting_grant/"><![CDATA[<p>The European Union uses the Starting Grant to promote outstanding research and, at the same time, early career researchers. The Starting Grant is aimed at researchers who can already demonstrate excellent work and now want to expand their own research or set up their own research group at the start of their careers.</p>

<p>Justus Thies is being funded for his project “Learning Digital Humans in Motion” . His aim is to develop AI-based image processing and graphic tools to create lifelike digital representations of people for the immersive digital world. Both projects will receive a total of around 1.5 million euros each over a period of five years.</p>

<p><a href="https://www.tu-darmstadt.de/universitaet/aktuelles_meldungen/einzelansicht_471040.en.jsp">TUDa News</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="ERC" /><summary type="html"><![CDATA[I have been awarded with an ERC Starting Grant 2024. We will work on learning digital humans in motion.]]></summary></entry><entry><title type="html">TeSMo - Generating Human Interaction Motions in Scenes with Text Control</title><link href="https://justusthies.github.io/posts/tesmo/" rel="alternate" type="text/html" title="TeSMo - Generating Human Interaction Motions in Scenes with Text Control" /><published>2024-09-29T11:00:00+02:00</published><updated>2024-09-29T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/tesmo</id><content type="html" xml:base="https://justusthies.github.io/posts/tesmo/"><![CDATA[<p>We present TeSMo, a method for text-controlled scene-aware motion generation based on denoising diffusion models. Previous text-to-motion methods focus on characters in isolation without considering scenes due to the limited availability of datasets that include motion, text descriptions, and interactive scenes. Our approach begins with pre-training a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. We then enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes. To facilitate training, we embed annotated navigation and interaction motions within scenes. The proposed method produces realistic and diverse human-object interactions, such as navigation and sitting, in different scenes with various object shapes, orientations, initial body positions, and poses. Extensive experiments demonstrate that our approach surpasses prior techniques in terms of the plausibility of human-scene interactions, as well as the realism and variety of the generated motions.</p>

<p><a href="https://research.nvidia.com/labs/toronto-ai/tesmo/">Project page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Hongwei Yi&quot;, &quot;website_link&quot;=&gt;&quot;https://xyyhw.top/&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[TeSMo is a method for text-controlled scene-aware motion generation based on denoising diffusion models. Specifically, we pre-train a scene-agnostic text-to-motion diffusion model, emphasizing goal-reaching constraints on large-scale motion-capture datasets. Then, we enhance this model with a scene-aware component, fine-tuned using data augmented with detailed scene information, including ground plane and object shapes.]]></summary></entry><entry><title type="html">Environment-Specific People</title><link href="https://justusthies.github.io/posts/esp/" rel="alternate" type="text/html" title="Environment-Specific People" /><published>2024-09-01T01:00:00+02:00</published><updated>2024-09-01T01:00:00+02:00</updated><id>https://justusthies.github.io/posts/esp</id><content type="html" xml:base="https://justusthies.github.io/posts/esp/"><![CDATA[<p>Despite significant progress in generative image synthesis and full-body generation in particular, state-of-the-art methods are either context-independent, overly reliant to text prompts, or bound to the curated training datasets, such as fashion images with monotonous backgrounds. Here, our goal is to generate people in clothing that is semantically appropriate for a given scene. To this end, we present ESP, a novel method for context-aware full-body generation, that enables photo-realistic inpainting of people into existing “in-the-wild” photographs. ESP is conditioned on a 2D pose and contextual cues that are extracted from the environment photograph and integrated into the generation process. Our models are trained on a dataset containing a set of in-the-wild photographs of people covering a wide range of different environments. The method is analyzed quantitatively and qualitatively, and we show that ESP outperforms state-of-the-art on the task of contextual full-body generation.</p>

<p><a href="https://esp.is.tue.mpg.de/">Project page</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Mirela Ostrek&quot;}</name></author><category term="publication" /><category term="reconstruction" /><category term="ECCV" /><summary type="html"><![CDATA[We present ESP, a novel method for context-aware full-body generation, that enables photo-realistic inpainting of people into existing "in-the-wild" photographs.]]></summary></entry><entry><title type="html">Lecture: AI-based 3D Graphics and Vision</title><link href="https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-SS24/" rel="alternate" type="text/html" title="Lecture: AI-based 3D Graphics and Vision" /><published>2024-08-24T23:00:00+02:00</published><updated>2024-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-SS24</id><content type="html" xml:base="https://justusthies.github.io/posts/AI-based-3D-Graphics-and-Vision-SS24/"><![CDATA[<h3 id="content">Content</h3>
<p>The lecture will cover AI-based 3D reconstruction from various input modalities (Webcams, RGB-D cameras (Kinect, Realsense, …). The lecture builds upon the classical 3D reconstruction methods discussed in the ‘3D Scanning &amp; Motion Capture’ lecture and shows how components and data structures of those methods can be replaced or extended by methods from AI. It will start with basic concepts of 2D neural rendering, including methods like Pix2Pix, Deferred Neural Rendering and alike. Then, more advanced topics like 3D/4D neural scene representations are discussed. To train those representations, differentiable rendering needs to be understood. The lecture will introduce methods for static and dynamic reconstruction with different levels of controllability and editability.
++ Concept of 2D Neural Rendering
++ Deferred Neural Rendering, AI-based Image-based Rendering.
++ 3D Neural Rendering and Neural Scene Representations
++ Neural Radiance Fields (NeRFs)
++ Neural Point-based Graphics, Gaussian Splatting
++ Differentiable Rendering (Rasterization, Volume Rendering, Shading)
++ Relighting and Material Reconstruction
++ DeepFakes
++ Outlook: detection of synthetic media</p>

<h3 id="aim">Aim</h3>
<p>Basic understanding of how methods of AI can be used for 3D capturing of objects, scenes, and humans. It includes the principles of 2D and 3D neural rendering, as well as the differentiable rendering (Inverse Graphics).</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[The lecture will cover AI-based 3D reconstruction, synthesis, and rendering from various input modalities (image, text, audio). It covers topics like 2D/3D neural rendering, deepfakes, natural image synthesis with different controls, as well as motion and video generation.]]></summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS24-25/" rel="alternate" type="text/html" title="Lecture: 3D Scanning and Motion Capture" /><published>2024-08-24T23:00:00+02:00</published><updated>2024-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS24-25</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS24-25/"><![CDATA[<h3 id="content">Content</h3>
<p>The lecture and exercises will cover 3D reconstruction from various input
modalities (Webcams, RGB-D cameras (Kinect, Realsense, …). It will start with
basic concepts of what is 3D, the different representations, how to capture 3D
and how the devices and sensors function. Based on this introduction, rigid and
non-rigid tracking and reconstruction will be discussed. Specialized face and
body tracking methods will be covered and the applications of the 3D
reconstruction and tracking will be shown. In addition to the 3D surface
reconstruction, techniques for appearance modelling and material estimation
will be shown.</p>

<p>++ Basic concepts of geometry (Meshes, Point Clouds, Pixels &amp; Voxels)
++ RGB and Depth Cameras (Calibration, active/passive stereo, Time of
Flight (ToF), Structured Light, Laser Scanner, Lidar)
++ Surface Representations (Polygonal meshes, parametric surfaces,
implicit surfaces (Radial basis functions, signed distance functions,
indicator function), Marching cubes)
++ Overview of reconstruction methods (Structure from Motion (SfM),
Multi-view Stereo (MVS), SLAM, Bundle Adjustment)
++ Rigid Surface Tracking &amp; Reconstruction (Pose alignment, ICP, online
surface reconstruction pipeline (KinectFusion), scalable surface
representations (VoxelHashing, OctTrees), loop closures and global
optimization)
++ Non-rigid Surface Tracking &amp; Reconstruction (Surface deformation for
modeling, Regularizers: ARAP, ED, etc., Non-rigid surface fitting: e.g.,
non-rigid ICP. Non-rigid reconstruction:
DynamicFusion/VolumeDeform/KillingFusion)
++ Face Tracking &amp; Reconstruction (Keypoint detection &amp; tracking,
Parametric / Statistical Models -&gt; BlendShapes)
++ Body Tracking &amp; Reconstruction (Skeleton Tracking and Inverse
Kinematics, Marker-based motion capture)
++ Material capture (Lightstage, BRDF estimation)
++ Outlook DeepLearning-based tracking</p>

<h3 id="aim">Aim</h3>
<p>Basic understanding of 3D capturing devices and underlying principles (active vs.
passive stereo, ToF etc.), modelling of geometry and conversion between
different representations, principles of static reconstruction (fusion, ICP) and
non-rigid reconstruction using deformation priors. Basic understanding of
specialized class-specific tracking (face, body, hands) and their applications.</p>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[This hands-on lecture focuses on cutting-edge 3D reconstruction approaches, including volumetric fusion approaches on implicit functions, face tracking, hand tracking, and much more! We also cover essential optimization techniques such as Gauss-Newton and Levenberg-Marquardt.]]></summary></entry><entry><title type="html">Seminar: Digital Humans</title><link href="https://justusthies.github.io/posts/Digital-Humans-WS24-25/" rel="alternate" type="text/html" title="Seminar: Digital Humans" /><published>2024-08-23T22:00:00+02:00</published><updated>2024-08-23T22:00:00+02:00</updated><id>https://justusthies.github.io/posts/Digital-Humans-WS24-25</id><content type="html" xml:base="https://justusthies.github.io/posts/Digital-Humans-WS24-25/"><![CDATA[<h3 id="content">Content</h3>
<p>The goal of the seminar is to get the students in touch with recent publications
in the research field. They will learn how publications are structured and
presented. They will learn how methods are validated / evaluated.
In addition, the students will train their presentation skills by preparing a talk on
a selected topic (recent publication of their choice).</p>

<p>The talks have to cover the following topics:</p>
<ul>
  <li>face tracking / reconstruction</li>
  <li>facial reenactment (audio driven, video driven animation)</li>
  <li>body tracking / reconstruction</li>
  <li>hand tracking</li>
  <li>inverse rendering, material estimation, light estimation</li>
</ul>

<h3 id="organization--requirements">Organization / Requirements</h3>
<ul>
  <li>Lecture: 3D Scanning &amp; Motion Capturing (optional)</li>
  <li>The participants have to present their topics in a talk (in English), which should last 30 minutes.</li>
  <li>The semi-final slides (PDF) should be sent one week before the talk; otherwise, the talk will be canceled.</li>
  <li>A short report (approximately 2-3 pages in the ACM SIGGRAPH TOG format (acmtog)) should be prepared and sent within two weeks after the talk. When you send the report, please send the final slides (PDF) together.</li>
</ul>]]></content><author><name>Justus Thies</name></author><category term="post" /><category term="teaching" /><category term="TUDa" /><summary type="html"><![CDATA[In this course, students will autonomously investigate recent research about Digital Humans. Independent investigation for further reading, critical analysis, and evaluation of the topic is required.]]></summary></entry><entry><title type="html">EG Young Researcher Award 2024</title><link href="https://justusthies.github.io/posts/eg_young_researcher_award/" rel="alternate" type="text/html" title="EG Young Researcher Award 2024" /><published>2024-05-01T11:00:00+02:00</published><updated>2024-05-01T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/eg_young_researcher_award</id><content type="html" xml:base="https://justusthies.github.io/posts/eg_young_researcher_award/"><![CDATA[<p>Justus Thies receives the EUROGRAPHICS Young Researcher Award 2024. Justus obtained his PhD from the University of Erlangen Nuremberg. He is now a full Professor at the Technical University of Darmstadt where he is leading the 3D Graphics &amp; Vision group. In addition, he is an independent research group leader at the Max Planck Institute for Intelligent Systems.<br />
Justus studies the capture and synthesis of digital humans. His work blends elements of computer graphics, computer vision, and machine learning, with the end goal of capturing and re-synthesizing reality. Justus has done profound work on important aspects of “digital humans”, namely marker-less capture, neural synthesis, and multi-media forensics, among others. 
Justus’ Face2Face algorithm was a pioneering and very successful approach to real-time facial performance capture from video data which also enabled facial re-enactment. Later, with the advent of neural networks, Justus again considerably advanced the state of the art with his work on deferred neural rendering. Importantly, new methods for high-quality image and video, colloquially known as “deep fakes,” can also be abused, e.g. for misinformation. Justus and colleagues have made great strides on this equally important aspect in their work FaceForensics++, which allows to detect manipulated facial images. As the saying goes: “It takes (someone that makes) one to know one”, thus it is of paramount importance that researchers that develop new machine learning-based image synthesis approaches with all their great potential for computer graphics, additionally invest in identifying synthesized imagery. Justus’ groundbreaking work is a demonstration of this principle. 
The work of Justus Thies is published in the top tier conferences and journals of computer graphics and computer vision, and has been very widely cited, His work has been honored with a Research Highlight report of the Communication of the ACM 2019, and an emerging technology award at SIGGRAPH 2016.  With his considerable and impactful contributions to the field of digital humans, Justus helps shaping the future of how the real and the virtual interact.</p>

<p>EUROGRAPHICS is pleased to recognize Justus Thies with the 2024 Young Researcher Award in recognition of his outstanding contributions to Computer Graphics in the area of marker-less motion capture and synthesis.</p>

<p><a href="https://www.eg.org/wp/eurographics-awards-programme/the-young-researcher-award/young-researcher-award-2024-justus-thies/">Eurographics Website</a></p>

<p><a href="https://www.informatik.tu-darmstadt.de/fb20/ueber_uns_details_299776.en.jsp">TUDa News</a></p>]]></content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="highlight" /><category term="EG" /><summary type="html"><![CDATA[I have been awarded with the Eurographics Young Researcher Award 2024.]]></summary></entry></feed>