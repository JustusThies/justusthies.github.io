<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.0.0">Jekyll</generator><link href="https://justusthies.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://justusthies.github.io/" rel="alternate" type="text/html" /><updated>2020-12-09T13:06:29+01:00</updated><id>https://justusthies.github.io/feed.xml</id><title type="html">Justus Thies</title><subtitle>Personal Website of Justus Thies covering his publications and teaching courses.
</subtitle><author><name>Justus Thies</name></author><entry><title type="html">NerFACE: Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction</title><link href="https://justusthies.github.io/posts/nerface/" rel="alternate" type="text/html" title="NerFACE&amp;#58; Dynamic Neural Radiance Fields for Monocular 4D Facial Avatar Reconstruction" /><published>2020-12-08T08:00:00+01:00</published><updated>2020-12-08T08:00:00+01:00</updated><id>https://justusthies.github.io/posts/nerface</id><content type="html" xml:base="https://justusthies.github.io/posts/nerface/">&lt;p&gt;We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face.
Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications.
Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoint or head-poses is required.
In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks.
To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions.
We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup.
In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://gafniguy.github.io/4D-Facial-Avatars&quot;&gt;Project Page&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Guy Gafni&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/guy_gafni/profile.html&quot;}</name></author><category term="ArXiv" /><summary type="html">We present dynamic neural radiance fields for modeling the appearance and dynamics of a human face. Digitally modeling and reconstructing a talking human is a key building-block for a variety of applications. Especially, for telepresence applications in AR or VR, a faithful reproduction of the appearance including novel viewpoint or head-poses is required. In contrast to state-of-the-art approaches that model the geometry and material properties explicitly, or are purely image-based, we introduce an implicit representation of the head based on scene representation networks. To handle the dynamics of the face, we combine our scene representation network with a low-dimensional morphable model which provides explicit control over pose and expressions. We use volumetric rendering to generate images from this hybrid representation and demonstrate that such a dynamic neural scene representation can be learned from monocular input data only, without the need of a specialized capture setup. In our experiments, we show that this learned volumetric representation allows for photo-realistic image generation that surpasses the quality of state-of-the-art video-based reenactment methods.</summary></entry><entry><title type="html">Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction</title><link href="https://justusthies.github.io/posts/neuraldeformationgraphs/" rel="alternate" type="text/html" title="Neural Deformation Graphs for Globally-consistent Non-rigid Reconstruction" /><published>2020-12-08T07:00:00+01:00</published><updated>2020-12-08T07:00:00+01:00</updated><id>https://justusthies.github.io/posts/neuraldeformationgraphs</id><content type="html" xml:base="https://justusthies.github.io/posts/neuraldeformationgraphs/">&lt;p&gt;We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. 
Specifically, we implicitly model a deformation graph via a deep neural network.
This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking.
Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object.
Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints,  the underlying network is trained in a self-supervised fashion.
We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation.
Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings.
Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="ArXiv" /><summary type="html">We introduce Neural Deformation Graphs for globally-consistent deformation tracking and 3D reconstruction of non-rigid objects. Specifically, we implicitly model a deformation graph via a deep neural network. This neural deformation graph does not rely on any object-specific structure and, thus, can be applied to general non-rigid deformation tracking. Our method globally optimizes this neural graph on a given sequence of depth camera observations of a non-rigidly moving object. Based on explicit viewpoint consistency as well as inter-frame graph and surface consistency constraints, the underlying network is trained in a self-supervised fashion. We additionally optimize for the geometry of the object with an implicit deformable multi-MLP shape representation. Our approach does not assume sequential input data, thus enabling robust tracking of fast motions or even temporally disconnected recordings. Our experiments demonstrate that our Neural Deformation Graphs outperform state-of-the-art non-rigid reconstruction approaches both qualitatively and quantitatively, with 64% improved reconstruction and 62% improved deformation tracking performance.</summary></entry><entry><title type="html">ID-Reveal: Identity-aware DeepFake Video Detection</title><link href="https://justusthies.github.io/posts/idreveal/" rel="alternate" type="text/html" title="ID-Reveal&amp;#58; Identity-aware DeepFake Video Detection" /><published>2020-12-08T06:00:00+01:00</published><updated>2020-12-08T06:00:00+01:00</updated><id>https://justusthies.github.io/posts/idreveal</id><content type="html" xml:base="https://justusthies.github.io/posts/idreveal/">&lt;p&gt;State-of-the-art DeepFake forgery detectors are trained in a supervised fashion to answer the question ‘is this video real or fake?’.
Given that their training is typically method-specific, these approaches show poor generalization across different types of facial manipulations, e.g., face swapping or facial reenactment.
In this work, we look at the problem from a different perspective by focusing on the facial characteristics of a specific identity; i.e., we want to answer the question ‘Is this the person who is claimed to be?’.
To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how each person moves while talking, by means of metric learning coupled with an adversarial training strategy. 
ur method is independent of the specific type of manipulation since it is trained only on real videos. Moreover, relying on high-level semantic features, it is robust to widespread and disruptive forms of post-processing.
We performed a thorough experimental analysis on several publicly available benchmarks, such as FaceForensics++, Google’s DFD, and Celeb-DF.
Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks.
In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://github.com/grip-unina/id-reveal&quot;&gt;Code&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Davide Cozzolino&quot;, &quot;website_link&quot;=&gt;&quot;http://www.grip.unina.it/people/userprofile/davide_cozzolino.html&quot;}</name></author><category term="ArXiv" /><summary type="html">State-of-the-art DeepFake forgery detectors are trained in a supervised fashion to answer the question ‘is this video real or fake?’. Given that their training is typically method-specific, these approaches show poor generalization across different types of facial manipulations, e.g., face swapping or facial reenactment. In this work, we look at the problem from a different perspective by focusing on the facial characteristics of a specific identity; i.e., we want to answer the question ‘Is this the person who is claimed to be?’. To this end, we introduce ID-Reveal, a new approach that learns temporal facial features, specific of how each person moves while talking, by means of metric learning coupled with an adversarial training strategy. ur method is independent of the specific type of manipulation since it is trained only on real videos. Moreover, relying on high-level semantic features, it is robust to widespread and disruptive forms of post-processing. We performed a thorough experimental analysis on several publicly available benchmarks, such as FaceForensics++, Google’s DFD, and Celeb-DF. Compared to state of the art, our method improves generalization and is more robust to low-quality videos, that are usually spread over social networks. In particular, we obtain an average improvement of more than 15% in terms of accuracy for facial reenactment on high compressed videos.</summary></entry><entry><title type="html">BIDT: Echt oder?</title><link href="https://justusthies.github.io/posts/bidt/" rel="alternate" type="text/html" title="BIDT&amp;#58; Echt oder?" /><published>2020-10-20T18:00:00+02:00</published><updated>2020-10-20T18:00:00+02:00</updated><id>https://justusthies.github.io/posts/bidt</id><content type="html" xml:base="https://justusthies.github.io/posts/bidt/">&lt;h2 id=&quot;was-ist-ein-deepfake&quot;&gt;Was ist ein DeepFake?&lt;/h2&gt;
&lt;p&gt;Die Bezeichnung DeepFake wird umgangssprachlich für jegliche Art der digitalen Bild- und Videomanipulation genutzt. Der Begriff DeepFake wurde vor allem von der digitalen Manipulation von Portraitbildern geprägt. Da neuere Methoden auf maschinellem Lernen, insbesondere dem sog. Deep Learning, basieren, um photorealistische Inhalte zu erzeugen, kam es zu der Wortneuschöpfung Deep-Fake. DeepFakes sind bereits weit verbreitet und entsprechende Programme werden von einer Vielzahl von Nutzern verwendet. Einige „Bildfilter“ in Apps wie Instagram, die das Aussehen einer Person verändern, können als DeepFakes aufgefasst werden. Insbesondere Face Swapping, also das Vertauschen von zwei Gesichtern, wird als DeepFake bezeichnet. Solche Manipulationen erzeugen virtuelle Personen, die so nicht existieren (das Gesicht passt nicht zum Rest des Körpers). Jedoch gibt es auch Techniken, die die Identität einer Person erhalten und nur die Gesichtszüge verändern - man spricht hierbei vom Facial Reenactment. D.h. der Gesichtsausdruck kann von einer Person auf eine andere übertragen werden.&lt;/p&gt;

&lt;h2 id=&quot;inwieweit-können-deepfakes-zu-einer-gefahr-für-die-gesellschaft-werden&quot;&gt;Inwieweit können DeepFakes zu einer Gefahr für die Gesellschaft werden?&lt;/h2&gt;
&lt;p&gt;DeepFakes sind zurzeit bereits eine Gefahr für einzelne Personen. Cyber-Mobbing (also digitales Mobbing) durch manipulierte Bilder und sogenannte FakePorns (Gesicht einer Person wird in ein Pornovideo eingefügt) werden bereits zur Diskreditierung von Personen erstellt.
Für die Gesellschaft im Allgemeinen ergeben sich auch gewisse Gefahren. Allein die Frage ob ein Inhalt gefälscht wurde oder nicht, hat einen Vertrauensverlust als Folge. Politiker sähen bereits Zweifel an diversen Berichterstattungen mit dem Hinweis auf fake news (siehe USA). Diese Entwicklungen sind besorgniserregend, da es für die Menschen zunehmend schwieriger wird, zu erkennen was der Realität entspricht und was nicht.&lt;/p&gt;

&lt;h2 id=&quot;welche-anwendungsfälle-gibt-es-für-solche-digital-erzeugten-medien&quot;&gt;Welche Anwendungsfälle gibt es für solche digital erzeugten Medien?&lt;/h2&gt;
&lt;p&gt;Die grundlegenden Techniken mit denen DeepFakes erstellt werden, haben diverse Anwendungsfälle. Digitale Inhalte sind omnipräsent, augmented reality (angereicherte Realität) und virtual reality (virtuelle Realität) kurz AR bzw. VR finden immer weitere Anwendungsfelder. Firmen arbeiten mit Hochdruck daran Telekonferenzsysteme weiterzuentwickeln - anstatt einfache 2D Videos ist es das Ziel 3D Inhalte zu erzeugen. 3D Inhalte haben den Vorteil, dass der Nutzer entscheiden kann, wo er hinschauen möchte - Telekonferenzen in 3D werden möglich. Die grundlegenden Algorithmen, um dies zu erreichen, basieren auch auf den Techniken und Methoden, die zu DeepFakes benutzt werden. Es geht darum photorealistische Abbilder von uns Menschen zu erzeugen (einen digitalen Double). Virtuelle Spiegel, um Makeup zu testen, eine virtuelle Anprobe von Kleidung, das Simulieren von chirurgischen Operationen, all dies wird durch einen digitalen Double möglich.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://padlet.com/bidt_Dialog/werkstattdigital_Deepfakes&quot;&gt;Padlet website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="BIDT" /><summary type="html">Was ist ein DeepFake? Die Bezeichnung DeepFake wird umgangssprachlich für jegliche Art der digitalen Bild- und Videomanipulation genutzt. Der Begriff DeepFake wurde vor allem von der digitalen Manipulation von Portraitbildern geprägt. Da neuere Methoden auf maschinellem Lernen, insbesondere dem sog. Deep Learning, basieren, um photorealistische Inhalte zu erzeugen, kam es zu der Wortneuschöpfung Deep-Fake. DeepFakes sind bereits weit verbreitet und entsprechende Programme werden von einer Vielzahl von Nutzern verwendet. Einige „Bildfilter“ in Apps wie Instagram, die das Aussehen einer Person verändern, können als DeepFakes aufgefasst werden. Insbesondere Face Swapping, also das Vertauschen von zwei Gesichtern, wird als DeepFake bezeichnet. Solche Manipulationen erzeugen virtuelle Personen, die so nicht existieren (das Gesicht passt nicht zum Rest des Körpers). Jedoch gibt es auch Techniken, die die Identität einer Person erhalten und nur die Gesichtszüge verändern - man spricht hierbei vom Facial Reenactment. D.h. der Gesichtsausdruck kann von einer Person auf eine andere übertragen werden.</summary></entry><entry><title type="html">Neural Non-Rigid Tracking</title><link href="https://justusthies.github.io/posts/neuraltracking/" rel="alternate" type="text/html" title="Neural Non-Rigid Tracking" /><published>2020-09-29T11:00:00+02:00</published><updated>2020-09-29T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/neuraltracking</id><content type="html" xml:base="https://justusthies.github.io/posts/neuraltracking/">&lt;p&gt;We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction.
Given two input RGB-D frames of a non-rigidly moving object, we employ a convolutional neural network to predict dense correspondences.
These correspondences are used as constraints in an as-rigid-as-possible (ARAP) optimization problem.
By enabling gradient back-propagation through the non-rigid optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking.
Furthermore, this formulation allows for learning correspondence weights in a self-supervised manner.
Thus, outliers and wrong correspondences are down-weighted to enable robust tracking.
Compared to state-of-the-art approaches, our algorithm shows improved reconstruction performance, while simultaneously achieving 85x faster correspondence prediction than comparable deep-learning based methods.&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Aljaz Bozic&quot;, &quot;website_link&quot;=&gt;&quot;http://niessnerlab.org/members/aljaz_bozic/profile.html&quot;}</name></author><category term="NeurIPS" /><summary type="html">We introduce a novel, end-to-end learnable, differentiable non-rigid tracker that enables state-of-the-art non-rigid reconstruction. Given two input RGB-D frames of a non-rigidly moving object, we employ a convolutional neural network to predict dense correspondences. These correspondences are used as constraints in an as-rigid-as-possible (ARAP) optimization problem. By enabling gradient back-propagation through the non-rigid optimization solver, we are able to learn correspondences in an end-to-end manner such that they are optimal for the task of non-rigid tracking. Furthermore, this formulation allows for learning correspondence weights in a self-supervised manner. Thus, outliers and wrong correspondences are down-weighted to enable robust tracking. Compared to state-of-the-art approaches, our algorithm shows improved reconstruction performance, while simultaneously achieving 85x faster correspondence prediction than comparable deep-learning based methods.</summary></entry><entry><title type="html">Egocentric Videoconferencing</title><link href="https://justusthies.github.io/posts/egochat/" rel="alternate" type="text/html" title="Egocentric Videoconferencing" /><published>2020-09-28T11:00:00+02:00</published><updated>2020-09-28T11:00:00+02:00</updated><id>https://justusthies.github.io/posts/egochat</id><content type="html" xml:base="https://justusthies.github.io/posts/egochat/">&lt;p&gt;We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices.
Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera.
Using a frontal camera in a hands-free setting when a person is on the move is impractical.
Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. 
To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses.
Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video.
To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. 
Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods.
We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. 
To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face.
This allows us to synthesis results at different head poses. 
Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator.
We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.&lt;/p&gt;

&lt;video width=&quot;800&quot; height=&quot;450&quot; controls=&quot;&quot;&gt;
	&lt;source src=&quot;http://gvv.mpi-inf.mpg.de/projects/EgoChat/data/Main.mp4&quot; type=&quot;video/mp4&quot; /&gt;
&lt;/video&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;http://gvv.mpi-inf.mpg.de/projects/EgoChat/&quot;&gt;Project Page&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Mohamed Elgharib&quot;, &quot;website_link&quot;=&gt;&quot;http://people.mpi-inf.mpg.de/~elgharib/&quot;}</name></author><category term="SIGGRAPH ASIA" /><summary type="html">We introduce a method for egocentric videoconferencing that enables hands-free video calls, for instance by people wearing smart glasses or other mixed-reality devices. Videoconferencing portrays valuable non-verbal communication and face expression cues, but usually requires a front-facing camera. Using a frontal camera in a hands-free setting when a person is on the move is impractical. Even holding a mobile phone camera in the front of the face while sitting for a long duration is not convenient. To overcome these issues, we propose a low-cost wearable egocentric camera setup that can be integrated into smart glasses. Our goal is to mimic a classical video call, and therefore, we transform the egocentric perspective of this camera into a front facing video. To this end, we employ a conditional generative adversarial neural network that learns a transition from the highly distorted egocentric views to frontal views common in videoconferencing. Our approach learns to transfer expression details directly from the egocentric view without using a complex intermediate parametric expressions model, as it is used by related face reenactment methods. We successfully handle subtle expressions, not easily captured by parametric blendshape-based solutions, e.g., tongue movement, eye movements, eye blinking, strong expressions and depth varying movements. To get control over the rigid head movements in the target view, we condition the generator on synthetic renderings of a moving neutral face. This allows us to synthesis results at different head poses. Our technique produces temporally smooth video-realistic renderings in real-time using a video-to-video translation network in conjunction with a temporal discriminator. We demonstrate the improved capabilities of our technique by comparing against related state-of-the art approaches.</summary></entry><entry><title type="html">WIPO: Artifical Intelligence and Intellectual Property</title><link href="https://justusthies.github.io/posts/wipo/" rel="alternate" type="text/html" title="WIPO&amp;#58; Artifical Intelligence and Intellectual Property" /><published>2020-09-18T09:00:00+02:00</published><updated>2020-09-18T09:00:00+02:00</updated><id>https://justusthies.github.io/posts/wipo</id><content type="html" xml:base="https://justusthies.github.io/posts/wipo/">&lt;p&gt;This virtual exhibition presents current approaches of AI-driven media creation.
It raises interesting questions regarding intellectual property.&lt;/p&gt;

&lt;p&gt;&lt;a href=&quot;https://www.wipo.int/about-ip/en/artificial_intelligence/&quot;&gt;Weblink to the official website.&lt;/a&gt;&lt;/p&gt;</content><author><name>{&quot;display_name&quot;=&gt;&quot;Justus Thies&quot;, &quot;website_link&quot;=&gt;&quot;/&quot;}</name></author><category term="WIPO" /><summary type="html">This virtual exhibition presents current approaches of AI-driven media creation. It raises interesting questions regarding intellectual property.</summary></entry><entry><title type="html">Seminar: 3D Vision</title><link href="https://justusthies.github.io/posts/3D-Vision-WS20-21/" rel="alternate" type="text/html" title="Seminar&amp;#58; 3D Vision" /><published>2020-08-24T23:00:00+02:00</published><updated>2020-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Vision-WS20-21</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Vision-WS20-21/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).&lt;/p&gt;

&lt;p&gt;The talks have to cover the following topics:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;face tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;facial reenactment (audio driven, video driven animation)&lt;/li&gt;
  &lt;li&gt;body tracking / reconstruction&lt;/li&gt;
  &lt;li&gt;hand tracking&lt;/li&gt;
  &lt;li&gt;static / dynamic scene reconstruction&lt;/li&gt;
  &lt;li&gt;scene understanding&lt;/li&gt;
  &lt;li&gt;inverse rendering, material estimation, light estimation&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;organization--requirements&quot;&gt;Organization / Requirements&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Lecture: 3D Scanning &amp;amp; Motion Capturing&lt;/li&gt;
  &lt;li&gt;The participants have to present their topics in a talk (in English), which should last 30 minutes.&lt;/li&gt;
  &lt;li&gt;The semi-final slides (PDF) should be sent one week before the talk; otherwise, the talk will be canceled.&lt;/li&gt;
  &lt;li&gt;A short report (approximately 2-3 pages in the ACM SIGGRAPH TOG format (acmtog)) should be prepared and sent within two weeks after the talk. When you send the report, please send the final slides (PDF) together.&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content In this course, students will autonomously investigate recent research about 3D Scanning &amp;amp; Motion Capturing. Independent investigation for further reading, critical analysis, and evaluation of the topic is required. Each participant has to give a talk about a paper of the top tier conferences in this field (SIGGRAPH, SIGGRAPH ASIA, CVPR, ICCV, ECCV, …). In addition to the talk we ask for a summary of the paper (minimum 2 pages).</summary></entry><entry><title type="html">Practical Course: 3D Scanning and Spatial Learning</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS20-21/" rel="alternate" type="text/html" title="Practical Course&amp;#58; 3D Scanning and Spatial Learning" /><published>2020-08-24T23:00:00+02:00</published><updated>2020-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS20-21</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Spatial-Learning-WS20-21/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp;amp; Motion Capture as well as by the Introduction to Deep Learning lecture.
In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field.
Specifically, we will have projects on:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;human motion capturing (e.g., Fusion4D, BodyFusion)&lt;/li&gt;
  &lt;li&gt;real-time facial motion capturing (spare and dense approaches)&lt;/li&gt;
  &lt;li&gt;3D scene reconstruction (e.g., BundleFusion)&lt;/li&gt;
  &lt;li&gt;scan refinement (e.g., ShapeFromShading)&lt;/li&gt;
  &lt;li&gt;neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering)&lt;/li&gt;
  &lt;li&gt;scene completion (e.g., ScanComplete)&lt;/li&gt;
  &lt;li&gt;3D object retrieval and alignment (e.g., Scan2CAD)&lt;/li&gt;
  &lt;li&gt;scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS)&lt;/li&gt;
  &lt;li&gt;neural scene representations (DeepSDF, OccupancyNets, NeRF,…)&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;objective&quot;&gt;Objective&lt;/h3&gt;
&lt;p&gt;Upon completion of this module, students will have acquired extensive theoretical concepts behind state-of-the art 3D reconstruction methods, in particular in the context of human motion capturing, static object scanning, scene understanding and synthesis of captured scenes. Besides the theoretical foundations, a significant aspect lies on the practical realization and implementation of such algorithms.&lt;/p&gt;

&lt;h3 id=&quot;organization&quot;&gt;Organization&lt;/h3&gt;
&lt;p&gt;In the practical course students shall get familiar with state-of-the-art 3D scanning. They will be assisted by current PhD students working in this field (regular office hours). To ensure a good progress during the semester, we will have mandatory meetings (every two weeks) where the students report their current state. In the end of the course, the students are asked to give a talk about their project and results.&lt;/p&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, 3D scanning &amp;amp; Motion Capture, Introduction to Deep Learning, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D scanning and motion capture is of paramount importance for content creation, man-machine as well as machine-environment interaction. In this course we will continue the topics covered by the 3D Scanning &amp;amp; Motion Capture as well as by the Introduction to Deep Learning lecture. In the spirit of ‘learning by doing’ the students are asked to implement state-of-the-art reconstruction methods or current research topics in the field. Specifically, we will have projects on: human motion capturing (e.g., Fusion4D, BodyFusion) real-time facial motion capturing (spare and dense approaches) 3D scene reconstruction (e.g., BundleFusion) scan refinement (e.g., ShapeFromShading) neural rendering of 3D content (e.g., DeepVoxel, NeuralRendering) scene completion (e.g., ScanComplete) 3D object retrieval and alignment (e.g., Scan2CAD) scene understanding, instance segmentation (e.g., ScanNet, 3D-SIS) neural scene representations (DeepSDF, OccupancyNets, NeRF,…)</summary></entry><entry><title type="html">Lecture: 3D Scanning and Motion Capture</title><link href="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21/" rel="alternate" type="text/html" title="Lecture&amp;#58; 3D Scanning and Motion Capture" /><published>2020-08-24T23:00:00+02:00</published><updated>2020-08-24T23:00:00+02:00</updated><id>https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21</id><content type="html" xml:base="https://justusthies.github.io/posts/3D-Scanning-and-Motion-Capture-WS20-21/">&lt;h3 id=&quot;content&quot;&gt;Content&lt;/h3&gt;
&lt;p&gt;3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Basic concepts of geometry
    &lt;ul&gt;
      &lt;li&gt;Meshes (polygonal), Point Clouds, Pixels &amp;amp; Voxels&lt;/li&gt;
      &lt;li&gt;RGB and Depth Cameras&lt;/li&gt;
      &lt;li&gt;Extrinsics and Intrinsics&lt;/li&gt;
      &lt;li&gt;Capture devices&lt;/li&gt;
      &lt;li&gt;RGB and Multi-view&lt;/li&gt;
      &lt;li&gt;RGB-D cameras&lt;/li&gt;
      &lt;li&gt;Stereo&lt;/li&gt;
      &lt;li&gt;Time of Flight (ToF)&lt;/li&gt;
      &lt;li&gt;Structured Light&lt;/li&gt;
      &lt;li&gt;Laser Scanner, Lidar&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Surface Representations
    &lt;ul&gt;
      &lt;li&gt;Polygonal meshes (trimeshes, etc.)&lt;/li&gt;
      &lt;li&gt;Parametric surfaces: splines, nurbs&lt;/li&gt;
      &lt;li&gt;Implicit surfaces&lt;/li&gt;
      &lt;li&gt;Ridge-based surfaces&lt;/li&gt;
      &lt;li&gt;Radial basis functions&lt;/li&gt;
      &lt;li&gt;Signed distance functions (volumetric, Curless &amp;amp; Levoy)&lt;/li&gt;
      &lt;li&gt;Indicator function (Poisson Surface Reconstruction)&lt;/li&gt;
      &lt;li&gt;More general: level sets&lt;/li&gt;
      &lt;li&gt;Marching cubes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;High-level overview of reconstructions
    &lt;ul&gt;
      &lt;li&gt;Structure from Motion (SfM)&lt;/li&gt;
      &lt;li&gt;Multi-view Stereo (MVS)&lt;/li&gt;
      &lt;li&gt;SLAM&lt;/li&gt;
      &lt;li&gt;Bundle Adjustment&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Optimization
    &lt;ul&gt;
      &lt;li&gt;Non-linear least squares&lt;/li&gt;
      &lt;li&gt;Gauss-Newton LM&lt;/li&gt;
      &lt;li&gt;Examples in Ceres&lt;/li&gt;
      &lt;li&gt;Symbolic diff vs auto-diff&lt;/li&gt;
      &lt;li&gt;Auto-diff with dual numbers&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Pose alignment&lt;/li&gt;
      &lt;li&gt;ICP (point cloud alignment; depth-to-model alignment; rigid ‘fitting’)&lt;/li&gt;
      &lt;li&gt;Online surface reconstruction pipeline: KinectFusion&lt;/li&gt;
      &lt;li&gt;Scalable surface representations: VoxelHashing, OctTrees&lt;/li&gt;
      &lt;li&gt;Loop closures and global optimization&lt;/li&gt;
      &lt;li&gt;Robust optimization&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Non-rigid Surface Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Surface deformation for modeling&lt;/li&gt;
      &lt;li&gt;Regularizers: ARAP, ED, etc.&lt;/li&gt;
      &lt;li&gt;Non-rigid surface fitting: e.g., non-rigid ICP&lt;/li&gt;
      &lt;li&gt;Non-rigid reconstruction: DynamicFusion/VolumeDeform/KillingFusion&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Body Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Skeleton Tracking and Inverse Kinematics&lt;/li&gt;
      &lt;li&gt;Learning-based approaches from RGB and RGB-D&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Face Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Keypoint detection &amp;amp; tracking&lt;/li&gt;
      &lt;li&gt;Parametric / Statistical Models -&amp;gt; BlendShapes&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Hand Tracking &amp;amp; Reconstruction
    &lt;ul&gt;
      &lt;li&gt;Parametric Models&lt;/li&gt;
      &lt;li&gt;Some DeepLearning-based things&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Discriminative vs generative tracking
    &lt;ul&gt;
      &lt;li&gt;Random forests&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Motion Capture in Movies
    &lt;ul&gt;
      &lt;li&gt;Marker-based motion capture&lt;/li&gt;
      &lt;li&gt;LightStage -&amp;gt; movies&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;BRDF and Material Capture&lt;/li&gt;
  &lt;li&gt;Open research questions&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;prerequisites&quot;&gt;Prerequisites&lt;/h3&gt;
&lt;p&gt;Introduction to Informatics I, Analysis, Linear Algebra, Computer Graphics, C++&lt;/p&gt;</content><author><name>Justus Thies</name></author><category term="TUM" /><summary type="html">Content 3D reconstruction, RGB-D scanning (Kinect, Tango, RealSense), ICP, camera tracking, sensor calibration, VolumetricFusion, Non-Rigid Registration, PoseTracking, Motion Capture, Body-, Face-, and Hand-Tracking, 3D DeepLearning, selected optimization techniques to solve the problem statements (GN, LM, gradient descent).</summary></entry></feed>